Title	Co-learning of Word Representations and Morpheme Representations	page=0 xpos=0 ypos=0 single-column centered left-indent right-indent font-largest page-top headchar-capital above-double-space above-line-space
Author	Siyu Qiu Qing Cui	page=0 xpos=2 ypos=0 single-column centered left-indent right-indent font-largest indented-line shorter-tail line-double-space line-space headchar-capital
B-Affiliation	Nankai University Tsinghua University	page=0 xpos=1 ypos=0 single-column left-indent right-indent font-largest hanged-line longer-tail headchar-capital
Address	Tianjin, 300071, China Beijing, 100084, China	page=0 xpos=1 ypos=0 single-column centered left-indent right-indent font-largest hanged-line longer-tail headchar-capital
Email	ppqq2356@gmail.com cuiq12@mails.tsinghua.edu.cn	page=0 xpos=1 ypos=0 single-column left-indent right-indent font-largest hanged-line longer-tail symbol-atmark headchar-lower above-double-space above-line-space
Author	Jiang Bian Bin Gao Tie-Yan Liu	page=0 xpos=1 ypos=1 single-column left-indent right-indent font-largest hanged-line shorter-tail line-double-space line-space headchar-capital
B-Affiliation	Microsoft Research Microsoft Research Microsoft Research	page=0 xpos=0 ypos=1 single-column centered left-indent right-indent font-largest hanged-line longer-tail headchar-capital
Address	Beijing, 100080, China Beijing, 100080, China Beijing, 100080, China	page=0 xpos=0 ypos=1 single-column centered left-indent right-indent font-largest hanged-line longer-tail headchar-capital
Email	jibian@microsoft.com bingao@microsoft.com tyliu@microsoft.com	page=0 xpos=0 ypos=1 single-column left-indent right-indent font-largest hanged-line longer-tail symbol-atmark headchar-lower above-line-space
AbstractHeader	Abstract	page=0 xpos=4 ypos=1 single-column centered left-indent right-indent font-largest indented-line shorter-tail line-space string-abstract headchar-capital above-double-space above-line-space
B-Abstract	The techniques of using neural networks to learn distributed word representations (i.e., word	page=0 xpos=0 ypos=2 single-column centered left-indent right-indent hanged-line longer-tail line-double-space line-space headchar-capital
I-Abstract	embeddings) have been used to solve a variety of natural language processing tasks. The re-	page=0 xpos=0 ypos=2 single-column centered left-indent right-indent aligned-line headchar-lower tailchar-hiphen
I-Abstract	cently proposed methods, such as CBOW and Skip-gram, have demonstrated their effectiveness	page=0 xpos=0 ypos=2 single-column centered left-indent right-indent aligned-line headchar-lower
I-Abstract	in learning word embeddings based on context information such that the obtained word embed-	page=0 xpos=0 ypos=2 single-column centered left-indent right-indent aligned-line headchar-lower tailchar-hiphen
I-Abstract	dings can capture both semantic and syntactic relationships between words. However, it is quite	page=0 xpos=0 ypos=2 single-column centered left-indent right-indent aligned-line headchar-lower
I-Abstract	challenging to produce high-quality word representations for rare or unknown words due to their	page=0 xpos=0 ypos=3 single-column centered left-indent right-indent aligned-line headchar-lower
I-Abstract	insufficient context information. In this paper, we propose to leverage morphological knowledge	page=0 xpos=0 ypos=3 single-column centered left-indent right-indent aligned-line headchar-lower
I-Abstract	to address this problem. Particularly, we introduce the morphological knowledge as both ad-	page=0 xpos=0 ypos=3 single-column centered left-indent right-indent aligned-line headchar-lower tailchar-hiphen
I-Abstract	ditional input representation and auxiliary supervision to the neural network framework. As a	page=0 xpos=0 ypos=3 single-column centered left-indent right-indent aligned-line headchar-lower
I-Abstract	result, beyond word representations, the proposed neural network model will produce morpheme	page=0 xpos=0 ypos=3 single-column centered left-indent right-indent aligned-line headchar-lower
I-Abstract	representations, which can be further employed to infer the representations of rare or unknown	page=0 xpos=0 ypos=4 single-column centered left-indent right-indent aligned-line headchar-lower
I-Abstract	words based on their morphological structure. Experiments on an analogical reasoning task and	page=0 xpos=0 ypos=4 single-column centered left-indent right-indent aligned-line headchar-lower
I-Abstract	several word similarity tasks have demonstrated the effectiveness of our method in producing	page=0 xpos=0 ypos=4 single-column centered left-indent right-indent aligned-line headchar-lower
I-Abstract	high-quality words embeddings compared with the state-of-the-art methods.	page=0 xpos=0 ypos=4 single-column left-indent right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SectionHeader	1 Introduction	page=0 xpos=0 ypos=5 single-column right-indent font-largest hanged-line shorter-tail line-blank-line line-double-space line-space numbered-heading1 above-double-space above-line-space
B-Body	Word representation is a key factor for many natural language processing (NLP) applications. In the	page=0 xpos=0 ypos=5 single-column full-justified aligned-line longer-tail line-double-space line-space headchar-capital
I-Body	conventional solutions to the NLP tasks, discrete word representations are often adopted, such as the	page=0 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	1-of-v representations, where v is the size of the entire vocabulary and each word in the vocabulary	page=0 xpos=0 ypos=5 single-column full-justified aligned-line
I-Body	is represented as a long vector with only one non-zero element. However, using discrete word vectors	page=0 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	cannot indicate any relationships between different words, even though they may yield high semantic	page=0 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	or syntactic correlations. For example, while careful and carefully have quite similar semantics, their	page=0 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	corresponding 1-of-v representations trigger different indexes to be the hot values, and it is not explicit	page=0 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
E-Body	that careful is much closer to carefully than other words using 1-of-v representations.	page=0 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	To deal with the problem, neural network models have been widely applied to obtain word repre-	page=0 xpos=0 ypos=6 single-column left-indent indented-line longer-tail headchar-capital tailchar-hiphen
I-Body	sentations. In particular, they usually take the 1-of-v representations as the word input vectors in the	page=0 xpos=0 ypos=6 single-column full-justified hanged-line headchar-lower
I-Body	neural networks, and learn new distributed word representations in a low-dimensional continuous em-	page=0 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	bedding space. The principle of these models is that words that are highly correlated in terms of either	page=0 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	semantics or syntactics should be close to each other in the embedding space. Representative works in	page=0 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	this field include feed-forward neural network language model (NNLM) (Bengio et al., 2003), recurrent	page=0 xpos=0 ypos=7 single-column full-justified aligned-line year headchar-lower
I-Body	neural network language model (RNNLM) (Mikolov et al., 2010), and the recently proposed continues	page=0 xpos=0 ypos=7 single-column full-justified aligned-line year headchar-lower
E-Body	bag-of-words (CBOW) model and continues skip-gram (Skip-gram) model (Mikolov et al., 2013a).	page=0 xpos=0 ypos=7 single-column right-indent aligned-line shorter-tail year headchar-lower tailchar-period
B-Body	However, there are still challenges for using neural network models to achieve high-quality word	page=0 xpos=0 ypos=8 single-column left-indent indented-line longer-tail headchar-capital
I-Body	embeddings. First, it is difficult to obtain word embeddings for emerging words as they are not included	page=0 xpos=0 ypos=8 single-column full-justified hanged-line headchar-lower
I-Body	in the vocabulary of the training data. Some previous studies (Mikolov, 2012) used one or more default	page=0 xpos=0 ypos=8 single-column full-justified aligned-line year headchar-lower
I-Body	indexes to represent all the unknown words, but such solution will lose information for the new words.	page=0 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower tailchar-period above-double-space above-line-space
B-Footnote	This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer	page=0 xpos=0 ypos=8 single-column full-justified font-smallest aligned-line line-double-space line-space headchar-capital
I-Footnote	are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/	page=0 xpos=0 ypos=9 single-column right-indent font-smallest aligned-line shorter-tail headchar-lower above-blank-line above-double-space above-line-space
Page	141	page=0 xpos=4 ypos=9 single-column centered left-indent right-indent indented-line shorter-tail line-blank-line line-double-space line-space numeric-only above-double-space above-line-space
B-Footer	Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,	page=0 xpos=0 ypos=9 single-column left-indent right-indent font-smallest hanged-line longer-tail line-double-space line-space year headchar-capital tailchar-comma
I-Footer	pages 141–150, Dublin, Ireland, August 23-29 2014.	page=0 xpos=2 ypos=9 single-column left-indent right-indent font-smallest indented-line shorter-tail year headchar-lower tailchar-period page-bottom
I-Body	Second, the embeddings for rare words are often of low quality due to the insufficient context information	page=1 xpos=0 ypos=0 single-column full-justified page-top headchar-capital
E-Body	in the training data.	page=1 xpos=0 ypos=0 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	Fortunately, semantically or syntactically similar words often share some common morphemes such	page=1 xpos=0 ypos=0 single-column left-indent indented-line longer-tail headchar-capital
I-Body	as roots, affixes, and syllables. For example, probably and probability share the same root, i.e., probab,	page=1 xpos=0 ypos=0 single-column full-justified hanged-line headchar-lower tailchar-comma
I-Body	as well as the same syllables, i.e., pro and ba. Therefore, morphological information can provide valu-	page=1 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	able knowledge to bridge the gap between rare or unknown words and well-known words in learning	page=1 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower
I-Body	word representations. In this paper, we propose a novel neural network architecture that can leverage	page=1 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	morphological knowledge to obtaining high-quality word embeddings. Specifically, we first segment the	page=1 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	words in the training data into morphemes, and then employ the 1-of-v representations of both the words	page=1 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	and their morphemes as the input to the neural network models. In addition, we propose to use mor-	page=1 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	phological information as auxiliary supervision. Particularly, in the output layer of the neural network	page=1 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	architecture, we predict both the words and their corresponding morphemes simultaneously. Moreover,	page=1 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	we introduce extra coefficients into the network to balance the weights between word embeddings and	page=1 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	morpheme embeddings. Therefore, in the back propagation stage, we will update the word embeddings,	page=1 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower tailchar-comma
E-Body	the morpheme embeddings, and the balancing coefficients simultaneously.	page=1 xpos=0 ypos=2 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-line-space
B-Body	Our proposed neural network model yields two major advantages: on one hand, it can leverage three	page=1 xpos=0 ypos=2 single-column left-indent indented-line longer-tail line-space headchar-capital
I-Body	types of co-occurrence information, including co-occurrence between word and word (conventional),	page=1 xpos=0 ypos=2 single-column full-justified hanged-line headchar-lower tailchar-comma
I-Body	co-occurrence between word and morpheme (newly added), and co-occurrence between morpheme and	page=1 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	morpheme (newly added); on the other hand, this new model allows to learn word embeddings and	page=1 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	morpheme embeddings simultaneously, so that it is convenient to build the representations for unknown	page=1 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	words from morpheme embeddings and enhance the representations for rare words. Experiments on	page=1 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	large-scale public datasets demonstrate that our proposed approach can help produce improved word	page=1 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	representations on an analogical reasoning task and several word similarity tasks compared with the	page=1 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
E-Body	state-of-the-art methods.	page=1 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	The rest of the paper is organized as follows. We briefly review the related work on word embedding	page=1 xpos=0 ypos=4 single-column left-indent indented-line longer-tail headchar-capital
I-Body	using neural networks in Section 2. In Section 3, we describe the proposed methods to leverage mor-	page=1 xpos=0 ypos=4 single-column full-justified hanged-line headchar-lower tailchar-hiphen
I-Body	phological knowledge in word embedding using neural network models. The experimental results are	page=1 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
E-Body	reported in Section 4. The paper is concluded in Section 5.	page=1 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-SectionHeader	2 Related Work	page=1 xpos=0 ypos=5 single-column right-indent font-largest aligned-line shorter-tail line-double-space line-space numbered-heading1 above-double-space above-line-space
B-Body	Neural Language Models (NLMs) (Bengio et al., 2003) have been applied in a number of NLP tasks (Col-	page=1 xpos=0 ypos=5 single-column full-justified aligned-line longer-tail line-double-space line-space year headchar-capital tailchar-hiphen
I-Body	lobert and Weston, 2008) (Glorot et al., 2011) (Mikolov et al., 2013a) (Mikolov et al., 2013b) (Socher	page=1 xpos=0 ypos=5 single-column full-justified aligned-line year headchar-lower
I-Body	et al., 2011) (Turney, 2013) (Turney and Pantel, 2010) (Weston et al., ) (Deng et al., 2013) (Collobert	page=1 xpos=0 ypos=6 single-column full-justified aligned-line year headchar-lower
I-Body	et al., 2011) (Mnih and Hinton, 2008) (Turian et al., 2010). In general, they learn distributed word rep-	page=1 xpos=0 ypos=6 single-column full-justified aligned-line year headchar-lower tailchar-hiphen
I-Body	resentations in a continuous embedding space. For example, Mikolov et al. proposed the continuous	page=1 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	bag-of-words model (CBOW) and the continuous skip-gram model (Skip-gram) (Mikolov et al., 2013a).	page=1 xpos=0 ypos=6 single-column full-justified aligned-line year headchar-lower tailchar-period
I-Body	Both of them assume that words co-occurring with the same context should be similar. Collobert et	page=1 xpos=0 ypos=6 single-column full-justified aligned-line headchar-capital
I-Body	al. (Collobert et al., 2011) fed their neural networks with extra features such as the capital letter feature	page=1 xpos=0 ypos=6 single-column full-justified aligned-line year headchar-lower
I-Body	and the part-of-speech (POS) feature, but they still met the challenge of producing high-quality word	page=1 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
E-Body	embeddings for rare words.	page=1 xpos=0 ypos=7 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	Besides using neural network, many different types of models were proposed for estimating continuous	page=1 xpos=0 ypos=7 single-column left-indent indented-line longer-tail headchar-capital
I-Body	representations of words, such as the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet	page=1 xpos=0 ypos=7 single-column full-justified hanged-line headchar-lower
I-Body	Allocation (LDA). However, Mikolov et al. (Mikolov et al., 2013c) have shown that words learned by	page=1 xpos=0 ypos=7 single-column full-justified aligned-line year headchar-capital
I-Body	neural networks are signicantly better than LSA for preserving linear regularities while LDA becomes	page=1 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
E-Body	computationally expensive on large datasets.	page=1 xpos=0 ypos=8 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-line-space
B-Body	There were a lot of previous attempts to include morphology in continuous models, especially in	page=1 xpos=0 ypos=8 single-column left-indent indented-line longer-tail line-space headchar-capital
I-Body	the speech recognition field. Represent works include Letter n-gram (Sperr et al., 2013) and feature-	page=1 xpos=0 ypos=8 single-column full-justified hanged-line year headchar-lower tailchar-hiphen
I-Body	rich DNN-LMs (Mousa et al., 2013). The first work improves the letter-based word representation by	page=1 xpos=0 ypos=8 single-column full-justified aligned-line year headchar-lower
I-Body	replacing the 1-of-v word input of restricted Boltzman machine with a vector indicating all n-grams of	page=1 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	order n and smaller that occur in the word. Additional information such as capitalization is added as well.	page=1 xpos=0 ypos=9 single-column full-justified aligned-line headchar-lower tailchar-period
I-Body	In the model of feature-rich DNN-LMs, the authors expand the inputs of the network to be a mixture of	page=1 xpos=0 ypos=9 single-column full-justified aligned-line headchar-capital above-blank-line above-double-space above-line-space
Page	142	page=1 xpos=4 ypos=9 single-column centered left-indent right-indent indented-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
I-Body	selected full words and morphemes together with their features such as morphological tags. Both of	page=2 xpos=0 ypos=0 single-column full-justified page-top headchar-lower
I-Body	these works intend to capture more morphological information so as to better generalize to unknown or	page=2 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower
E-Body	rare words and to lower the out-of-vocabulary rate.	page=2 xpos=0 ypos=0 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-line-space
B-Body	There are some other related works that consider morphological knowledge when learning the word	page=2 xpos=0 ypos=0 single-column left-indent indented-line longer-tail line-space headchar-capital
I-Body	embeddings, such as factored NLMs (Alexandrescu and Kirchhoff, 2006) and csmRNN (Luong et al.,	page=2 xpos=0 ypos=0 single-column full-justified hanged-line year headchar-lower tailchar-comma
I-Body	2013), both of which are designed to handle rare words. In factored NLMs, each word is viewed as a	page=2 xpos=0 ypos=0 single-column full-justified aligned-line year
I-Body	vector of shape features (e.g., affixed, capitalization, hyphenation, and classes) and a word is predicted	page=2 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	based on several previous vectors of factors. Although they made use of the co-occurrence of morphemes	page=2 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	and words, the context information is lost after chopping the words and feeding the neural network with	page=2 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	morphemes. In our model, we also utilize the co-occurrence information between morphemes, which has	page=2 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	not been investigated before. In csmRNN, Luong et al proposed a hierarchical model considering the	page=2 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	knowledge of both morphological constitutionality and context. The hierarchical structure looks more	page=2 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	sophisticated, but the relatedness of words with morphological similarity are weaken by layers when	page=2 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	combining morphemes into words. In addition, the noise accumulated in the hierarchical structure in	page=2 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	building a word might be propagated to the context layer. In our model, the morphological and contextual	page=2 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	knowledge are combined in parallel, and their contributions to the input vector are decided by a pair of	page=2 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
E-Body	learned tradeoff coefficients.	page=2 xpos=0 ypos=2 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SectionHeader	3 The Morpheme powered CBOW Models	page=2 xpos=0 ypos=3 single-column right-indent font-largest aligned-line longer-tail line-blank-line line-double-space line-space numbered-heading1 above-double-space above-line-space
B-Body	In this section, we introduce the architecture of our proposed neural network model based on the CBOW	page=2 xpos=0 ypos=3 single-column full-justified aligned-line longer-tail line-double-space line-space headchar-capital
I-Body	model. In CBOW (see Figure 1), a sliding window is employed on the train text stream to obtain the train-	page=2 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ing samples. In each sliding window, the model aims to predict the central word using the surrounding	page=2 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	words as the input. Specifically, the input words are represented in the 1-of-v format. In the feed-forward	page=2 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	process, these input words are first mapped into the embedding space by the same weight matrix M , and	page=2 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	then the embedding vectors are summed up to a combined embedding vector. After that, the combined	page=2 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	embedding vector is mapped back to the 1-of-v space by another weight matrix M <sup>0</sup> , and the resulting	page=2 xpos=0 ypos=4 single-column full-justified font-larger aligned-line headchar-lower
I-Body	vector is used to predict the central word after conducting softmax on it. In the back-propagation process,	page=2 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	the prediction errors are propagated back to the network to update the two weight matrices. After the	page=2 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
E-Body	training process converges, the weight matrix M is regarded as the learned word representations.	page=2 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period column-bottom above-double-space above-line-space
Table	__Figure 1__	page=2 xpos=2 ypos=5 Figure-column left-indent right-indent box column-top line-double-space line-space figure-area column-bottom above-double-space above-line-space
B-Caption	Figure 1: The CBOW model.	page=2 xpos=3 ypos=9 single-column centered left-indent right-indent column-top line-double-space line-space string-figure headchar-capital tailchar-period above-blank-line above-double-space above-line-space
Page	143	page=2 xpos=4 ypos=9 single-column centered left-indent right-indent indented-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Body	In our proposed model, we address the challenge of producing high-quality word embeddings for rare	page=3 xpos=0 ypos=0 single-column left-indent page-top headchar-capital
I-Body	words and unknown words by leveraging the three types of co-occurrence information between words	page=3 xpos=0 ypos=0 single-column full-justified hanged-line headchar-lower
E-Body	and morphemes.	page=3 xpos=0 ypos=0 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	On the input side, we segment the words into morphemes and put both the words and the morphemes	page=3 xpos=0 ypos=0 single-column left-indent indented-line longer-tail headchar-capital
I-Body	as input. That is, the vocabulary for the 1-of-v representation contains both words and morphemes.	page=3 xpos=0 ypos=0 single-column full-justified hanged-line headchar-lower tailchar-period
I-Body	As shown in Figure 2, the surrounding words in the sliding window are w <sub>−s</sub> , · · · , w −1 , w 1 , · · · , w s and	page=3 xpos=0 ypos=0 single-column full-justified font-larger aligned-line headchar-capital
I-Body	their corresponding morphemes are m <sub>−s,1</sub> , m −s,2 , · · · , m −s,t <sub>−s</sub> ; · · · ; m −1,1 , m −1,2 , · · · , m −1,t <sub>−1</sub> ; m 1,1 ,	page=3 xpos=0 ypos=1 single-column full-justified font-larger aligned-line headchar-lower tailchar-comma
I-Body	m <sub>1,2</sub> , · · · , m 1,t <sub>1</sub> ; · · · ; m s,1 , m s,2 , · · · , m s,t s , where 2s is the number of the surrounding words and t i is	page=3 xpos=0 ypos=1 single-column full-justified font-larger aligned-line itemization headchar-lower
I-Body	the number of morphemes for w <sub>i</sub> (i = −s, · · · , −1, 1, · · · , s). Note that t i depends on the formation of	page=3 xpos=0 ypos=1 single-column full-justified font-larger aligned-line headchar-lower
I-Body	w <sub>i</sub> so that it may vary from word to word. If a word is also a morpheme, there will be two embedding	page=3 xpos=0 ypos=1 single-column full-justified font-larger aligned-line itemization headchar-lower
I-Body	vectors which are tagged differently. We use v <sub>w</sub> <sub>i</sub> and v m i,j to represent the 1-of-v vectors of word w i and	page=3 xpos=0 ypos=1 single-column full-justified font-larger aligned-line headchar-lower
I-Body	morpheme m <sub>i,j</sub> respectively. On the input side, both the words and their morphemes are mapped into	page=3 xpos=0 ypos=2 single-column full-justified font-larger aligned-line headchar-lower
I-Body	the embedding space by the same weight matrix M , and then the weighted sum v <sub>I</sub> of the combination of	page=3 xpos=0 ypos=2 single-column full-justified font-larger aligned-line headchar-lower
I-Body	word embeddings and the combination of morpheme embeddings is calculate as below,	page=3 xpos=0 ypos=2 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-comma above-double-space above-line-space
B-Equation	X <sup>s</sup> X <sup>s</sup> X <sup>t</sup> i	page=3 xpos=4 ypos=2 single-column left-indent right-indent font-larger indented-line shorter-tail line-double-space line-space headchar-capital
I-Equation	v <sub>I</sub> = φ w · v <sub>w</sub> <sub>i</sub> + φ m · v <sub>m</sub> <sub>i,j</sub> ,	page=3 xpos=2 ypos=2 single-column centered left-indent right-indent font-larger hanged-line longer-tail itemization headchar-lower tailchar-comma above-line-space
I-Equation	i=−s i=−s j=1	page=3 xpos=3 ypos=3 single-column left-indent right-indent font-smallest indented-line shorter-tail line-space headchar-lower
I-Equation	i6 =0 i6 =0	page=3 xpos=4 ypos=3 single-column centered left-indent right-indent font-smallest shorter-tail headchar-lower above-double-space above-line-space
I-Body	where φ <sub>w</sub> and φ m are the tradeoff coefficients between the combination of word embeddings and the	page=3 xpos=0 ypos=3 single-column full-justified font-larger hanged-line longer-tail line-double-space line-space headchar-lower
E-Body	combination of morpheme embeddings.	page=3 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	On the output side, we map the combined embedding vector v <sub>I</sub> back to the 1-of-v space by another	page=3 xpos=0 ypos=3 single-column left-indent font-larger indented-line longer-tail headchar-capital
I-Body	weight matrix M <sup>0</sup> to do the prediction. We have four settings of the structure. In the first setting, we only	page=3 xpos=0 ypos=3 single-column full-justified font-larger hanged-line headchar-lower
I-Body	predict the central word w <sub>0</sub> , and we name the model under this setting as MorphemeCBOW. In the second	page=3 xpos=0 ypos=4 single-column full-justified font-larger aligned-line headchar-lower
I-Body	setting, we predict both the central word w <sub>0</sub> and its morphemes m 0,1 , m 0,2 , · · · , m 0,t <sub>0</sub> , and we name this	page=3 xpos=0 ypos=4 single-column full-justified font-larger aligned-line headchar-lower
I-Body	setting as MorphemeCBOW+. In the above two settings, the tradeoff weights φ <sub>w</sub> and φ m are fixed. If	page=3 xpos=0 ypos=4 single-column full-justified font-larger aligned-line headchar-lower
I-Body	we update the two weights in the learning process of MorphemeCBOW, we will get the third setting and	page=3 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	we name it as MorphemeCBOW*, while updating the two weights in MorphemeCBOW+ yields the forth	page=3 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
E-Body	setting named MorphemeCBOW++ .	page=3 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	Take MorphemeCBOW+ as example, the objective is to maximize the following conditional co-	page=3 xpos=0 ypos=5 single-column left-indent indented-line longer-tail headchar-capital tailchar-hiphen
I-Body	occurrence probability,	page=3 xpos=0 ypos=5 single-column right-indent hanged-line shorter-tail headchar-lower tailchar-comma above-double-space above-line-space
B-Equation	X <sup>t</sup> 0	page=3 xpos=5 ypos=5 single-column left-indent right-indent font-larger indented-line longer-tail line-double-space line-space headchar-capital
I-Equation	log(P (w <sub>0</sub> | {w i }, {m i,j })) + log( P (m 0,j | {w i }, {m i,j })), (1)	page=3 xpos=1 ypos=5 single-column left-indent font-larger hanged-line longer-tail headchar-lower above-line-space
I-Equation	j=1	page=3 xpos=5 ypos=6 single-column left-indent right-indent font-smallest indented-line shorter-tail line-space headchar-lower above-double-space above-line-space
I-Body	where {w <sub>i</sub> }, {m i,j } represent the bag of words and bag of morphemes separately. The conditional prob-	page=3 xpos=0 ypos=6 single-column full-justified font-larger hanged-line longer-tail line-double-space line-space headchar-lower tailchar-hiphen
I-Body	ability in the above formula is defined using the softmax function,	page=3 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-comma above-double-space above-line-space
B-Equation	exp(v <sup>0T</sup> <sub>w</sub> <sub>0</sub> · v I ) exp(v <sub>m</sub> <sup>0T</sup> · v I )	page=3 xpos=2 ypos=6 single-column left-indent right-indent font-largest indented-line longer-tail line-double-space line-space headchar-lower
I-Equation	P (w <sub>0</sub> | {w i }, {m i,j }) = X , P (m <sub>0,j</sub> | {w i }, {m i,j }) = X <sup>0,j</sup> , (2)	page=3 xpos=0 ypos=6 single-column left-indent font-larger hanged-line longer-tail headchar-capital
I-Equation	exp(v <sup>0T</sup> · v <sub>I</sub> ) exp(v <sup>0T</sup> · v <sub>I</sub> )	page=3 xpos=3 ypos=7 single-column left-indent right-indent font-largest indented-line shorter-tail headchar-lower above-line-space
I-Equation	v <sup>0</sup> ∈V <sub>O</sub> v <sup>0</sup> ∈V <sub>O</sub>	page=3 xpos=2 ypos=7 single-column left-indent right-indent font-smallest hanged-line shorter-tail line-space itemization headchar-lower above-double-space above-line-space
I-Body	where V <sub>O</sub> is the set of the output representations for the whole vocabulary; v <sup>0</sup> is used to differentiate with	page=3 xpos=0 ypos=7 single-column full-justified font-largest hanged-line longer-tail line-double-space line-space headchar-lower
E-Body	input representations; and v <sub>w</sub> <sup>0</sup> <sub>0</sub> , v m 0 <sub>0,j</sub> represent the output embedding vectors of w 0 and m 0,j respectively.	page=3 xpos=0 ypos=7 single-column full-justified font-largest aligned-line headchar-lower tailchar-period
B-Body	Usually, the computation cost for Formula (2) is expensive since it is proportional to the vocabulary	page=3 xpos=0 ypos=7 single-column left-indent indented-line headchar-capital
I-Body	size. In our model, we use negative sampling discussed in (Mikolov et al., 2013b) to speed up the	page=3 xpos=0 ypos=8 single-column full-justified hanged-line year headchar-lower
I-Body	computation. Particularly, we random select k negative samples u <sub>1</sub> , u 2 , · · · , u <sub>k</sub> for each prediction target	page=3 xpos=0 ypos=8 single-column full-justified font-larger aligned-line headchar-lower
I-Body	(word or morpheme). By using this technique, Formula (1) can be equally written as,	page=3 xpos=0 ypos=8 single-column right-indent aligned-line shorter-tail tailchar-comma above-double-space above-line-space
B-Equation	X <sup>t</sup> 0 X <sup>k</sup>	page=3 xpos=3 ypos=8 single-column left-indent right-indent font-larger indented-line shorter-tail line-double-space line-space headchar-capital
I-Equation	G(v <sub>I</sub> ) ≡ log σ(v <sup>0T</sup> <sub>w</sub> <sub>0</sub> · v I ) + log σ(v m 0T <sub>0,j</sub> · v I ) + E <sub>u</sub> <sub>i</sub> ∼P n (u) [log σ(−v u <sup>0T</sup> <sub>i</sub> · v I )],	page=3 xpos=0 ypos=8 single-column centered left-indent right-indent font-largest hanged-line longer-tail headchar-capital tailchar-comma
I-Equation	j=1 i=1	page=3 xpos=3 ypos=9 single-column left-indent right-indent font-smallest indented-line shorter-tail headchar-lower
I-Equation	u <sub>i</sub> = 6 w 0	page=3 xpos=5 ypos=9 single-column left-indent right-indent font-smallest indented-line longer-tail itemization headchar-lower
I-Equation	u <sub>i</sub> 6 = ∀m 0,j	page=3 xpos=5 ypos=9 single-column left-indent right-indent font-smallest hanged-line longer-tail itemization headchar-lower above-blank-line above-double-space above-line-space
Page	144	page=3 xpos=4 ypos=9 single-column centered left-indent right-indent hanged-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
I-Body	where σ denotes the logistic function, and P <sub>n</sub> (u) is the vocabulary distribution used to select the negative	page=4 xpos=0 ypos=0 single-column full-justified font-larger page-top headchar-lower
I-Body	samples. P <sub>n</sub> (u) is set as the 3/4rd power of the unigram distribution U (u) <sup>1</sup> . The negative samples should	page=4 xpos=0 ypos=0 single-column full-justified font-largest aligned-line headchar-lower
I-Body	not be the same as any of the prediction targets w <sub>0</sub> and m 0,j (j = 1, · · · , t 0 ). By using negative sampling,	page=4 xpos=0 ypos=0 single-column full-justified font-larger aligned-line headchar-lower tailchar-comma
I-Body	the training time spent on summing up the whole vocabulary in Formula (2) is greatly reduced so that it	page=4 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower
I-Body	becomes linear with the number of the negative samples. Thus, we can calculate the gradient of G(v <sub>I</sub> )	page=4 xpos=0 ypos=0 single-column full-justified font-larger aligned-line headchar-lower
E-Body	as below,	page=4 xpos=0 ypos=0 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-comma above-double-space above-line-space
B-Equation	∂G(v <sub>I</sub> ) =(1 − σ(v 0T <sub>w</sub> <sub>0</sub> · v I )) · ∂(v <sup>0T</sup> ∂v w 0 <sub>I</sub> · v I ) + <sub>j=1</sub> X <sup>t</sup> 0 (1 − σ(v m 0T 0,j · v I )) · ∂(v 0T m ∂v 0,j I · v I )	page=4 xpos=1 ypos=1 single-column left-indent right-indent font-largest indented-line longer-tail line-double-space line-space
I-Equation	∂v <sub>I</sub>	page=4 xpos=1 ypos=1 single-column left-indent right-indent font-larger indented-line shorter-tail above-double-space above-line-space
I-Equation	X <sup>k</sup>	page=4 xpos=2 ypos=1 single-column left-indent right-indent font-larger indented-line longer-tail line-double-space line-space headchar-capital
I-Equation	[σ(v <sup>0T</sup> <sub>u</sub> <sub>i</sub> · v I ) · ∂(v u ∂v <sup>0T</sup> i · <sub>I</sub> v I ) ].	page=4 xpos=3 ypos=1 single-column left-indent right-indent font-largest indented-line longer-tail tailchar-period
I-Equation	−	page=4 xpos=2 ypos=1 single-column left-indent right-indent hanged-line shorter-tail above-line-space
I-Equation	i=1	page=4 xpos=2 ypos=2 single-column left-indent right-indent font-smallest indented-line longer-tail line-space headchar-lower
I-Equation	u <sub>i</sub> = 6 w 0	page=4 xpos=2 ypos=2 single-column left-indent right-indent font-smallest hanged-line longer-tail itemization headchar-lower
I-Equation	u <sub>i</sub> = 6 ∀m 0,j	page=4 xpos=2 ypos=2 single-column left-indent right-indent font-smallest hanged-line longer-tail itemization headchar-lower above-double-space above-line-space
B-Body	In the back-propagation process, the weights in the matrices M and M <sup>0</sup> are updated. When the training	page=4 xpos=0 ypos=2 single-column left-indent font-larger hanged-line longer-tail line-double-space line-space headchar-capital
E-Body	process converges, we take the matrix M as the learned word embeddings and morpheme embeddings.	page=4 xpos=0 ypos=2 single-column right-indent hanged-line shorter-tail headchar-lower tailchar-period column-bottom above-double-space above-line-space
Figure	__Figure 2__	page=4 xpos=0 ypos=3 Figure-column left-indent right-over box column-top line-double-space line-space figure-area column-bottom above-double-space above-line-space
B-Caption	Figure 2: The proposed neural network model.	page=4 xpos=2 ypos=6 single-column centered left-indent right-indent column-top line-double-space line-space string-figure headchar-capital tailchar-period above-blank-line above-double-space above-line-space
B-SectionHeader	4 Experimental Evaluation	page=4 xpos=0 ypos=7 single-column right-indent font-largest hanged-line shorter-tail line-blank-line line-double-space line-space numbered-heading1 above-double-space above-line-space
B-Body	In this section we test the effectiveness of our model in generating high-quality word embeddings. We	page=4 xpos=0 ypos=7 single-column full-justified aligned-line longer-tail line-double-space line-space headchar-capital
I-Body	first introduce the experimental settings, and then we report the results on one analogical reasoning task	page=4 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
E-Body	and several word similarity tasks.	page=4 xpos=0 ypos=8 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-SubsectionHeader	4.1 Datasets	page=4 xpos=0 ypos=8 single-column right-indent aligned-line shorter-tail line-double-space line-space numbered-heading2 above-double-space above-line-space
B-Body	We used two datasets for training: enwiki9 <sup>2</sup> and wiki2010 3 .	page=4 xpos=0 ypos=8 single-column right-indent font-larger aligned-line longer-tail line-double-space line-space year headchar-capital tailchar-period above-double-space above-line-space
B-Footnote	<sup>1</sup> http://www.cs.bgu.ac.il/˜yoavg/publications/negative-sampling.pdf	page=4 xpos=0 ypos=8 single-column left-indent right-indent indented-line longer-tail line-double-space line-space headchar-super
B-Footnote	<sup>2</sup> http://mattmahoney.ent/dc/enwik9.zip	page=4 xpos=0 ypos=9 single-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-super
B-Footnote	<sup>3</sup> http://www.psych.ualberta.ca/˜westburylab/downloads/westburylab.wikicorp.	page=4 xpos=0 ypos=9 single-column left-indent right-indent aligned-line longer-tail headchar-super tailchar-period
I-Footnote	download.html	page=4 xpos=0 ypos=9 single-column right-indent font-smallest hanged-line shorter-tail headchar-lower above-blank-line above-double-space above-line-space
Page	145	page=4 xpos=4 ypos=9 single-column centered left-indent right-indent indented-line longer-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Listitem	• The enwiki9 dataset contains about 123.4 million words. We used Matt Mahoney’s text pre-	page=5 xpos=0 ypos=0 single-column left-indent page-top itemization tailchar-hiphen
I-Listitem	processing script <sup>4</sup> to process the corpus. Thus, we removed all non-Roman characters and mapped	page=5 xpos=0 ypos=0 single-column left-indent font-larger indented-line headchar-lower
I-Listitem	all digits to English words. In addition, words occurred less than 5 times in the training corpus were	page=5 xpos=0 ypos=0 single-column left-indent aligned-line headchar-lower
I-Listitem	discarded. We used the learned word embeddings from enwiki9 to test an analogical reasoning task	page=5 xpos=0 ypos=0 single-column left-indent aligned-line headchar-lower
I-Listitem	described in (Mikolov et al., 2013a).	page=5 xpos=0 ypos=0 single-column left-indent right-indent aligned-line shorter-tail year headchar-lower tailchar-period above-double-space above-line-space
B-Listitem	• The wiki2010 dataset contains about 990 million words. The learned embeddings from this dataset	page=5 xpos=0 ypos=1 single-column left-indent hanged-line longer-tail line-double-space line-space itemization year
I-Listitem	were used on word similarity tasks as it was convenient to compare with the csmRNN model (Luong	page=5 xpos=0 ypos=1 single-column left-indent indented-line headchar-lower
I-Listitem	et al., 2013). We did the same data pre-processing as csmRNN did. That is, we removed all non-	page=5 xpos=0 ypos=1 single-column left-indent aligned-line year headchar-lower tailchar-hiphen
I-Listitem	Roman characters and mapped all digits to zero.	page=5 xpos=0 ypos=1 single-column left-indent right-indent aligned-line shorter-tail headchar-capital tailchar-period above-double-space above-line-space
B-SubsectionHeader	4.2 Settings	page=5 xpos=0 ypos=1 single-column right-indent hanged-line shorter-tail line-double-space line-space numbered-heading2 above-double-space above-line-space
B-Body	In the analogical reasoning task, we used the CBOW model as the baseline. In both CBOW and our	page=5 xpos=0 ypos=2 single-column full-justified aligned-line longer-tail line-double-space line-space headchar-capital
I-Body	proposed model, we set the context window size to be 5, and generated three dimension sizes (100, 200,	page=5 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	and 300) of word embeddings. We used negative sampling (Mikolov et al., 2013b) in the output layer	page=5 xpos=0 ypos=2 single-column full-justified aligned-line year headchar-lower
E-Body	and the number of negative samples is chosen as 3.	page=5 xpos=0 ypos=2 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	In the word similarity tasks, we used the csmRNN model as the baseline. The context window size of	page=5 xpos=0 ypos=2 single-column left-indent indented-line longer-tail headchar-capital
I-Body	our model was set to be 5. To make a fair comparison with the csmRNN model, we conducted the same	page=5 xpos=0 ypos=3 single-column full-justified hanged-line headchar-lower
I-Body	settings in our experiments as csmRNN. First, as csmRNN used the Morfessor (Creutz and Lagus, 2007)	page=5 xpos=0 ypos=3 single-column full-justified aligned-line year headchar-lower
I-Body	method to segment words into morphemes, we also used Morfessor as one of our word segmentation	page=5 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	methods to avoid the influence caused by the segmentation methods. Second, as csmRNN used two	page=5 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	existing embeddings C&W <sup>5</sup> (Collobert et al., 2011) and HSMN 6 (Huang et al., 2012) to initialize the	page=5 xpos=0 ypos=3 single-column full-justified font-larger aligned-line year headchar-lower
I-Body	training process, we also used the two embeddings as the initial weights of M in our experiments. Third,	page=5 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower tailchar-comma
E-Body	we set the dimension of the embedding space to 50 as csmRNN did.	page=5 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	In our model, we employed three methods to segment a word into morphemes. The first method is	page=5 xpos=0 ypos=4 single-column left-indent indented-line longer-tail headchar-capital
I-Body	called Morfessor, which is a public tool implemented based on the minimum descriptions length algo-	page=5 xpos=0 ypos=4 single-column full-justified hanged-line headchar-lower tailchar-hiphen
I-Body	rithm (Creutz and Lagus, 2007). The second method is called Root, which segments a word into roots	page=5 xpos=0 ypos=4 single-column full-justified aligned-line year headchar-lower
I-Body	and affixes according to a predefined list in Longman Dictionaries. The third method is called Syllable,	page=5 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	which is implemented based on the hyphenation tool proposed by Liang (Liang, 1983). Besides, the ar-	page=5 xpos=0 ypos=5 single-column full-justified aligned-line year headchar-lower tailchar-hiphen
I-Body	chitecture of the proposed model can be specified into four types: MorphemeCBOW, MorphemeCBOW*,	page=5 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	MorphemeCBOW+, and MorphemeCBOW++. For the model MorphemeCBOW and MorphemeCBOW+	page=5 xpos=0 ypos=5 single-column full-justified aligned-line headchar-capital
I-Body	with fixed tradeoff coefficients, we set the weights φ <sub>w</sub> and φ m to be 0.8 and 0.2 respectively; while for	page=5 xpos=0 ypos=5 single-column full-justified font-larger aligned-line headchar-lower
I-Body	the other two models with updated tradeoff weights, the weights φ <sub>w</sub> and φ m are initialized as 1. These	page=5 xpos=0 ypos=5 single-column full-justified font-larger aligned-line headchar-lower
E-Body	weight settings are chosen empirically.	page=5 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-SubsectionHeader	4.3 Evaluation Tasks	page=5 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail line-double-space line-space numbered-heading2 above-double-space above-line-space
B-SubsubsectionHeader	4.3.1 Analogical reasoning task	page=5 xpos=0 ypos=6 single-column right-indent aligned-line longer-tail line-double-space line-space numbered-heading3 above-line-space
B-Body	The analogical reasoning task was introduced by Mikolov et al (Mikolov et al., 2013a). All the questions	page=5 xpos=0 ypos=6 single-column full-justified aligned-line longer-tail line-space year headchar-capital
I-Body	are in the form “a is to b is as c is to ?”, denoted as a : b → c : ?. The task consists of 19,544 questions	page=5 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	involving semantic analogies (e.g., England: London → China: Beijing) and syntactic analogies (e.g.,	page=5 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	amazing: amazingly → unfortunate: unfortunately). Suppose that the corresponding vectors are → − a , − → b ,	page=5 xpos=0 ypos=7 single-column full-justified font-largest aligned-line headchar-lower tailchar-comma
I-Body	and − → c , we will answer the question by finding the word with the representation having the maximum	page=5 xpos=0 ypos=7 single-column full-justified font-largest aligned-line headchar-lower
I-Body	→ − → − − →	page=5 xpos=2 ypos=7 single-column left-indent right-indent font-largest indented-line shorter-tail
I-Body	cosine similarity to vector b − a + c , i.e,	page=5 xpos=0 ypos=7 single-column right-indent hanged-line longer-tail headchar-lower tailchar-comma above-double-space above-line-space
B-Equation	→ − − → − → T → −	page=5 xpos=4 ypos=8 single-column left-indent right-indent font-largest indented-line longer-tail line-double-space line-space
I-Equation	x∈V,x6 max = b,x6 = c ( b − a + c ) x	page=5 xpos=3 ypos=8 single-column left-indent right-indent font-largest hanged-line headchar-lower above-double-space above-line-space
I-Body	where V is the vocabulary. Only when the computed word is exactly the answer word in evaluation set	page=5 xpos=0 ypos=8 single-column full-justified hanged-line longer-tail line-double-space line-space headchar-lower
E-Body	can the question be regarded as answered correctly.	page=5 xpos=0 ypos=8 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-Footnote	<sup>4</sup> http://mattmahoney.net/dc/textdata.html	page=5 xpos=0 ypos=9 single-column left-indent right-indent font-smallest indented-line line-double-space line-space headchar-super
B-Footnote	<sup>5</sup> http://ronan.collobert.com/senna/	page=5 xpos=0 ypos=9 single-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-super
B-Footnote	<sup>6</sup> http://ai.stanford.edu/˜ehhuang/	page=5 xpos=0 ypos=9 single-column left-indent right-indent aligned-line shorter-tail headchar-super above-blank-line above-double-space above-line-space
Page	146	page=5 xpos=4 ypos=9 single-column centered left-indent right-indent indented-line longer-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-SubsubsectionHeader	4.3.2 Word similarity task	page=6 xpos=0 ypos=0 single-column right-indent page-top numbered-heading3 above-line-space
B-Body	The word similarity task was tested on five evaluation sets: WS353 (Finkelstein et al., 2002),	page=6 xpos=0 ypos=0 single-column full-justified aligned-line longer-tail line-space year headchar-capital tailchar-comma
I-Body	SCWS* (Huang et al., 2012), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965)	page=6 xpos=0 ypos=0 single-column full-justified aligned-line year headchar-capital
I-Body	and RW (Luong et al., 2013), which contain 353, 1,762, 30, 65 and 2,034 pairs of words respectively.	page=6 xpos=0 ypos=0 single-column full-justified aligned-line year headchar-lower tailchar-period
I-Body	Table 1 shows some statistics about the datasets. Furthermore, the words in WS353, MC, RG are mostly	page=6 xpos=0 ypos=0 single-column full-justified aligned-line string-table headchar-capital
I-Body	frequent words, while SCWS* and RW have much more rare words and unknown words (i.e., unseen	page=6 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower
I-Body	words in the training corpus) than the first three sets. The word distributions of these datasets are shown	page=6 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	in Figure 3, from which we can see that RW contains the largest number of rare and unknown words.	page=6 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower tailchar-period
I-Body	For the unknown words, we segmented them into morphemes, and calculated their word embeddings by	page=6 xpos=0 ypos=1 single-column full-justified aligned-line headchar-capital
I-Body	summing up their corresponding morpheme embeddings. Each word pair in these datasets is associated	page=6 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	with several human judgments on similarity and relatedness on a scale from 0 to 10 or 0 to 4. For ex-	page=6 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ample, (cup, drink) received an average score of 7.25, while (cup, substance) received an average score	page=6 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	of 1.92. To evaluate the quality of the learned word embeddings, we computed Spearman’s ρ correlation	page=6 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
E-Body	between the similarity scores calculated on the learned word embeddings and the human judgments.	page=6 xpos=0 ypos=2 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period column-bottom above-double-space above-line-space
Table	__Figure 3__	page=6 xpos=0 ypos=2 Figure-column left-indent right-indent box column-top line-double-space line-space figure-area column-bottom above-line-space
B-Caption	Figure 3: Word distribution by frequency. Distinct words in each test dataset are grouped according	page=6 xpos=0 ypos=5 single-column full-justified column-top line-space string-figure headchar-capital
E-Caption	to frequencies. The figure shows the percentage of words in each bin.	page=6 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-Caption	Table 1: Statistics on the word similarity evaluation sets.	page=6 xpos=2 ypos=5 single-column centered left-indent right-indent indented-line longer-tail line-blank-line line-double-space line-space string-table headchar-capital tailchar-period column-bottom above-double-space above-line-space
Table	__Table 1__	page=6 xpos=0 ypos=6 Table-column centered left-indent right-indent box column-top line-double-space line-space table-area column-bottom above-blank-line above-double-space above-line-space
B-SubsectionHeader	4.4 Experimental Results	page=6 xpos=0 ypos=7 single-column right-indent column-top line-blank-line line-double-space line-space numbered-heading2 above-double-space above-line-space
B-SubsubsectionHeader	4.4.1 Results on analogical reasoning task	page=6 xpos=0 ypos=7 single-column right-indent aligned-line longer-tail line-double-space line-space numbered-heading3 above-line-space
B-Body	The experimental results on the analogical reasoning task are shown in Table 2, including semantic	page=6 xpos=0 ypos=7 single-column full-justified aligned-line longer-tail line-space headchar-capital
I-Body	accuracy, syntactic accuracy, and total accuracy of all competition settings. Semantic/syntactic accuracy	page=6 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	refers to the number of correct answers over the total number of all semantic/syntactic questions. From	page=6 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	the results, we have the following observations:	page=6 xpos=0 ypos=8 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-colon above-double-space above-line-space
B-Listitem	• In MorphemeCBOW, we used the surrounding words and their morphemes to predict the central	page=6 xpos=0 ypos=8 single-column left-indent indented-line longer-tail line-double-space line-space itemization
I-Listitem	word. The total accuracies are all improved compared with baseline using the three word segmen-	page=6 xpos=0 ypos=8 single-column left-indent indented-line headchar-lower tailchar-hiphen
I-Listitem	tation methods across three different dimensions of the embedding space. Generally, the improve-	page=6 xpos=0 ypos=8 single-column left-indent aligned-line headchar-lower tailchar-hiphen
I-Listitem	ments on semantic accuracies are less than those on syntactic accuracies. The reason is that the	page=6 xpos=0 ypos=9 single-column left-indent aligned-line headchar-lower
I-Listitem	morphological information favors more for the syntactic tasks than the semantic tasks. Further-	page=6 xpos=0 ypos=9 single-column left-indent aligned-line headchar-lower tailchar-hiphen above-blank-line above-double-space above-line-space
Page	147	page=6 xpos=4 ypos=9 single-column centered left-indent right-indent indented-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
I-Listitem	more, the Root method achieved the best among the three segmentation methods, showing that the	page=7 xpos=0 ypos=0 single-column left-indent page-top headchar-lower
I-Listitem	roots and affixes from the dictionary can help produce a high-quality morpheme segmentation tool.	page=7 xpos=0 ypos=0 single-column left-indent aligned-line headchar-lower tailchar-period above-double-space above-line-space
B-Listitem	• In MorphemeCBOW*, we predicted the central word, and updated the tradeoff coefficients in	page=7 xpos=0 ypos=0 single-column left-indent hanged-line line-double-space line-space itemization
I-Listitem	the learning process. We can see that the results are comparable or slightly better than Morphe-	page=7 xpos=0 ypos=0 single-column left-indent indented-line headchar-lower tailchar-hiphen
I-Listitem	meCBOW using the three word segmentation methods across three different dimensions of the	page=7 xpos=0 ypos=0 single-column left-indent aligned-line headchar-lower
I-Listitem	embedding space, showing that updating the tradeoff coefficients may further boost the model per-	page=7 xpos=0 ypos=1 single-column left-indent aligned-line headchar-lower tailchar-hiphen
I-Listitem	formance under some specific settings.	page=7 xpos=0 ypos=1 single-column left-indent right-indent aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-Listitem	• In MorphemeCBOW+, we predicted both the central word and its morphemes. MorphemeCBOW+	page=7 xpos=0 ypos=1 single-column left-indent hanged-line longer-tail line-double-space line-space itemization
I-Listitem	can provide slightly better results compared with MorphemeCBOW and MorphemeCBOW*, indi-	page=7 xpos=0 ypos=1 single-column left-indent indented-line headchar-lower tailchar-hiphen
I-Listitem	cating that putting morphemes (especially roots) in the output layer can do extra help in generating	page=7 xpos=0 ypos=1 single-column left-indent aligned-line headchar-lower
I-Listitem	high-quality word embeddings.	page=7 xpos=0 ypos=2 single-column left-indent right-indent aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-Listitem	• In MorphemeCBOW++, we predicted the central word and its morphemes, and updated the trade-	page=7 xpos=0 ypos=2 single-column left-indent hanged-line longer-tail line-double-space line-space itemization tailchar-hiphen
I-Listitem	off coefficients in the learning process. The performance under all of the three word segmentation	page=7 xpos=0 ypos=2 single-column left-indent indented-line headchar-lower
I-Listitem	methods got further improved compared with MorphemeCBOW+. It tells that the contributions	page=7 xpos=0 ypos=2 single-column left-indent aligned-line headchar-lower
I-Listitem	from words and morphemes are different to the analogical reasoning task. According to our obser-	page=7 xpos=0 ypos=2 single-column left-indent aligned-line headchar-lower tailchar-hiphen
I-Listitem	vations, the weight for words is usually higher than that for morphemes.	page=7 xpos=0 ypos=3 single-column left-indent right-indent aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-Listitem	• By comparing MorphemeCBOW with MorphemeCBOW* as well as MorphemeCBOW+ with Mor-	page=7 xpos=0 ypos=3 single-column left-indent hanged-line longer-tail line-double-space line-space itemization tailchar-hiphen
I-Listitem	phemeCBOW++, we can observe that updating the weights of tradeoff coefficients seem to essen-	page=7 xpos=0 ypos=3 single-column left-indent indented-line headchar-lower tailchar-hiphen
I-Listitem	tially boost syntactic accuracy by trading off a bit of semantic accuracy. As introduced in Section	page=7 xpos=0 ypos=3 single-column left-indent aligned-line headchar-lower
I-Listitem	4.2, in the fixed weight model the ratio of weight of morphemes to the weight of word is 0.25; while	page=7 xpos=0 ypos=3 single-column left-indent aligned-line
I-Listitem	our experiment records show that the averaged ratio are 0.43 if the two weights are updated, mean-	page=7 xpos=0 ypos=4 single-column left-indent aligned-line headchar-lower tailchar-hiphen
I-Listitem	ing that the weight of the combination of morphemes increases and the contribution of the original	page=7 xpos=0 ypos=4 single-column left-indent aligned-line headchar-lower
I-Listitem	word to the final combined embedding decreased. As a result, the syntactic accuracy which largely	page=7 xpos=0 ypos=4 single-column left-indent aligned-line headchar-lower
I-Listitem	reflected in the morphological structure of a word increased, but the semantic accuracy hurts a little.	page=7 xpos=0 ypos=4 single-column left-indent aligned-line headchar-lower tailchar-period above-double-space above-line-space
B-SubsubsectionHeader	4.4.2 Results on word similarity task	page=7 xpos=0 ypos=4 single-column right-indent hanged-line shorter-tail line-double-space line-space numbered-heading3 above-line-space
B-Body	Experimental results on the word similarity tasks are shown in Table 3 <sup>7</sup> ,where the labels of C&W + csm-	page=7 xpos=0 ypos=5 single-column full-justified font-larger aligned-line longer-tail line-space headchar-capital tailchar-hiphen
I-Body	RNN and HSMN + csmRNN mean that using C&W and HSMN to initialize csmRNN model as what had	page=7 xpos=0 ypos=5 single-column full-justified aligned-line headchar-capital
I-Body	been introduced in the paper of Luong et al. In our experiments, the architecture of MorphemeCBOW*	page=7 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	performs the best, so we only show the results related to MorphemeCBOW* in the table. We have the	page=7 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	following observations from the results:	page=7 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-colon above-line-space
B-Listitem	• On WS353, MC, RG, and SCWS*, MorphemeCBOW* performs consistently better than the csm-	page=7 xpos=0 ypos=6 single-column left-indent indented-line longer-tail line-space itemization tailchar-hiphen
I-Listitem	RNN model, showing that our model can achieve better representations for common words.	page=7 xpos=0 ypos=6 single-column left-indent right-indent indented-line shorter-tail headchar-capital tailchar-period above-double-space above-line-space
B-Footnote	<sup>7</sup> csmRNN embeddings are available on http://www-nlp.stanford.edu/˜lmthang/morphoNLM/, Perfor-	page=7 xpos=0 ypos=6 single-column left-indent hanged-line longer-tail line-double-space line-space headchar-super tailchar-hiphen
I-Footnote	mances are tested based on the two embeddings.	page=7 xpos=0 ypos=6 single-column right-indent font-smallest hanged-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-Caption	Table 2: Performance of leveraging morphological information on the analogical reasoning task.	page=7 xpos=0 ypos=7 single-column centered left-indent right-indent indented-line longer-tail line-blank-line line-double-space line-space string-table headchar-capital tailchar-period column-bottom above-double-space above-line-space
Table	__Table 2__	page=7 xpos=-1 ypos=7 Table-column right-over box column-top line-double-space line-space table-area column-bottom above-blank-line above-double-space above-line-space
Page	148	page=7 xpos=4 ypos=9 single-column centered left-indent right-indent column-top line-blank-line line-double-space line-space numeric-only page-bottom
B-Caption	Table 3: Performance of leveraging morphological information on the word similarity task.	page=8 xpos=0 ypos=0 single-column centered left-indent right-indent page-top string-table headchar-capital tailchar-period column-bottom above-double-space above-line-space
Table	__Table 3__	page=8 xpos=0 ypos=0 Table-column left-indent right-over box column-top line-double-space line-space table-area column-bottom above-double-space above-line-space
B-SectionHeader	5 Conclusions and Future Work	page=8 xpos=0 ypos=5 single-column right-indent font-largest column-top line-double-space line-space numbered-heading1 above-double-space above-line-space
B-Body	We proposed a novel neural network model to learn word representations from text. The model can lever-	page=8 xpos=0 ypos=6 single-column full-justified aligned-line longer-tail line-double-space line-space headchar-capital tailchar-hiphen
I-Body	age several types of morphological information to produce high-quality word embeddings, especially for	page=8 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	rare words and unknown words. Empirical experiments on an analogical reasoning task and several word	page=8 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	similarity tasks have shown that the proposed model can generate better word representations compared	page=8 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
E-Body	with several state-of-the-art approaches.	page=8 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	For the future work, we plan to separate words and morphemes into several buckets according to their	page=8 xpos=0 ypos=6 single-column left-indent indented-line longer-tail headchar-capital
I-Body	frequencies. Different buckets will be associated with different coefficients, so that we can tune the	page=8 xpos=0 ypos=7 single-column full-justified hanged-line headchar-lower
I-Body	coefficients to approach even better word embeddings. We also plan to run our model on more training	page=8 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	corpus to obtain the embedding vectors for rare words, especially those new words invented out recently.	page=8 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower tailchar-period
I-Body	These emerging new words usually do not exist in standard training corpus such as Wikipedia, but exists	page=8 xpos=0 ypos=7 single-column full-justified aligned-line headchar-capital
I-Body	in some noisy data such as news articles and web pages. How well our model performs on these new	page=8 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
E-Body	training corpus is an interesting question to explore.	page=8 xpos=0 ypos=8 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
ReferenceHeader	References	page=8 xpos=0 ypos=8 single-column right-indent font-largest aligned-line shorter-tail line-blank-line line-double-space line-space string-reference headchar-capital above-double-space above-line-space
B-Reference	Andrei Alexandrescu and Katrin Kirchhoff. 2006. Factored neural language models. In Proceedings of the Human	page=8 xpos=0 ypos=8 single-column full-justified font-smallest aligned-line longer-tail line-double-space line-space year headchar-capital
I-Reference	Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 1–4, New York City,	page=8 xpos=0 ypos=8 single-column left-indent font-smallest indented-line headchar-capital tailchar-comma
I-Reference	USA, June. Association for Computational Linguistics.	page=8 xpos=0 ypos=9 single-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period above-double-space above-line-space
B-Footnote	<sup>8</sup> 34.36% in the paper of Luong et al; 32.06% in their project website, see note7	page=8 xpos=0 ypos=9 single-column left-indent right-indent font-smallest longer-tail line-double-space line-space headchar-super above-blank-line above-double-space above-line-space
Page	149	page=8 xpos=4 ypos=9 single-column centered left-indent right-indent indented-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Reference	Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language	page=9 xpos=0 ypos=0 single-column full-justified font-smallest page-top year headchar-capital
I-Reference	model. J. Mach. Learn. Res., 3:1137–1155, March.	page=9 xpos=0 ypos=0 single-column left-indent right-indent font-smallest indented-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-Reference	R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks	page=9 xpos=0 ypos=0 single-column full-justified font-smallest hanged-line longer-tail line-double-space line-space year headchar-capital
I-Reference	with multitask learning. In ICML.	page=9 xpos=0 ypos=0 single-column left-indent right-indent font-smallest indented-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-Reference	R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. 2011. Natural language processing	page=9 xpos=0 ypos=0 single-column full-justified font-smallest hanged-line longer-tail line-double-space line-space year headchar-capital
I-Reference	(almost) from scratch. JMLR, 12.	page=9 xpos=0 ypos=0 single-column left-indent right-indent font-smallest indented-line shorter-tail tailchar-period above-double-space above-line-space
B-Reference	M. Creutz and K. Lagus. 2007. Unsupervised models for morpheme segmentation and morphology learning.	page=9 xpos=0 ypos=1 single-column full-justified font-smallest hanged-line longer-tail line-double-space line-space year headchar-capital tailchar-period
I-Reference	TSLP.	page=9 xpos=0 ypos=1 single-column left-indent right-indent font-smallest indented-line shorter-tail headchar-capital tailchar-period above-double-space above-line-space
B-Reference	L. Deng, X. He, and J. Gao. 2013. Deep stacking networks for information retrieval. In ICASSP, pages 3153–	page=9 xpos=0 ypos=1 single-column full-justified font-smallest hanged-line longer-tail line-double-space line-space year headchar-capital
I-Reference	3157.	page=9 xpos=0 ypos=1 single-column left-indent right-indent font-smallest indented-line shorter-tail tailchar-period above-double-space above-line-space
B-Reference	L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2002. Placing search	page=9 xpos=0 ypos=1 single-column full-justified font-smallest hanged-line longer-tail line-double-space line-space year headchar-capital
I-Reference	in context: The concept revisited. In ACM Transactions on Information Systems.	page=9 xpos=0 ypos=2 single-column left-indent right-indent font-smallest indented-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-Reference	X. Glorot, A. Bordes, and Y. Bengio. 2011. Domain adaptation for large-scale sentiment classification: A deep	page=9 xpos=0 ypos=2 single-column full-justified font-smallest hanged-line longer-tail line-double-space line-space year headchar-capital
I-Reference	learning approach. In ICML.	page=9 xpos=0 ypos=2 single-column left-indent right-indent font-smallest indented-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-Reference	Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving Word Represen-	page=9 xpos=0 ypos=2 single-column full-justified font-smallest hanged-line longer-tail line-double-space line-space year headchar-capital tailchar-hiphen
I-Reference	tations via Global Context and Multiple Word Prototypes. In Annual Meeting of the Association for Computa-	page=9 xpos=0 ypos=2 single-column left-indent font-smallest indented-line headchar-lower tailchar-hiphen
I-Reference	tional Linguistics (ACL).	page=9 xpos=0 ypos=3 single-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-Reference	F. M. Liang. 1983. Word hy-phen-a-tion by com-put-er. Technical report.	page=9 xpos=0 ypos=3 single-column right-indent font-smallest hanged-line longer-tail line-double-space line-space year headchar-capital tailchar-period above-double-space above-line-space
B-Reference	M.-T. Luong, R. Socher, and C. D. Manning. 2013. Better word representations with recursive neural networks	page=9 xpos=0 ypos=3 single-column full-justified font-smallest aligned-line longer-tail line-double-space line-space year headchar-capital
I-Reference	for morphology. CoNLL-2013, 104.	page=9 xpos=0 ypos=3 single-column left-indent right-indent font-smallest indented-line shorter-tail year headchar-lower tailchar-period above-double-space above-line-space
B-Reference	Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernocký, and Sanjeev Khudanpur. 2010. Recurrent neural	page=9 xpos=0 ypos=3 single-column full-justified font-smallest hanged-line longer-tail line-double-space line-space year headchar-capital
I-Reference	network based language model. In INTERSPEECH, pages 1045–1048.	page=9 xpos=0 ypos=4 single-column left-indent right-indent font-smallest indented-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-Reference	T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013a. Efficient estimation of word representations in vector space.	page=9 xpos=0 ypos=4 single-column full-justified font-smallest hanged-line longer-tail line-double-space line-space year headchar-capital tailchar-period
I-Reference	ICLR ’13.	page=9 xpos=0 ypos=4 single-column left-indent right-indent font-smallest indented-line shorter-tail headchar-capital tailchar-period above-double-space above-line-space
B-Reference	T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. 2013b. Distributed representations of words and	page=9 xpos=0 ypos=4 single-column full-justified font-smallest hanged-line longer-tail line-double-space line-space year headchar-capital
I-Reference	phrases and their compositionality. In NIPS, pages 3111–3119.	page=9 xpos=0 ypos=4 single-column left-indent right-indent font-smallest indented-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-Reference	T. Mikolov, W.-T. Yih, and G. Zweig. 2013c. Linguistic regularities in continuous space word representations. In	page=9 xpos=0 ypos=5 single-column full-justified font-smallest hanged-line longer-tail line-double-space line-space year headchar-capital
I-Reference	In NAACL-HLT, pages 746–751.	page=9 xpos=0 ypos=5 single-column left-indent right-indent font-smallest indented-line shorter-tail headchar-capital tailchar-period above-double-space above-line-space
B-Reference	T. Mikolov. 2012. Statistical Language Models Based on Neural Networks. Ph.D. thesis, Brno University of	page=9 xpos=0 ypos=5 single-column full-justified font-smallest hanged-line longer-tail line-double-space line-space year headchar-capital
I-Reference	Technology.	page=9 xpos=0 ypos=5 single-column left-indent right-indent font-smallest indented-line shorter-tail headchar-capital tailchar-period above-double-space above-line-space
B-Reference	G.A. Miller and W.G. Charles. 1991. Contextual correlates of semantic similarity. 6(1):1–28.	page=9 xpos=0 ypos=5 single-column right-indent font-smallest hanged-line longer-tail line-double-space line-space year headchar-capital tailchar-period above-double-space above-line-space
B-Reference	A. Mnih and G. E. Hinton. 2008. A scalable hierarchical distributed language model. In NIPS, pages 1081–1088.	page=9 xpos=0 ypos=6 single-column full-justified font-smallest aligned-line longer-tail line-double-space line-space year headchar-capital tailchar-period above-double-space above-line-space
I-Reference	Amr El-Desoky Mousa, Hong-Kwang Jeff Kuo, Lidia Mangu, and Hagen Soltau. 2013. Morpheme-based feature-	page=9 xpos=0 ypos=6 single-column full-justified font-smallest aligned-line line-double-space line-space year headchar-capital tailchar-hiphen
I-Reference	rich language models using deep neural networks for lvcsr of egyptian arabic. In ICASSP, pages 8435–8439.	page=9 xpos=0 ypos=6 single-column left-indent right-indent font-smallest indented-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-Reference	Herbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Commun. ACM,	page=9 xpos=0 ypos=6 single-column full-justified font-smallest hanged-line longer-tail line-double-space line-space year headchar-capital tailchar-comma
I-Reference	8(10):627–633, October.	page=9 xpos=0 ypos=6 single-column left-indent right-indent font-smallest indented-line shorter-tail tailchar-period above-double-space above-line-space
B-Reference	R. Socher, C. C. Lin, A. Y. Ng, and C. D. Manning. 2011. Parsing natural scenes and natural language with	page=9 xpos=0 ypos=7 single-column full-justified font-smallest hanged-line longer-tail line-double-space line-space year headchar-capital
I-Reference	recursive neural networks. In ICML.	page=9 xpos=0 ypos=7 single-column left-indent right-indent font-smallest indented-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-Reference	Henning Sperr, Jan Niehues, and Alex Waibel. 2013. Letter n-gram-based input encoding for continuous space	page=9 xpos=0 ypos=7 single-column full-justified font-smallest hanged-line longer-tail line-double-space line-space year headchar-capital
I-Reference	language models. In Proceedings of the Workshop on Continuous Vector Space Models and their Composition-	page=9 xpos=0 ypos=7 single-column left-indent font-smallest indented-line headchar-lower tailchar-hiphen
I-Reference	ality, pages 30–39, Sofia, Bulgaria, August. Association for Computational Linguistics.	page=9 xpos=0 ypos=7 single-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-Reference	J. P. Turian, L.-A. Ratinov, and Y. Bengio. 2010. Word representations: A simple and general method for semi-	page=9 xpos=0 ypos=8 single-column full-justified font-smallest hanged-line longer-tail line-double-space line-space year headchar-capital tailchar-hiphen
I-Reference	supervised learning. In ACL, pages 384–394.	page=9 xpos=0 ypos=8 single-column left-indent right-indent font-smallest indented-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-Reference	P. D. Turney and P. Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of	page=9 xpos=0 ypos=8 single-column full-justified font-smallest hanged-line longer-tail line-double-space line-space year headchar-capital
I-Reference	Artificial Intelligence Research, 37:141–188.	page=9 xpos=0 ypos=8 single-column left-indent right-indent font-smallest indented-line shorter-tail headchar-capital tailchar-period above-double-space above-line-space
B-Reference	P. D. Turney. 2013. Distributional semantics beyond words: Supervised learning of analogy and paraphrase.	page=9 xpos=0 ypos=8 single-column full-justified font-smallest hanged-line longer-tail line-double-space line-space year headchar-capital tailchar-period
I-Reference	TACL, pages 353–366.	page=9 xpos=0 ypos=9 single-column left-indent right-indent font-smallest indented-line shorter-tail headchar-capital tailchar-period above-double-space above-line-space
B-Reference	J. Weston, S. Bengio, and N. Usunier. Wsabie: Scaling up to large vocabulary image annotation. In IJCAI.	page=9 xpos=0 ypos=9 single-column right-indent font-smallest hanged-line longer-tail line-double-space line-space headchar-capital tailchar-period above-blank-line above-double-space above-line-space
Page	150	page=9 xpos=4 ypos=9 single-column centered left-indent right-indent indented-line shorter-tail line-blank-line line-double-space line-space numeric-only
