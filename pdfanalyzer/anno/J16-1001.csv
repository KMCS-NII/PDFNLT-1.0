Title	Optimization for Statistical Machine	page=0 xpos=0 ypos=0 single-column right-indent font-largest page-top headchar-capital above-line-space
Title	Translation: A Survey	page=0 xpos=0 ypos=0 single-column right-indent font-largest aligned-line shorter-tail line-space headchar-capital above-blank-line above-double-space above-line-space
Author	Graham Neubig <sup>∗</sup>	page=0 xpos=0 ypos=0 single-column right-indent font-largest aligned-line shorter-tail line-blank-line line-double-space line-space headchar-capital
B-Affiliation	Graduate School of Information Science	page=0 xpos=0 ypos=1 single-column right-indent aligned-line longer-tail headchar-capital above-line-space
I-Affiliation	Nara Institute of Science and Technology	page=0 xpos=0 ypos=1 single-column right-indent aligned-line longer-tail line-space headchar-capital above-double-space above-line-space
Author	Taro Watanabe <sup>∗∗</sup>	page=0 xpos=0 ypos=1 single-column right-indent font-largest aligned-line shorter-tail line-double-space line-space headchar-capital
B-Affiliation	Google Inc.	page=0 xpos=0 ypos=2 single-column right-indent aligned-line shorter-tail headchar-capital tailchar-period above-blank-line above-double-space above-line-space
B-Abstract	In statistical machine translation (SMT), the optimization of the system parameters to maximize	page=0 xpos=0 ypos=2 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital above-line-space
I-Abstract	translation accuracy is now a fundamental part of virtually all modern systems. In this article,	page=0 xpos=0 ypos=2 single-column full-justified aligned-line line-space headchar-lower tailchar-comma above-line-space
I-Abstract	we survey 12 years of research on optimization for SMT, from the seminal work on discriminative	page=0 xpos=0 ypos=3 single-column full-justified aligned-line line-space headchar-lower above-line-space
I-Abstract	models (Och and Ney 2002) and minimum error rate training (Och 2003), to the most recent	page=0 xpos=0 ypos=3 single-column full-justified aligned-line line-space year headchar-lower above-line-space
I-Abstract	advances. Starting with a brief introduction to the fundamentals of SMT systems, we follow by	page=0 xpos=0 ypos=3 single-column full-justified aligned-line line-space headchar-lower above-line-space
I-Abstract	covering a wide variety of optimization algorithms for use in both batch and online optimization.	page=0 xpos=0 ypos=3 single-column full-justified aligned-line line-space headchar-lower tailchar-period above-line-space
I-Abstract	Specifically, we discuss losses based on direct error minimization, maximum likelihood, max-	page=0 xpos=0 ypos=3 single-column full-justified aligned-line line-space headchar-capital tailchar-hiphen above-line-space
I-Abstract	imum margin, risk minimization, ranking, and more, along with the appropriate methods for	page=0 xpos=0 ypos=4 single-column full-justified aligned-line line-space headchar-lower above-line-space
I-Abstract	minimizing these losses. We also cover recent topics, including large-scale optimization, non-	page=0 xpos=0 ypos=4 single-column full-justified aligned-line line-space headchar-lower tailchar-hiphen above-line-space
I-Abstract	linear models, domain-dependent optimization, and the effect of MT evaluation measures or	page=0 xpos=0 ypos=4 single-column full-justified aligned-line line-space headchar-lower above-line-space
I-Abstract	search on optimization. Finally, we discuss the current state of affairs in MT optimization,	page=0 xpos=0 ypos=4 single-column full-justified aligned-line line-space headchar-lower tailchar-comma above-line-space
I-Abstract	and point out some unresolved problems that will likely be the target of further research in	page=0 xpos=0 ypos=4 single-column full-justified aligned-line line-space headchar-lower above-line-space
I-Abstract	optimization for MT.	page=0 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail line-space headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SectionHeader	1. Introduction	page=0 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space itemization above-blank-line above-double-space above-line-space
B-Body	Machine translation (MT) has long been both one of the most promising applications	page=0 xpos=0 ypos=5 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	of natural language processing technology and one of the most elusive. However, over	page=0 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	approximately the past decade, huge gains in translation accuracy have been achieved	page=0 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	(Graham et al. 2014), and commercial systems deployed for hundreds of language	page=0 xpos=0 ypos=6 single-column full-justified aligned-line year
I-Body	pairs are being used by hundreds of millions of users. There are many reasons for	page=0 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	these advances in the accuracy and coverage of MT, but among them two particularly	page=0 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	stand out: statistical machine translation (SMT) techniques that make it possible to learn	page=0 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	statistical models from data, and massive increases in the amount of data available to	page=0 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
E-Body	learn SMT models.	page=0 xpos=0 ypos=7 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-Footnote	∗ 8916-5 Takayama-cho, Ikoma, Nara, Japan. E-mail: neubig@is.naist.jp.	page=0 xpos=0 ypos=8 single-column left-indent right-indent font-smallest indented-line longer-tail line-blank-line line-double-space line-space symbol-atmark tailchar-period
I-Footnote	∗∗ 6-10-1 Roppongi, Minato-ku, Tokyo, Japan. E-mail: tarow@google.com.	page=0 xpos=0 ypos=8 single-column right-indent font-smallest hanged-line shorter-tail symbol-atmark tailchar-period
I-Footnote	This work was mostly done while the second author was affiliated with the National Institute	page=0 xpos=0 ypos=8 single-column left-indent right-indent font-smallest indented-line longer-tail headchar-capital
I-Footnote	of Information and Communications Technology, 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto,	page=0 xpos=0 ypos=8 single-column left-indent right-indent font-smallest aligned-line headchar-lower tailchar-comma
I-Footnote	619-0289, Japan.	page=0 xpos=0 ypos=8 single-column left-indent right-indent font-smallest aligned-line shorter-tail tailchar-period above-double-space above-line-space
B-Footer	Submission received: 3 June 2014; revised version received: 18 March 2015; accepted for publication:	page=0 xpos=0 ypos=8 single-column right-indent font-smallest hanged-line longer-tail line-double-space line-space year headchar-capital tailchar-colon
I-Footer	11 October 2015.	page=0 xpos=0 ypos=9 single-column right-indent font-smallest aligned-line shorter-tail numbered-heading1 year tailchar-period above-double-space above-line-space
B-Footer	doi:10.1162/COLI a 00241	page=0 xpos=0 ypos=9 single-column right-indent font-smallest aligned-line longer-tail line-double-space line-space headchar-lower above-blank-line above-double-space above-line-space
B-Footer	© 2016 Association for Computational Linguistics	page=0 xpos=0 ypos=9 single-column right-indent font-smallest aligned-line longer-tail line-blank-line line-double-space line-space year page-bottom
B-Header	Computational Linguistics Volume 42, Number 1	page=1 xpos=0 ypos=0 left-column right-over font-smallest page-top headchar-capital column-bottom above-blank-line above-double-space above-line-space
B-Body	Within the SMT framework, there have been two revolutions in the way we math-	page=1 xpos=0 ypos=0 single-column left-indent column-top line-blank-line line-double-space line-space headchar-capital tailchar-hiphen
I-Body	ematically model the translation process. The first was the pioneering work of Brown	page=1 xpos=0 ypos=0 single-column full-justified hanged-line headchar-lower
I-Body	et al. (1993), who proposed the idea of SMT, and described methods for estimation of the	page=1 xpos=0 ypos=0 single-column full-justified aligned-line year headchar-lower
I-Body	parameters used in translation. In that work, the parameters of a word-based generative	page=1 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	translation model were optimized to maximize the conditional likelihood of the training	page=1 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	corpus. The second major advance in SMT is the discriminative training framework	page=1 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	proposed by Och and Ney (2002) and Och (2003), who propose log-linear models for	page=1 xpos=0 ypos=1 single-column full-justified aligned-line year headchar-lower
I-Body	MT, optimized to maximize either the probability of getting the correct sentence from	page=1 xpos=0 ypos=1 single-column full-justified aligned-line headchar-capital
I-Body	a k-best list of candidates, or to directly achieve the highest accuracy over the entire	page=1 xpos=0 ypos=1 single-column full-justified aligned-line itemization headchar-lower
I-Body	corpus. By describing the scoring function for MT as a flexibly parameterizable log-	page=1 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	linear model, and describing discriminative algorithms to optimize these parameters, it	page=1 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	became possible to think of MT like many other structured prediction problems, such	page=1 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
E-Body	as POS tagging or parsing (Collins 2002).	page=1 xpos=0 ypos=2 single-column right-indent aligned-line shorter-tail year headchar-lower tailchar-period
B-Body	However, within the general framework of structured prediction, MT stands apart	page=1 xpos=0 ypos=2 single-column left-indent indented-line longer-tail headchar-capital
I-Body	in many ways, and as a result requires a number of unique design decisions not neces-	page=1 xpos=0 ypos=3 single-column full-justified hanged-line headchar-lower tailchar-hiphen
I-Body	sary in other frameworks (as summarized in Table 1). The first is the search space that	page=1 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	must be considered. The search space in MT is generally too large to expand exhaus-	page=1 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tively, so it is necessary to decide which subset of all the possible hypotheses should	page=1 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	be used in optimization. In addition, the evaluation of MT accuracy is not straight-	page=1 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	forward, with automatic evaluation measures for MT still being researched to this day.	page=1 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower tailchar-period
I-Body	From the optimization perspective, even once we have chosen an automatic evaluation	page=1 xpos=0 ypos=4 single-column full-justified aligned-line headchar-capital
I-Body	metric, it is not necessarily the case that it can be decomposed for straightforward	page=1 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	integration with structured learning algorithms. Given this evaluation measure, it is	page=1 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	necessary to incorporate it into a loss function to target. The loss function should be	page=1 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	closely related to the final evaluation objective, while allowing for the use of efficient	page=1 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	optimization algorithms. Finally, it is necessary to choose an optimization algorithm.	page=1 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower tailchar-period
I-Body	In many cases it is possible to choose a standard algorithm from other fields, but there	page=1 xpos=0 ypos=5 single-column full-justified aligned-line headchar-capital
I-Body	are also algorithms that have been tailored towards the unique challenges posed by	page=1 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
E-Body	MT.	page=1 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail headchar-capital tailchar-period column-bottom above-blank-line above-double-space above-line-space
B-Caption	Table 1	page=1 xpos=0 ypos=6 left-column right-indent font-smallest column-top line-blank-line line-double-space line-space string-table headchar-capital column-bottom
Table	__Table 1__	page=1 xpos=6 ypos=6 single-column left-indent right-over box column-top table-area
B-Table	A road map of the various elements that affect MT	page=1 xpos=0 ypos=6 single-column right-indent font-smallest hanged-line shorter-tail headchar-capital column-bottom above-double-space above-line-space
I-Table	Which Loss Functions?	page=1 xpos=0 ypos=6 left-column right-indent font-smallest column-top line-double-space line-space headchar-capital
I-Table	Error (§3.1)	page=1 xpos=0 ypos=7 left-column right-indent font-smallest aligned-line shorter-tail headchar-capital
I-Table	Softmax (§3.2)	page=1 xpos=0 ypos=7 left-column right-indent font-smallest aligned-line longer-tail headchar-capital
I-Table	Risk (§3.3)	page=1 xpos=0 ypos=7 left-column right-indent font-smallest aligned-line shorter-tail headchar-capital
I-Table	Margin, Perceptron (§3.4)	page=1 xpos=0 ypos=7 left-column right-indent font-smallest aligned-line longer-tail headchar-capital
I-Table	Ranking (§3.5)	page=1 xpos=0 ypos=7 left-column right-indent font-smallest aligned-line shorter-tail headchar-capital
I-Table	Minimum Squared Error (§3.6)	page=1 xpos=0 ypos=7 left-column right-indent font-smallest aligned-line longer-tail headchar-capital above-blank-line above-double-space above-line-space
I-Table	Which Evaluation Measure?	page=1 xpos=0 ypos=8 left-column right-indent font-smallest aligned-line shorter-tail line-blank-line line-double-space line-space headchar-capital
I-Table	Corpus-level, Sentence Level (§2.5)	page=1 xpos=0 ypos=8 left-column right-indent font-smallest aligned-line longer-tail headchar-capital
I-Table	BLEU and Approximations (§2.5.1, §2.5.2)	page=1 xpos=0 ypos=8 left-column centered right-indent font-smallest aligned-line longer-tail headchar-capital
I-Table	Other Measures (§8.3)	page=1 xpos=0 ypos=8 left-column right-indent font-smallest aligned-line shorter-tail headchar-capital above-double-space above-line-space
I-Table	Other Topics:	page=1 xpos=0 ypos=8 left-column left-indent right-indent font-smallest indented-line shorter-tail line-double-space line-space headchar-capital tailchar-colon column-bottom
I-Table	Which	page=1 xpos=4 ypos=6 right-column right-indent font-smallest column-top headchar-capital
I-Table	Minimum Error Rate Training (§5.1)	page=1 xpos=4 ypos=7 right-column right-indent font-smallest aligned-line longer-tail headchar-capital
I-Table	Gradient-based Methods (§5.2, §6.5)	page=1 xpos=4 ypos=7 right-column right-indent font-smallest aligned-line headchar-capital
I-Table	Margin-based Methods (§5.3)	page=1 xpos=4 ypos=7 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital
I-Table	Linear Regression (§5.4)	page=1 xpos=4 ypos=7 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital
I-Table	Perceptron (§6.2)	page=1 xpos=4 ypos=7 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital
I-Table	MIRA (§6.3)	page=1 xpos=4 ypos=7 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital
I-Table	AROW (§6.4)	page=1 xpos=4 ypos=7 right-column right-indent font-smallest aligned-line longer-tail headchar-capital above-double-space above-line-space
I-Table	Which Hypotheses to Target?	page=1 xpos=4 ypos=8 right-column right-indent font-smallest aligned-line longer-tail line-double-space line-space headchar-capital
I-Table	k-best vs. Lattice vs. Forest (§2.4)	page=1 xpos=4 ypos=8 right-column right-indent font-smallest aligned-line longer-tail headchar-lower
I-Table	Merged k-bests (§5)	page=1 xpos=4 ypos=8 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital
I-Table	Forced Decoding (§2.4), Oracles (§4)	page=1 xpos=4 ypos=8 right-column right-indent font-smallest aligned-line longer-tail headchar-capital column-bottom above-blank-line above-double-space above-line-space
I-Table	Large Data Sets (§7), Non-linear Models (§8.1),	page=1 xpos=0 ypos=9 single-column left-indent right-indent font-smallest column-top line-blank-line line-double-space line-space headchar-capital tailchar-comma
I-Table	Domain Adaptation (§8.2), Search and Optimization (§8.4)	page=1 xpos=0 ypos=9 single-column left-indent right-indent font-smallest aligned-line longer-tail headchar-capital column-bottom above-blank-line above-double-space above-line-space
Page	2	page=1 xpos=0 ypos=9 left-column right-indent column-top line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Neubig and Watanabe Optimization for Statistical Machine Translation	page=2 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
B-Body	In this article, we survey the state of the art in machine translation optimization	page=2 xpos=0 ypos=0 single-column left-indent indented-line line-blank-line line-double-space line-space headchar-capital
I-Body	in a comprehensive and systematic fashion, covering a wide variety of topics, with a	page=2 xpos=0 ypos=0 single-column full-justified hanged-line headchar-lower
I-Body	unified set of terminology. In Section 2, we first provide definitions of the problem of	page=2 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower
I-Body	machine translation, describe briefly how models are built, how features are defined,	page=2 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	and how translations are evaluated, and finally define the optimization setting. In	page=2 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	Section 3, we next describe a variety of loss functions that have been targeted in machine	page=2 xpos=0 ypos=1 single-column full-justified aligned-line headchar-capital
I-Body	translation optimization. In Section 4, we explain the selection of oracle translations,	page=2 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	a non-trivial process that directly affects the optimization results. In Section 5, we	page=2 xpos=0 ypos=1 single-column full-justified aligned-line itemization headchar-lower
I-Body	describe batch optimization algorithms, starting with the popular minimum error rate	page=2 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	training, and continuing with other approaches using likelihood, margin, rank loss, or	page=2 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	risk as objectives. In Section 6, we describe online learning algorithms, first explaining	page=2 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	the relationship between corpus-level optimization and sentence-level optimization,	page=2 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	and then moving on to algorithms based on perceptron, margin, or likelihood-based	page=2 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	objectives. In Section 7, we describe the recent advances in scaling training of MT	page=2 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	systems up to large amounts of data through parallel computing, and in Section 8, we	page=2 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	cover a number of other topics in MT optimization such as non-linear models, domain	page=2 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	adaptation, and the relationship between MT evaluation and optimization. Finally, we	page=2 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	conclude in Section 9, overviewing the methods described, making a brief note about	page=2 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	which methods see the most use in actual systems, and outlining some of the unsolved	page=2 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
E-Body	problems in the optimization of MT systems.	page=2 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	2. Machine Translation Preliminaries and Definitions	page=2 xpos=0 ypos=4 single-column right-indent aligned-line longer-tail line-blank-line line-double-space line-space itemization above-blank-line above-double-space above-line-space
B-Body	Before delving into the details of actual optimization algorithms, we first introduce pre-	page=2 xpos=0 ypos=4 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital tailchar-hiphen
I-Body	liminaries and definitions regarding MT in general and the MT optimization problem in	page=2 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	particular. We focus mainly on the aspects of MT that are relevant to optimization, and	page=2 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
E-Body	readers may refer to Koehn (2010) or Lopez (2008) for more details about MT in general.	page=2 xpos=0 ypos=5 single-column full-justified aligned-line year headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	2.1 Machine Translation	page=2 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	Machine translation is the problem of automatically translating from one natural lan-	page=2 xpos=0 ypos=5 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital tailchar-hiphen
I-Body	guage to another. Formally, we define this problem by specifying F to be the collection	page=2 xpos=0 ypos=6 single-column full-justified font-larger aligned-line headchar-lower
I-Body	of all source sentences to be translated, f ∈ F as one of the sentences, and E ( f ) as the	page=2 xpos=0 ypos=6 single-column full-justified font-larger aligned-line headchar-lower
I-Body	collection of all possible target language sentences that can be obtained by translating f .	page=2 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower tailchar-period
I-Body	Machine translation systems perform this translation process by dividing the translation	page=2 xpos=0 ypos=6 single-column full-justified aligned-line headchar-capital
I-Body	of a full sentence into the translation and recombination of smaller parts, which are	page=2 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
E-Body	represented as hidden variables, which together form a derivation.	page=2 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	For example, in phrase-based translation (Koehn, Och, and Marcu 2003), the hidden	page=2 xpos=0 ypos=7 single-column left-indent indented-line longer-tail year headchar-capital
I-Body	variables will be the alignment between the phrases of the source and target sentences,	page=2 xpos=0 ypos=7 single-column full-justified hanged-line headchar-lower tailchar-comma
I-Body	and in tree-based translation models (Yamada and Knight 2001; Chiang 2007), the	page=2 xpos=0 ypos=7 single-column full-justified aligned-line year headchar-lower
I-Body	hidden variables will represent the latent tree structure used to generate the translation.	page=2 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower tailchar-period
I-Body	We will define D( f ) to be the space of possible derivations that can be acquired from	page=2 xpos=0 ypos=7 single-column full-justified font-larger aligned-line headchar-capital
I-Body	source sentence f , and d ∈ D( f ) to be one of those derivations. Any particular deriva-	page=2 xpos=0 ypos=8 single-column full-justified font-larger aligned-line headchar-lower tailchar-hiphen
I-Body	tion d will correspond to exactly one e ∈ E ( f ), although the opposite is not true (the	page=2 xpos=0 ypos=8 single-column full-justified font-larger aligned-line headchar-lower
I-Body	derivation uniquely determines the translation, but there can be multiple derivations	page=2 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	corresponding to a particular translation). We also define tuple h e, d i consisting of a	page=2 xpos=0 ypos=8 single-column full-justified font-larger aligned-line headchar-lower
I-Body	target sentence and its corresponding derivation, and T ( f ) ⊆ E ( f ) × D( f ) as the set of	page=2 xpos=0 ypos=8 single-column full-justified font-larger aligned-line headchar-lower
E-Body	all of these tuples.	page=2 xpos=0 ypos=8 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	Because the set of all possible translations E ( f ) will contain both good and	page=2 xpos=0 ypos=9 single-column left-indent font-larger indented-line longer-tail headchar-capital
I-Body	bad translations, it is necessary to have a method to identify and output the good	page=2 xpos=0 ypos=9 single-column full-justified hanged-line headchar-lower above-blank-line above-double-space above-line-space
Page	3	page=2 xpos=9 ypos=9 single-column left-indent indented-line line-blank-line line-double-space line-space numeric-only page-bottom
I-Body	Computational Linguistics Volume 42, Number 1	page=3 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Body	translations. In order to do so, in machine translation it is common to define a linear	page=3 xpos=0 ypos=0 single-column full-justified aligned-line line-blank-line line-double-space line-space headchar-lower
I-Body	model that determines the score of each translation candidate. In this linear model	page=3 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower
I-Body	we first define an M-dimensional feature vector for each output and its derivation as	page=3 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower
I-Body	h( f , e, d) : F × E × D → R <sup>M</sup> . For each feature, we also define a corresponding weight,	page=3 xpos=0 ypos=1 single-column full-justified font-largest aligned-line headchar-lower tailchar-comma
I-Body	resulting in an M-dimensional weight vector w ∈ R <sup>M</sup> . Based on these feature and	page=3 xpos=0 ypos=1 single-column full-justified font-largest aligned-line headchar-lower
I-Body	weight vectors, we proceed to define the problem of selecting the best h e, d i as the	page=3 xpos=0 ypos=1 single-column full-justified font-larger aligned-line headchar-lower
I-Body	following maximization problem	page=3 xpos=0 ypos=1 single-column right-indent aligned-line shorter-tail headchar-lower above-double-space above-line-space
B-Equation	h ê, d̂ i = arg max w <sup>></sup> h( f , e, d) (1)	page=3 xpos=3 ypos=1 single-column left-indent font-largest indented-line longer-tail line-double-space line-space itemization headchar-lower
I-Equation	h e , d i∈ T ( f )	page=3 xpos=4 ypos=2 single-column left-indent right-indent font-smallest indented-line shorter-tail itemization headchar-lower above-blank-line above-double-space above-line-space
I-Body	where the dot product of the parameters and features is equivalent to the score assigned	page=3 xpos=0 ypos=2 single-column full-justified hanged-line longer-tail line-blank-line line-double-space line-space headchar-lower
E-Body	to a particular translation.	page=3 xpos=0 ypos=2 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	The optimization problem that we will be surveying in this article is generally	page=3 xpos=0 ypos=2 single-column left-indent indented-line longer-tail headchar-capital
I-Body	concerned with finding the most effective weight vector w from the set of possible	page=3 xpos=0 ypos=3 single-column full-justified hanged-line headchar-lower
I-Body	weight vectors R <sup>M</sup> . 1 Optimization is also widely called tuning in the SMT literature. In	page=3 xpos=0 ypos=3 single-column full-justified font-largest aligned-line headchar-lower
I-Body	addition, because of the exponentially large number of possible translations in E ( f ) that	page=3 xpos=0 ypos=3 single-column full-justified font-larger aligned-line headchar-lower
I-Body	must be considered, it is necessary to take advantage of the problem structure, making	page=3 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
E-Body	MT optimization an instance of structured learning.	page=3 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail headchar-capital tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	2.2 Model Construction	page=3 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	The first step of creating a machine translation system is model construction, in which	page=3 xpos=0 ypos=4 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	translation models (TMs) are extracted from a large parallel corpus. The TM is usually	page=3 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	created by first aligning the parallel text (Och and Ney 2003), using this text to extract	page=3 xpos=0 ypos=4 single-column full-justified aligned-line year headchar-lower
I-Body	multi-word phrase pairs or synchronous grammar rules (Koehn, Och, and Marcu 2003;	page=3 xpos=0 ypos=5 single-column full-justified aligned-line year headchar-lower tailchar-semicolon
I-Body	Chiang 2007), and scoring these rules according to several features explained in more	page=3 xpos=0 ypos=5 single-column full-justified aligned-line year headchar-capital
I-Body	detail in Section 2.3. The construction of the TM is generally performed first in a manner	page=3 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	that does not directly consider the optimization of translation accuracy, followed by an	page=3 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	optimization step that explicitly considers the accuracy achieved by the system. <sup>2</sup> In this	page=3 xpos=0 ypos=5 single-column full-justified font-largest aligned-line headchar-lower
I-Body	survey, we focus on the optimization step, and thus do not cover elements of model	page=3 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	construction that do not directly optimize an objective function related to translation	page=3 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
E-Body	accuracy, but interested readers can reference Koehn (2010) for more details.	page=3 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail year headchar-lower tailchar-period
B-Body	In the context of this article, however, the TM is particularly important in the role	page=3 xpos=0 ypos=6 single-column left-indent indented-line longer-tail headchar-capital
I-Body	it plays in defining our derivation space D( f ). For example, in the case of phrase-based	page=3 xpos=0 ypos=6 single-column full-justified font-larger hanged-line headchar-lower
I-Body	translation, only phrase pairs included in the TM will be expanded during the process	page=3 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
E-Body	of searching for the best translation (explained in Section 2.4).	page=3 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	This has major implications from the point of view of optimization, the most impor-	page=3 xpos=0 ypos=7 single-column left-indent indented-line longer-tail headchar-capital tailchar-hiphen
I-Body	tant of which being that we must use separate data for training the TM and optimizing	page=3 xpos=0 ypos=7 single-column full-justified hanged-line headchar-lower
I-Body	the parameters w. The reason for this lies in the fact that the TM is constructed in such a	page=3 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	way that allows it to “memorize” long multi-word phrases included in the training data.	page=3 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower tailchar-period
I-Body	Using the same data to train the model parameters will result in overfitting, learning	page=3 xpos=0 ypos=7 single-column full-justified aligned-line headchar-capital
I-Body	parameters that heavily favor using these memorized multi-word phrases, which will	page=3 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
E-Body	not be present in a separate test set.	page=3 xpos=0 ypos=8 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-Footnote	1 It should be noted that although most work on MT optimization is concerned with linear models (and	page=3 xpos=0 ypos=8 single-column left-indent right-indent font-smallest indented-line longer-tail line-blank-line line-double-space line-space numbered-heading1
I-Footnote	thus we will spend the majority of this article discussing optimization of these models), optimization	page=3 xpos=0 ypos=8 single-column left-indent right-indent font-smallest indented-line shorter-tail headchar-lower
I-Footnote	using non-linear models is also possible, and is discussed in Section 8.1.	page=3 xpos=0 ypos=9 single-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Footnote	2 It should also be noted there have been a few recent attempts to jointly perform rule extraction and	page=3 xpos=0 ypos=9 single-column left-indent right-indent font-smallest hanged-line longer-tail numbered-heading1
I-Footnote	optimization, doing away with this two-step process (Xiao and Xiong 2013).	page=3 xpos=0 ypos=9 single-column left-indent right-indent font-smallest indented-line shorter-tail year headchar-lower tailchar-period above-blank-line above-double-space above-line-space
Page	4	page=3 xpos=0 ypos=9 single-column right-indent hanged-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Neubig and Watanabe Optimization for Statistical Machine Translation	page=4 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
B-Body	The traditional way to solve this problem is to train the TM on a large parallel	page=4 xpos=0 ypos=0 single-column left-indent indented-line line-blank-line line-double-space line-space headchar-capital
I-Body	corpus on the order of hundreds of thousands to tens of millions of sentences, then	page=4 xpos=0 ypos=0 single-column full-justified hanged-line headchar-lower
I-Body	perform optimization of parameters on a separate set of data consisting of around one	page=4 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower
I-Body	thousand sentences, often called the development set. When learning the weights for	page=4 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	larger feature sets, however, a smaller development set is often not sufficient, and it is	page=4 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	common to perform cross-validation, holding out some larger portion of the training	page=4 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	set for parameter optimization. It is also possible to perform leaving-one-out training,	page=4 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	where counts of rules extracted from a particular sentence are subtracted from the	page=4 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
E-Body	model before translating the sentence (Wuebker, Mauser, and Ney 2010).	page=4 xpos=0 ypos=1 single-column right-indent aligned-line shorter-tail year headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	2.3 Features for Machine Translation	page=4 xpos=0 ypos=2 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numbered-heading2 above-double-space above-line-space
B-Body	Given this overall formulation of MT, the features h( f , e, d) that we choose to use to	page=4 xpos=0 ypos=2 single-column full-justified aligned-line longer-tail line-double-space line-space headchar-capital
I-Body	represent each translation hypothesis are of great importance. In particular, with regard	page=4 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	to optimization, there are two important distinctions between types of features: local vs.	page=4 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower tailchar-period
E-Body	non-local, and dense vs. sparse.	page=4 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	With regard to the first distinction, local features, such as phrase translation prob-	page=4 xpos=0 ypos=3 single-column left-indent indented-line longer-tail headchar-capital tailchar-hiphen
I-Body	abilities, do not require additional contexts from other partial derivations, and they are	page=4 xpos=0 ypos=3 single-column full-justified hanged-line headchar-lower
I-Body	computed independently from one another. On the other hand, when features for a	page=4 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	particular phrase pair or synchronous rule cannot be computed independently from	page=4 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	other pairs, they are called non-local features. This distinction is important, as local	page=4 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	features will not result in an increase in the size of the search space, whereas non-local	page=4 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
E-Body	features have the potential to make search more difficult.	page=4 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	The second distinction is between dense features, which define a small number of	page=4 xpos=0 ypos=4 single-column left-indent indented-line longer-tail headchar-capital
I-Body	highly informative feature functions, and sparse features, which define a large number	page=4 xpos=0 ypos=4 single-column full-justified hanged-line headchar-lower
I-Body	of less informative feature functions. Dense features are generally easier to optimize,	page=4 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	both from a computational point of view because the smaller number of features re-	page=4 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	duces computational and memory requirements, and because the smaller number of	page=4 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	parameters reduces the risk of overfitting. On the other hand, sparse features allow	page=4 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	for more flexibility, as their parameters can be directly optimized to increase translation	page=4 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	accuracy, so if optimization is performed well they have the potential to greatly increase	page=4 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	translation accuracy. The remainder of this section describes some of the widely used	page=4 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
E-Body	features in more detail.	page=4 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-Body	2.3.1 Dense Features. Dense features, which are generally continuously valued and	page=4 xpos=0 ypos=6 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space numbered-heading3
I-Body	present in nearly all translation hypotheses, are used in the majority of machine trans-	page=4 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	lation systems. The most fundamental set of dense features are phrase/rule translation	page=4 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	probabilities or relative frequencies in which the log of sentence-wise probability	page=4 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	distributions p( f | e) and p(e | f ), are split into the sum of phrase or rule log probabilities	page=4 xpos=0 ypos=7 single-column right-indent font-larger aligned-line headchar-lower above-double-space above-line-space
B-Equation	h <sub>φ</sub> ( f , e, d) = <sup>X</sup> log p φ ( α|β ), h <sub>φ</sub> 0 ( f , e, d) = P <sub>hα</sub> <sub>,</sub> βi∈ d log p φ 0 ( β|α ) (2)	page=4 xpos=0 ypos=7 single-column left-indent font-largest indented-line line-double-space line-space itemization headchar-lower
I-Equation	hα , βi∈ d	page=4 xpos=2 ypos=8 single-column left-indent right-indent font-smallest indented-line shorter-tail headchar-lower above-blank-line above-double-space above-line-space
B-Body	Here α and β are the source and target sides of a phrase pair or rule. These features are	page=4 xpos=0 ypos=8 single-column full-justified hanged-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	estimated using counts of each phrase derived from the training corpus as follows:	page=4 xpos=0 ypos=8 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-colon above-double-space above-line-space
B-Equation	p <sub>φ</sub> ( α|β ) = <sub>P</sub> <sub>α</sub> <sup>count(</sup> 0 count( α , α β 0 ) , β ) , p <sub>φ</sub> 0 ( β|α ) = <sub>P</sub> <sub>β</sub> <sup>count(</sup> 0 count( α , α β , ) β 0 )	page=4 xpos=1 ypos=9 single-column centered left-indent right-indent font-largest indented-line shorter-tail line-double-space line-space itemization headchar-lower
I-Equation	(3)	page=4 xpos=9 ypos=9 single-column left-indent indented-line longer-tail above-blank-line above-double-space above-line-space
Page	5	page=4 xpos=9 ypos=9 single-column left-indent indented-line line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Computational Linguistics Volume 42, Number 1	page=5 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
B-Body	In addition, it is also common to use lexical weighting, which estimates parameters	page=5 xpos=0 ypos=0 single-column full-justified aligned-line line-blank-line line-double-space line-space headchar-capital
I-Body	for each phrase pair or rule by further decomposing them into word-wise probabilities	page=5 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower
I-Body	(Koehn, Och, and Marcu 2003). This helps more accurately estimate the reliability of	page=5 xpos=0 ypos=0 single-column full-justified aligned-line year
I-Body	phrase pairs or rules that have low counts. It should be noted that all of these features	page=5 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
E-Body	can be calculated directly from the rules themselves, and are thus local features.	page=5 xpos=0 ypos=1 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	Another set of important features are language models (LMs), which capture the	page=5 xpos=0 ypos=1 single-column left-indent indented-line longer-tail headchar-capital
I-Body	fluency of translation e, and are usually modeled by n-grams	page=5 xpos=0 ypos=1 single-column right-indent hanged-line shorter-tail headchar-lower above-blank-line above-double-space above-line-space
B-Equation	| e |	page=5 xpos=4 ypos=1 single-column left-indent right-indent font-smallest indented-line shorter-tail line-blank-line line-double-space line-space
I-Equation	h <sub>lm</sub> ( f , e, d) = <sup>X</sup> log p lm (e i | e i <sub>i</sub> − − 1 n + <sub>1</sub> ) (4)	page=5 xpos=3 ypos=2 single-column left-indent font-largest hanged-line longer-tail itemization headchar-lower
I-Equation	i = 1	page=5 xpos=4 ypos=2 single-column left-indent right-indent font-smallest indented-line shorter-tail itemization headchar-lower above-double-space above-line-space
B-Body	Note that the n-gram LM is computed over e regardless of the boundaries of phrase	page=5 xpos=0 ypos=2 single-column full-justified hanged-line longer-tail line-double-space line-space headchar-capital
E-Body	pairs or rules in the derivation, and is thus a non-local feature.	page=5 xpos=0 ypos=2 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	The n-gram language model assigns higher penalties for longer translations, and	page=5 xpos=0 ypos=3 single-column left-indent indented-line longer-tail headchar-capital
I-Body	it is common to add a word penalty feature that measures the length of translation	page=5 xpos=0 ypos=3 single-column full-justified hanged-line headchar-lower
I-Body	e to compensate for this. Similarly, phrase penalty or rule penalty features express	page=5 xpos=0 ypos=3 single-column full-justified aligned-line itemization headchar-lower
I-Body	the trade-off between longer or shorter derivations. There exist other features that are	page=5 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	dependent on the underlying MT system model. Phrase-based MT heavily relies on	page=5 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	the distortion probabilities that are computed by the distance on the source side of	page=5 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	target-adjacent phrase pairs. More refined lexicalized reordering models estimate the	page=5 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	parameters from the training data based on the relative distance of two phrase pairs	page=5 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
E-Body	(Tillman 2004; Galley and Manning 2008).	page=5 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail year tailchar-period above-blank-line above-double-space above-line-space
B-Body	2.3.2 Sparse features. Although dense features form the foundation of most SMT systems,	page=5 xpos=0 ypos=4 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space numbered-heading3 tailchar-comma
I-Body	in recent years the ability to define richer feature sets and directly optimize the system	page=5 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	using rich features has been shown to allow for significant increases in accuracy. On the	page=5 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	other hand, large and sparse feature sets make the MT optimization problem signifi-	page=5 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	cantly harder, and many of the optimization methods we will cover in the rest of this	page=5 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
E-Body	survey are aimed at optimizing rich feature sets.	page=5 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	The first variety of sparse features that we can think of are phrase features or	page=5 xpos=0 ypos=5 single-column left-indent indented-line longer-tail headchar-capital
I-Body	rule features, which count the occurrence of every phrase or rule. Of course, it is only	page=5 xpos=0 ypos=6 single-column full-justified hanged-line headchar-lower
I-Body	possible to learn parameters for a translation rule if it exists in the training data used in	page=5 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	optimization, so when using a smaller data set for optimization it is difficult to robustly	page=5 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	learn these features. Chiang, Knight, and Wang (2009) have noted that this problem can	page=5 xpos=0 ypos=6 single-column full-justified aligned-line year headchar-lower
I-Body	be alleviated by only selecting and optimizing the more frequent of the sparse features.	page=5 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower tailchar-period
I-Body	Simianer, Riezler, and Dyer (2012) also propose features using the “shape” of translation	page=5 xpos=0 ypos=6 single-column full-justified aligned-line year headchar-capital
E-Body	rules, transforming a rule	page=5 xpos=0 ypos=7 single-column right-indent aligned-line shorter-tail headchar-lower above-double-space above-line-space
B-Equation	X → h ne X <sub>1</sub> pas, did not X 1 i (5)	page=5 xpos=3 ypos=7 single-column left-indent font-largest indented-line longer-tail line-double-space line-space headchar-capital above-double-space above-line-space
I-Body	into a string simply indicating whether each word is a terminal (T) or non-terminal (N)	page=5 xpos=0 ypos=7 single-column full-justified hanged-line line-double-space line-space headchar-lower above-double-space above-line-space
B-Equation	N → h T N T, T T N i (6)	page=5 xpos=3 ypos=8 single-column left-indent font-larger indented-line line-double-space line-space headchar-capital above-blank-line above-double-space above-line-space
B-Body	Count-based features can also be extended to cover other features of the translation,	page=5 xpos=0 ypos=8 single-column full-justified hanged-line line-blank-line line-double-space line-space headchar-capital tailchar-comma
I-Body	such as phrase or rule bigrams, indicating which phrases or rules tend to be used	page=5 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
E-Body	together (Simianer, Riezler, and Dyer 2012).	page=5 xpos=0 ypos=8 single-column right-indent aligned-line shorter-tail year headchar-lower tailchar-period
B-Body	Another alternative for the creation of features that are sparse, but less sparse than	page=5 xpos=0 ypos=9 single-column left-indent indented-line longer-tail headchar-capital
I-Body	features of phrases or rules, are lexical features (Watanabe et al. 2007). Lexical features,	page=5 xpos=0 ypos=9 single-column full-justified hanged-line year headchar-lower tailchar-comma above-blank-line above-double-space above-line-space
Page	6	page=5 xpos=0 ypos=9 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Neubig and Watanabe Optimization for Statistical Machine Translation	page=6 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Body	similar to lexical weighting, focus on the correspondence between the individual words	page=6 xpos=0 ypos=0 single-column full-justified aligned-line line-blank-line line-double-space line-space headchar-lower
I-Body	that are included in a phrase or rule. The simplest variety of lexical features remembers	page=6 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower
I-Body	which source words f are aligned with which target words e, and fires a feature for	page=6 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower
I-Body	each pair. It is also possible to condition lexical features on the surrounding context	page=6 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	in the source language (Chiang, Knight, and Wang 2009; Xiao et al. 2011), fire features	page=6 xpos=0 ypos=1 single-column full-justified aligned-line year headchar-lower
I-Body	between every pair of words in the source or target sentences (Watanabe et al. 2007),	page=6 xpos=0 ypos=1 single-column full-justified aligned-line year headchar-lower tailchar-comma
I-Body	or integrate bigrams on the target side (Watanabe et al. 2007). Of these, the former two	page=6 xpos=0 ypos=1 single-column full-justified aligned-line year headchar-lower
I-Body	can be calculated from source and local target context, but target bigrams require target	page=6 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
E-Body	bigram context and are thus non-local features.	page=6 xpos=0 ypos=1 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	One final variety of features that has proven useful is syntax-based features	page=6 xpos=0 ypos=2 single-column left-indent indented-line longer-tail headchar-capital
I-Body	(Blunsom and Osborne 2008; Marton and Resnik 2008). In particular, phrase-based and	page=6 xpos=0 ypos=2 single-column full-justified hanged-line year
I-Body	hierarchical phrase-based translations do not directly consider syntax (in the linguistic	page=6 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	sense) in the construction of the models, so introducing this information in the form of	page=6 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	features has a potential for benefit. One way to introduce this information is to parse	page=6 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	the input sentence before translation, and use the information in the parse tree in the	page=6 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	calculation of features. For example, we can count the number of times a phrase or	page=6 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	translation rule matches, or partially matches (Marton and Resnik 2008), a span with	page=6 xpos=0 ypos=3 single-column full-justified aligned-line year headchar-lower
I-Body	a particular label, based on the assumption that rules that match a syntactic span are	page=6 xpos=0 ypos=3 single-column full-justified aligned-line itemization headchar-lower
E-Body	more likely to be syntactically reasonable.	page=6 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-Body	2.3.3 Summary features. Although sparse features are useful, training of sparse features	page=6 xpos=0 ypos=4 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space numbered-heading3
I-Body	is an extremely difficult optimization problem, and at this point there is still no method	page=6 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	that has been widely demonstrated as being able to robustly estimate the parameters of	page=6 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	millions of features. Because of this, a third approach of first training the parameters of	page=6 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	sparse features, then condensing the sparse features into dense features and performing	page=6 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	one more optimization pass (potentially with a different algorithm), has been widely	page=6 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	used in a large number of research papers and systems (Dyer et al. 2009; He and Deng	page=6 xpos=0 ypos=5 single-column full-justified aligned-line year headchar-lower
I-Body	2012; Flanigan, Dyer, and Carbonell 2013; Setiawan and Zhou 2013). A dense feature	page=6 xpos=0 ypos=5 single-column full-justified aligned-line year
I-Body	created from a large group of sparse features and their weights is generally called a	page=6 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	summary feature, and can be expressed as follows	page=6 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail headchar-lower above-double-space above-line-space
B-Equation	h <sub>sum</sub> ( f , e, d) = w <sup>></sup> <sub>sparse</sub> h sparse ( f , e, d) (7)	page=6 xpos=3 ypos=6 single-column left-indent font-largest indented-line longer-tail line-double-space line-space itemization headchar-lower above-blank-line above-double-space above-line-space
I-Body	There has also been work that splits sparse features into not one, but multiple	page=6 xpos=0 ypos=6 single-column full-justified hanged-line line-blank-line line-double-space line-space headchar-capital
I-Body	groups, creating a dense feature for each group (Xiang and Ittycheriah 2011; Liu et al.	page=6 xpos=0 ypos=6 single-column full-justified aligned-line year headchar-lower tailchar-period
E-Body	2013).	page=6 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail year tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	2.4 Decoding	page=6 xpos=0 ypos=7 single-column right-indent aligned-line longer-tail line-blank-line line-double-space line-space numbered-heading2 above-double-space above-line-space
B-Body	Given an input sentence f , the task of decoding is defined as an inference problem	page=6 xpos=0 ypos=7 single-column full-justified aligned-line longer-tail line-double-space line-space headchar-capital
I-Body	of finding the best scoring derivation h ê, d̂ i according to Equation (1). In general, the	page=6 xpos=0 ypos=7 single-column full-justified font-larger aligned-line headchar-lower
I-Body	inference is intractable if we enumerate all possible derivations in T ( f ) and rank	page=6 xpos=0 ypos=8 single-column full-justified font-larger aligned-line headchar-lower
I-Body	each derivation by the model. We assume that a derivation is composed of a set of	page=6 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	steps	page=6 xpos=0 ypos=8 single-column right-indent aligned-line shorter-tail headchar-lower above-double-space above-line-space
B-Equation	d = d <sub>1</sub> , d 2 , · · · , d | d | (8)	page=6 xpos=3 ypos=8 single-column left-indent font-largest indented-line longer-tail line-double-space line-space itemization headchar-lower above-double-space above-line-space
I-Body	where each d <sub>j</sub> is a step—for example, a phrase pair in phrase-based MT or a syn-	page=6 xpos=0 ypos=9 single-column full-justified font-largest hanged-line line-double-space line-space headchar-lower tailchar-hiphen
I-Body	chronous rule in tree-based MT—ordered in a particular way. We also assume that	page=6 xpos=0 ypos=9 single-column full-justified aligned-line headchar-lower above-blank-line above-double-space above-line-space
Page	7	page=6 xpos=9 ypos=9 single-column left-indent indented-line line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Computational Linguistics Volume 42, Number 1	page=7 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Body	each feature function can be decomposed over each step, and Equation (1) can be	page=7 xpos=0 ypos=0 single-column full-justified aligned-line line-blank-line line-double-space line-space headchar-lower
I-Body	expressed by	page=7 xpos=0 ypos=0 single-column right-indent aligned-line shorter-tail headchar-lower above-blank-line above-double-space above-line-space
B-Equation	| w | | d |	page=7 xpos=4 ypos=1 single-column left-indent right-indent font-smallest indented-line longer-tail line-blank-line line-double-space line-space
I-Equation	h ê, d̂ i = arg max <sup>X</sup> w <sub>i</sub> X h i (d j , ρ i (d <sup>j</sup> <sub>1</sub> <sup>−</sup> 1 )) (9)	page=7 xpos=2 ypos=1 single-column left-indent font-largest hanged-line longer-tail itemization headchar-lower
I-Equation	h e , d i∈ T ( f ) i j = 1	page=7 xpos=3 ypos=1 single-column left-indent right-indent indented-line shorter-tail itemization headchar-lower above-blank-line above-double-space above-line-space
I-Body	where h <sub>i</sub> (d j , ρ i (d <sup>j</sup> <sub>1</sub> <sup>−</sup> 1 )) is a feature function for the jth step decomposed from the global	page=7 xpos=0 ypos=2 single-column full-justified font-largest hanged-line longer-tail line-blank-line line-double-space line-space headchar-lower
I-Body	feature function of h <sub>i</sub> ( f , e, d). As mentioned in the previous section, non-local fea-	page=7 xpos=0 ypos=2 single-column full-justified font-largest aligned-line headchar-lower tailchar-hiphen
I-Body	tures require information that cannot be calculated directly from the rule itself, and	page=7 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	ρ i (d <sup>j</sup> <sub>1</sub> <sup>−</sup> 1 ) is a variable that defines the residual information to score this ith feature func-	page=7 xpos=0 ypos=2 single-column full-justified font-largest aligned-line tailchar-hiphen
I-Body	tion using the partial derivation d <sup>j</sup> <sub>1</sub> <sup>−</sup> 1 (Gesmundo and Henderson 2011; Green, Cer,	page=7 xpos=0 ypos=2 single-column full-justified font-largest aligned-line year headchar-lower tailchar-comma
I-Body	and Manning 2014). For example, in phrase-based translation, for an n-gram language	page=7 xpos=0 ypos=3 single-column full-justified aligned-line year headchar-lower
I-Body	model feature, ρ <sub>i</sub> (d <sup>j</sup> <sub>1</sub> <sup>−</sup> 1 ) will be the n − 1 word suffix of the partial translation (Koehn,	page=7 xpos=0 ypos=3 single-column full-justified font-largest aligned-line headchar-lower tailchar-comma
I-Body	Och, and Marcu 2003). The local feature functions, such as phrase translation probabili-	page=7 xpos=0 ypos=3 single-column full-justified aligned-line year headchar-capital tailchar-hiphen
E-Body	ties in Section 2.3.1, require no context from partial derivations, and thus ρ <sub>i</sub> (d <sup>j</sup> <sub>1</sub> <sup>−</sup> 1 ) = ∅ .	page=7 xpos=0 ypos=3 single-column right-indent font-largest aligned-line shorter-tail headchar-lower tailchar-period
B-Body	The problem of decoding is treated as a search problem in which partial derivations	page=7 xpos=0 ypos=3 single-column left-indent indented-line longer-tail headchar-capital
I-Body	ḋ together with ρ <sub>i</sub> ( ḋ) in Equation (9) are enumerated to form hypotheses or states. In	page=7 xpos=0 ypos=4 single-column full-justified font-largest hanged-line headchar-lower
I-Body	phrase-based MT, search is carried out by enumerating partial derivations in left-to-	page=7 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	right order on the target side while remembering the translated source word positions.	page=7 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower tailchar-period
I-Body	Similarly, the search in MT with synchronous grammars is performed by using the	page=7 xpos=0 ypos=4 single-column full-justified aligned-line headchar-capital
I-Body	CYK+ algorithm (Chappelier and Rajman 1998) on the source side and generating	page=7 xpos=0 ypos=4 single-column full-justified aligned-line year headchar-capital
I-Body	partial derivations for progressively longer source spans. Because of the enormous	page=7 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	search space brought about by maintaining ρ <sub>i</sub> ( ḋ) in each partial derivation, beam search	page=7 xpos=0 ypos=5 single-column full-justified font-largest aligned-line headchar-lower
I-Body	is used to heuristically prune the search space. As a result, the search is inexact because	page=7 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	of the search error caused by heuristic pruning, in which the best scoring hypothesis is	page=7 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
E-Body	not necessarily optimal in terms of given model parameters.	page=7 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	The search is efficiently carried out by merging equivalent states encoded as ρ	page=7 xpos=0 ypos=5 single-column left-indent indented-line longer-tail headchar-capital
I-Body	(Koehn, Och, and Marcu 2003; Huang and Chiang 2007), and the space is succinctly	page=7 xpos=0 ypos=6 single-column full-justified hanged-line year
I-Body	represented by compact data structures, such as graphs (Ueffing, Och, and Ney 2002)	page=7 xpos=0 ypos=6 single-column full-justified aligned-line year headchar-lower
I-Body	(or lattices) in phrase-based MT (Koehn, Och, and Marcu 2003) and hypergraphs (Klein	page=7 xpos=0 ypos=6 single-column full-justified aligned-line year
I-Body	and Manning 2004) (or packed forests) in tree-based MT (Huang and Chiang 2007).	page=7 xpos=0 ypos=6 single-column full-justified aligned-line year headchar-lower tailchar-period
I-Body	These data structures may be directly used as compact representations of all derivations	page=7 xpos=0 ypos=6 single-column full-justified aligned-line headchar-capital
E-Body	for optimization.	page=7 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	However, using these data structures directly can be unwieldly, and thus it is	page=7 xpos=0 ypos=7 single-column left-indent indented-line longer-tail headchar-capital
I-Body	more common to obtain a k-best list as an approximation of the derivation space.	page=7 xpos=0 ypos=7 single-column full-justified hanged-line headchar-lower tailchar-period
I-Body	Figure 1(a) shows an example of k-best English translations for a French input sentence,	page=7 xpos=0 ypos=7 single-column full-justified aligned-line string-figure headchar-capital tailchar-comma
I-Body	‘la délégation chinoise appuiera pleinement la présidence.’ The k-best list may be obtained	page=7 xpos=0 ypos=7 single-column full-justified aligned-line
I-Body	either from a lattice in Figure 1(b) or from a forest in Figure 1(c). It should be noted	page=7 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	that different derivations in a k-best list may share the same translation due to the	page=7 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	variation of phrases or rules in constructing a translation, e.g., the choice of support	page=7 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	the chair or support and the chair in Figure 1(b). A diverse k-best list can be obtained by	page=7 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	extracting a unique k-best list that maintains only the best scored derivation sharing	page=7 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	the same translation (Huang, Knight, and Joshi 2006; Hasan, Zens, and Ney 2007),	page=7 xpos=0 ypos=8 single-column full-justified aligned-line year headchar-lower tailchar-comma
I-Body	by incorporating a penalty term when scoring derivations (Gimpel et al. 2013), or by	page=7 xpos=0 ypos=8 single-column full-justified aligned-line year headchar-lower
I-Body	performing Monte Carlo sampling to acquire a more diverse set of candidates (Blunsom	page=7 xpos=0 ypos=9 single-column full-justified aligned-line headchar-lower
E-Body	and Osborne 2008).	page=7 xpos=0 ypos=9 single-column right-indent aligned-line shorter-tail year headchar-lower tailchar-period above-blank-line above-double-space above-line-space
Page	8	page=7 xpos=0 ypos=9 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Neubig and Watanabe Optimization for Statistical Machine Translation	page=8 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
B-Figure	the delegation of china will support the chair in full .	page=8 xpos=2 ypos=0 single-column left-indent right-indent font-smallest indented-line shorter-tail line-blank-line line-double-space line-space headchar-lower tailchar-period
I-Figure	the chinese delegation will fully support the chair .	page=8 xpos=2 ypos=0 single-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
I-Figure	the chinese delegation will fully support the presidency .	page=8 xpos=2 ypos=0 single-column left-indent right-indent font-smallest aligned-line longer-tail headchar-lower tailchar-period
I-Figure	the delegation of china will in full support the presidency .	page=8 xpos=2 ypos=0 single-column centered left-indent right-indent font-smallest aligned-line longer-tail headchar-lower tailchar-period
I-Figure	the china delegation will in full support the chair .	page=8 xpos=2 ypos=1 single-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period above-line-space
I-Figure	(a) k-best	page=8 xpos=4 ypos=1 single-column centered left-indent right-indent font-smallest indented-line shorter-tail line-space itemization above-double-space above-line-space
I-Figure	X	page=8 xpos=7 ypos=1 single-column left-indent right-indent indented-line longer-tail line-double-space line-space headchar-capital above-double-space above-line-space
I-Figure	X X . X will X .	page=8 xpos=6 ypos=1 single-column left-indent right-indent hanged-line longer-tail line-double-space line-space headchar-capital tailchar-period above-line-space
I-Figure	of support the chair X support X	page=8 xpos=1 ypos=2 single-column left-indent right-indent font-largest hanged-line longer-tail line-space headchar-lower
I-Figure	delegation china support in full will support X X	page=8 xpos=0 ypos=2 single-column left-indent right-indent font-largest hanged-line shorter-tail headchar-lower
I-Figure	chinese	page=8 xpos=0 ypos=2 single-column left-indent right-indent font-smaller indented-line shorter-tail headchar-lower
I-Figure	the X X the <sub>X</sub> of X	page=8 xpos=4 ypos=2 single-column left-indent right-indent font-largest indented-line longer-tail headchar-lower
I-Figure	will X support X	page=8 xpos=8 ypos=2 single-column left-indent right-indent indented-line longer-tail headchar-lower
I-Figure	the will fully <sup>the</sup> chair .	page=8 xpos=0 ypos=2 single-column left-indent right-indent font-largest hanged-line shorter-tail headchar-lower tailchar-period
I-Figure	china fully	page=8 xpos=0 ypos=2 single-column left-indent right-indent font-largest indented-line shorter-tail headchar-lower
I-Figure	chinese delegation <sup>support</sup> in full <sup>the</sup> presidency delegation china chinese <sup>fully</sup> in full the presidency the chair	page=8 xpos=0 ypos=2 single-column left-indent right-indent font-largest hanged-line longer-tail headchar-lower above-line-space
I-Figure	(b) lattice (c) forest	page=8 xpos=1 ypos=3 single-column left-indent right-indent font-smallest indented-line shorter-tail line-space itemization above-double-space above-line-space
B-Caption	Figure 1	page=8 xpos=0 ypos=3 single-column right-indent font-smallest hanged-line shorter-tail line-double-space line-space string-figure headchar-capital
E-Caption	Example of a k-best list, lattice, and forest.	page=8 xpos=0 ypos=3 single-column right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-period above-blank-line above-double-space above-line-space
B-Body	Another class of decoding problem is forced decoding, in which the output from a	page=8 xpos=0 ypos=4 single-column left-indent indented-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	decoder is forced to match with a reference translation of the input sentence. In phrase-	page=8 xpos=0 ypos=4 single-column full-justified hanged-line headchar-lower tailchar-hiphen
I-Body	based MT, this is implemented by adding additional features to reward hypotheses that	page=8 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	match with the given target sentence (Liang, Zhang, and Zhao 2012; Yu et al. 2013). In	page=8 xpos=0 ypos=4 single-column full-justified aligned-line year headchar-lower
I-Body	MT using synchronous grammars, it is carried out by biparsing over two languages, for	page=8 xpos=0 ypos=4 single-column full-justified aligned-line headchar-capital
I-Body	instance, by a variant of the CYK algorithm (Wu 1997) or by a more efficient two-step	page=8 xpos=0 ypos=4 single-column full-justified aligned-line year headchar-lower
I-Body	algorithm (Dyer 2010b; Peitz et al. 2012). Even if we perform forced decoding, we are	page=8 xpos=0 ypos=5 single-column full-justified aligned-line year headchar-lower
I-Body	still not guaranteed that the decoder will be able to produce the reference translation	page=8 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	(because of unknown words, reordering limits, or other factors). This problem can be	page=8 xpos=0 ypos=5 single-column full-justified aligned-line
I-Body	resolved by preserving the prefix of partial derivations (Yu et al. 2013), or by allowing	page=8 xpos=0 ypos=5 single-column full-justified aligned-line year headchar-lower
I-Body	approximate matching of the target side (Liang, Zhang, and Zhao 2012). It is also	page=8 xpos=0 ypos=5 single-column full-justified aligned-line year headchar-lower
I-Body	possible to create a neighborhood of a forced decoding derivation by adding additional	page=8 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	hyperedges to the true derivation, which allows for efficient generation of negative	page=8 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
E-Body	examples for discriminative learning algorithms (Xiao et al. 2011).	page=8 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail year headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	2.5 Evaluation	page=8 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	Once we have a machine translation system that can produce translations, we next must	page=8 xpos=0 ypos=7 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	perform evaluation to judge how good the generated translations actually are. As the	page=8 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	final consumer of machine translation output is usually a human, the most natural form	page=8 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	of evaluation is manual evaluation by human annotators. However, because human	page=8 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	evaluation is expensive and time-consuming, in recent years there has been a shift to	page=8 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
E-Body	automatic calculation of the quality of MT output.	page=8 xpos=0 ypos=8 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	In general, automatic evaluation measures use a set of data consisting of N input	page=8 xpos=0 ypos=8 single-column left-indent indented-line longer-tail headchar-capital
I-Body	sentences F = <sup>n</sup> f (i) o <sup>N</sup> <sub>i</sub> = <sub>1</sub> , each of which having a reference translation E =  e (i)   N i = 1 that	page=8 xpos=0 ypos=8 single-column full-justified font-largest hanged-line headchar-lower
I-Body	was created by a human translator. The input F is automatically translated using a ma-	page=8 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	chine translation system to acquire MT results Ê = <sup></sup> ê <sup>(i)</sup>   <sup>N</sup> <sub>i</sub> = <sub>1</sub> , which are then compared	page=8 xpos=0 ypos=8 single-column full-justified font-largest aligned-line headchar-lower
I-Body	to the corresponding references. The closer the MT output is to the reference, the better	page=8 xpos=0 ypos=9 single-column full-justified aligned-line headchar-lower
I-Body	it is deemed to be, according to automatic evaluation. In addition, as there are often	page=8 xpos=0 ypos=9 single-column full-justified aligned-line headchar-lower above-blank-line above-double-space above-line-space
Page	9	page=8 xpos=9 ypos=9 single-column left-indent indented-line line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Computational Linguistics Volume 42, Number 1	page=9 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Body	many ways to translate a particular sentence, it is also possible to perform evaluation	page=9 xpos=0 ypos=0 single-column full-justified aligned-line line-blank-line line-double-space line-space headchar-lower
I-Body	with multiple references created by different translators. There has also been some work	page=9 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower
I-Body	on encoding a huge number of references in a lattice, created either by hand (Dreyer and	page=9 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower
E-Body	Marcu 2012) or by automatic paraphrasing (Zhou, Lin, and Hovy 2006).	page=9 xpos=0 ypos=1 single-column right-indent aligned-line shorter-tail year headchar-capital tailchar-period
B-Body	One major distinction between optimization measures is whether they are calcu-	page=9 xpos=0 ypos=1 single-column left-indent indented-line longer-tail headchar-capital tailchar-hiphen
I-Body	lated on the corpus level or the sentence level. Corpus-level measures are calculated by	page=9 xpos=0 ypos=1 single-column full-justified hanged-line headchar-lower
I-Body	taking statistics over the whole corpus, whereas sentence-level measures are calculated	page=9 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	by measuring sentence-level accuracy, and defining the corpus-level accuracy as the	page=9 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	average of the sentence-level accuracies. All optimization algorithms that are applicable	page=9 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	to corpus-level measures are applicable to sentence-level measures, but the opposite is	page=9 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
E-Body	not true, making this distinction important from the optimization point of view.	page=9 xpos=0 ypos=2 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	The most commonly used MT evaluation measure BLEU (Papineni et al. 2002) is	page=9 xpos=0 ypos=2 single-column left-indent indented-line longer-tail year headchar-capital
I-Body	defined on the corpus level, and we will cover it in detail as it plays an important role	page=9 xpos=0 ypos=2 single-column full-justified hanged-line headchar-lower
I-Body	in some of the methods that follow. Of course, there have been many other evaluation	page=9 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	measures proposed since BLEU, with TER (Snover et al. 2006) and METEOR (Banerjee	page=9 xpos=0 ypos=3 single-column full-justified aligned-line year headchar-lower
I-Body	and Lavie 2005) being among the most widely used. The great majority of metrics other	page=9 xpos=0 ypos=3 single-column full-justified aligned-line year headchar-lower
I-Body	than BLEU are defined on the sentence level, and thus are conducive to optimization	page=9 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	algorithms that require sentence-level evaluation measures. We discuss the role of	page=9 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
E-Body	evaluation in MT optimization more completely in Section 8.3.	page=9 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-Body	2.5.1 BLEU. BLEU is defined as the geometric mean of n-gram precisions (usually for n	page=9 xpos=0 ypos=4 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space numbered-heading3
I-Body	from 1 to 4), and a brevity penalty to prevent short sentences from receiving unfairly	page=9 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	high evaluation scores. For a single reference sentence e and a corresponding system	page=9 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	output ê, we can define c <sub>n</sub> (ê) as the number of n-grams in ê, and m n (e, ê) as the number	page=9 xpos=0 ypos=4 single-column full-justified font-largest aligned-line headchar-lower
I-Body	of n-grams in ê that match e	page=9 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail headchar-lower above-double-space above-line-space
B-Equation	c <sub>n</sub> (ê) = |{ g n ∈ ê }|	page=9 xpos=3 ypos=5 single-column left-indent right-indent font-largest indented-line longer-tail line-double-space line-space itemization headchar-lower above-line-space
I-Equation	m <sub>n</sub> (e, ê) = |{ g n ∈ ê } ∩ { g <sup>0</sup> <sub>n</sub> ∈ e }|	page=9 xpos=3 ypos=5 single-column centered left-indent right-indent font-largest hanged-line longer-tail line-space itemization headchar-lower above-double-space above-line-space
B-Body	Here, { g <sub>n</sub> ∈ ê } and { g <sup>0</sup> <sub>n</sub> ∈ e } are multisets that can contain identical n-grams more than	page=9 xpos=0 ypos=5 single-column full-justified font-largest hanged-line longer-tail line-double-space line-space headchar-capital
I-Body	once, and ∩ is an operator for multisets that allows for consideration of multiple in-	page=9 xpos=0 ypos=6 single-column full-justified font-larger aligned-line headchar-lower tailchar-hiphen
I-Body	stances of the same n-gram. <sup>3</sup> Note that the total count for a candidate n-gram is clipped	page=9 xpos=0 ypos=6 single-column full-justified font-largest aligned-line headchar-lower
I-Body	to be no more than the count in the reference translation. If we have a corpus of reference	page=9 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	sets R = { e <sup>(1)</sup> , . . . , e (N) } , where each sentence has M references e (i) = { e <sub>1</sub> <sup>(i)</sup> , . . . , e (i) M } , the	page=9 xpos=0 ypos=6 single-column full-justified font-largest aligned-line headchar-lower
I-Body	BLEU score of the corresponding system outputs E = { ê <sup>(1)</sup> , . . . , ê (N) } can be defined	page=9 xpos=0 ypos=6 single-column full-justified font-largest aligned-line headchar-capital
I-Body	as	page=9 xpos=0 ypos=7 single-column right-indent aligned-line shorter-tail headchar-lower above-double-space above-line-space
B-Equation	i = 1 m n ( { e <sub>1</sub> <sup>(i)</sup> , . . . , e (i) M } , ê (i) ) <sup>!</sup> <sup>14</sup> <sub>·</sub> <sub>BP(E,</sub> Ê)	page=9 xpos=4 ypos=7 single-column left-indent right-indent font-largest indented-line longer-tail line-double-space line-space itemization headchar-lower
I-Equation	4 P N	page=9 xpos=3 ypos=7 single-column left-indent right-indent font-smallest hanged-line shorter-tail numbered-heading1
I-Equation	BLEU(E, Ê) = <sup>Y</sup> (10)	page=9 xpos=1 ypos=7 single-column left-indent font-largest hanged-line longer-tail headchar-capital
I-Equation	P N	page=9 xpos=4 ypos=7 single-column left-indent right-indent font-smallest indented-line shorter-tail headchar-capital
I-Equation	n = 1 i = 1 c n (ê <sup>(i)</sup> )	page=9 xpos=3 ypos=7 single-column left-indent right-indent font-largest hanged-line longer-tail itemization headchar-lower above-double-space above-line-space
I-Body	where the first term corresponds to geometric mean of the n-gram precisions, and	page=9 xpos=0 ypos=8 single-column full-justified hanged-line longer-tail line-double-space line-space headchar-lower
I-Body	the second term BP(E, Ê) is the brevity penalty. The brevity penalty is necessary here	page=9 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	because evaluation of precision favors systems that output only the words and phrases	page=9 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	that have high accuracy, and avoids outputting more difficult-to-translate content that	page=9 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower above-blank-line above-double-space above-line-space
B-Footnote	3 We let # A (a) denote the number of times a appeared in a multiset A, and define: | A | = <sup>P</sup> <sub>a</sub> # A (a),	page=9 xpos=0 ypos=9 single-column left-indent right-indent font-larger indented-line shorter-tail line-blank-line line-double-space line-space numbered-heading1 tailchar-comma
I-Footnote	# A ∪ B (a) = max { # A (a), # B (a) } , and # A ∩ B (a) = min { # A (a), # B (a) } .	page=9 xpos=0 ypos=9 single-column left-indent right-indent font-smaller indented-line shorter-tail tailchar-period above-blank-line above-double-space above-line-space
Page	10	page=9 xpos=0 ypos=9 single-column right-indent hanged-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Neubig and Watanabe Optimization for Statistical Machine Translation	page=10 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Body	might not match the reference. The brevity penalty prevents this by discounting outputs	page=10 xpos=0 ypos=0 single-column full-justified aligned-line line-blank-line line-double-space line-space headchar-lower
I-Body	that are shorter than the reference	page=10 xpos=0 ypos=0 single-column right-indent aligned-line shorter-tail headchar-lower above-double-space above-line-space
B-Equation	(	page=10 xpos=4 ypos=1 single-column left-indent right-indent font-smallest indented-line longer-tail line-double-space line-space
I-Equation	P N | ẽ (i) | <sup>!)</sup>	page=10 xpos=5 ypos=1 single-column left-indent right-indent font-largest indented-line longer-tail headchar-capital
I-Equation	BP(E, Ê) = min 1, exp 1 − <sub>P</sub> <sup>i</sup> N <sub>i</sub> = = 1 <sub>1</sub> | ê (i) |	page=10 xpos=2 ypos=1 single-column left-indent right-indent font-largest hanged-line shorter-tail headchar-capital
I-Equation	(11)	page=10 xpos=9 ypos=1 single-column left-indent indented-line longer-tail above-blank-line above-double-space above-line-space
I-Body	where ẽ <sup>(i)</sup> is defined as the longest reference with a length shorter than or equal to ê <sup>(i)</sup> .	page=10 xpos=0 ypos=1 single-column right-indent font-largest hanged-line shorter-tail line-blank-line line-double-space line-space headchar-lower tailchar-period above-double-space above-line-space
I-Body	2.5.2 BLEU+1. One thing to notice here is that BLEU is calculated by taking statistics	page=10 xpos=0 ypos=2 single-column full-justified aligned-line longer-tail line-double-space line-space numbered-heading3
I-Body	over the entire corpus, and thus it is a corpus-level measure. There is nothing inherently	page=10 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	preventing us from calculating BLEU on a single sentence, but in the single-sentence	page=10 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	case it is common for the number of matches of higher order n-grams to become zero,	page=10 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	resulting in a BLEU score of zero for the entire sentence. One common solution to this	page=10 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	problem is the use of a smoothed version of BLEU, commonly referred to as BLEU+1	page=10 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	(Lin and Och 2004). In BLEU+1, we add one to the numerators and denominators of	page=10 xpos=0 ypos=3 single-column full-justified aligned-line year
I-Body	each n-gram of order greater than one	page=10 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail headchar-lower above-double-space above-line-space
B-Equation	c <sup>0</sup> <sub>n</sub> (ê) = |{ g n ∈ ê }| + δ (n > 1)	page=10 xpos=2 ypos=3 single-column left-indent right-indent font-largest indented-line longer-tail line-double-space line-space itemization headchar-lower above-line-space
I-Equation	m <sup>0</sup> <sub>n</sub> (e, ê) = |{ g n ∈ ê } ∩ { g 0 <sub>n</sub> ∈ e }| + δ (n > 1)	page=10 xpos=2 ypos=3 single-column centered left-indent right-indent font-largest hanged-line longer-tail line-space itemization headchar-lower above-double-space above-line-space
I-Body	where δ ( · ) is a function that takes a value of 1 when the corresponding statement is true.	page=10 xpos=0 ypos=4 single-column full-justified font-larger hanged-line longer-tail line-double-space line-space headchar-lower tailchar-period
I-Body	We can then re-define a sentence-level BLEU using these smoothed counts	page=10 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail headchar-capital above-double-space above-line-space
B-Equation	4  m 0 <sub>(</sub> { e , . . . , e <sub>M</sub> } , ê)  <sup>1</sup> 4 <sub>·</sub> <sub>BP(e,</sub> ê)	page=10 xpos=3 ypos=4 single-column left-indent right-indent font-largest indented-line shorter-tail line-double-space line-space numbered-heading1
I-Equation	BLEU’(e, ê) = <sup>Y</sup> n 1 <sub>c</sub> 0 <sub>n</sub> (ê)	page=10 xpos=2 ypos=4 single-column left-indent right-indent font-largest hanged-line shorter-tail headchar-capital
I-Equation	(12)	page=10 xpos=9 ypos=5 single-column left-indent indented-line longer-tail above-line-space
I-Equation	n = 1	page=10 xpos=3 ypos=5 single-column left-indent right-indent font-smallest hanged-line shorter-tail line-space itemization headchar-lower above-double-space above-line-space
I-Body	and the corpus-level evaluation can be re-defined as the average of sentence level	page=10 xpos=0 ypos=5 single-column full-justified hanged-line longer-tail line-double-space line-space headchar-lower
I-Body	evaluations	page=10 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail headchar-lower above-double-space above-line-space
B-Equation	BLEU’(E, Ê) = 1 X <sup>N</sup> BLEU’(e (i) , ê (i) ) (13)	page=10 xpos=2 ypos=6 single-column left-indent font-largest indented-line longer-tail line-double-space line-space headchar-capital
I-Equation	N	page=10 xpos=4 ypos=6 single-column left-indent right-indent indented-line shorter-tail headchar-capital
I-Equation	i = 1	page=10 xpos=4 ypos=6 single-column left-indent right-indent font-smallest indented-line longer-tail itemization headchar-lower above-double-space above-line-space
B-Body	It has also been noted, however, that the average of sentence-level BLEU+1 is not a	page=10 xpos=0 ypos=6 single-column full-justified hanged-line longer-tail line-double-space line-space headchar-capital
I-Body	very accurate approximation of corpus-level BLEU, but by adjusting the smoothing	page=10 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	heuristics it is possible to achieve a more accurate approximation (Nakov, Guzman, and	page=10 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
E-Body	Vogel 2012).	page=10 xpos=0 ypos=7 single-column right-indent aligned-line shorter-tail year headchar-capital tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	2.6 The Optimization Setting	page=10 xpos=0 ypos=7 single-column right-indent aligned-line longer-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	During the optimization process, we will assume that we have some data consisting	page=10 xpos=0 ypos=7 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	of sources F = <sup>n</sup> f (i) o <sup>N</sup> <sub>i</sub> = <sub>1</sub> with corresponding references E =  e (i)   N i = 1 as defined in the	page=10 xpos=0 ypos=8 single-column full-justified font-largest aligned-line headchar-lower
I-Body	previous section, and that we would like to use these to optimize the parameters of the	page=10 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	model. As mentioned in Section 2.5, it is also possible to use more than one reference	page=10 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	translation in evaluation, but in this survey we will assume for simplicity of exposition	page=10 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
E-Body	that only one reference is used.	page=10 xpos=0 ypos=8 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	When we select a certain weight vector w, this will affect the scores calculated	page=10 xpos=0 ypos=9 single-column left-indent indented-line longer-tail headchar-capital
I-Body	according to the model, and thus the result acquired during decoding, as described	page=10 xpos=0 ypos=9 single-column full-justified hanged-line headchar-lower above-blank-line above-double-space above-line-space
Page	11	page=10 xpos=9 ypos=9 single-column left-indent indented-line line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Computational Linguistics Volume 42, Number 1	page=11 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Body	in Section 2.4. To express whether this effect is a positive or negative one, we define	page=11 xpos=0 ypos=0 single-column full-justified aligned-line line-blank-line line-double-space line-space headchar-lower
I-Body	a loss function ` (F, E; w) : F <sup>N</sup> × E N × <sub>R</sub> M → R that provides a numerical indicator of	page=11 xpos=0 ypos=0 single-column full-justified font-largest aligned-line itemization headchar-lower
I-Body	how “bad” the translations generated when we use a particular w are. As the goal of	page=11 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower
I-Body	optimization is to achieve better translations, we would like to choose parameters that	page=11 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	reduce this loss. More formally, we can cast the problem as minimizing the expectation	page=11 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	of ` ( · ), or risk minimization:	page=11 xpos=0 ypos=1 single-column right-indent font-larger aligned-line shorter-tail headchar-lower tailchar-colon above-blank-line above-double-space above-line-space
B-Equation	ŵ = arg min E <sub>Pr(F,E)</sub> [ ` (F, E; w)] (14)	page=11 xpos=3 ypos=1 single-column left-indent font-largest indented-line longer-tail line-blank-line line-double-space line-space headchar-lower
I-Equation	w ∈ R <sup>M</sup>	page=11 xpos=3 ypos=2 single-column left-indent right-indent font-smallest indented-line shorter-tail itemization headchar-lower above-blank-line above-double-space above-line-space
B-Body	Here, Pr(F, E) is the true joint distribution over all sets of input and output sentences that	page=11 xpos=0 ypos=2 single-column full-justified hanged-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	we are likely to be required to translate. However, in reality we will not know the true	page=11 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	distribution over all sets of sentences a user may ask us to translate. Instead, we have a	page=11 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	single set of data (henceforth, training data), and attempt to find the w that minimizes	page=11 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	the loss on this data:	page=11 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-colon above-blank-line above-double-space above-line-space
B-Equation	ŵ = arg min ` (F, E; w) (15)	page=11 xpos=3 ypos=3 single-column left-indent indented-line longer-tail line-blank-line line-double-space line-space headchar-lower
I-Equation	w ∈ R <sup>M</sup>	page=11 xpos=4 ypos=3 single-column left-indent right-indent font-smallest indented-line shorter-tail itemization headchar-lower above-blank-line above-double-space above-line-space
B-Body	Because we are now optimizing on a single empirically derived set of training data, this	page=11 xpos=0 ypos=4 single-column full-justified hanged-line longer-tail line-blank-line line-double-space line-space headchar-capital
E-Body	framework is called empirical risk minimization.	page=11 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	In machine learning problems, it is common to introduce regularization to prevent	page=11 xpos=0 ypos=4 single-column left-indent indented-line longer-tail headchar-capital
I-Body	the learning of parameters that over-fit the training data. This gives us the framework of	page=11 xpos=0 ypos=4 single-column full-justified hanged-line headchar-lower
I-Body	regularized empirical risk minimization, which will encompass most of the methods	page=11 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	described in this survey, and is formalized as	page=11 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail headchar-lower above-blank-line above-double-space above-line-space
B-Equation	ŵ = arg min ` (F, E; w) + λΩ (w) (16)	page=11 xpos=3 ypos=5 single-column left-indent indented-line longer-tail line-blank-line line-double-space line-space headchar-lower
I-Equation	w ∈ R <sup>M</sup>	page=11 xpos=3 ypos=5 single-column left-indent right-indent font-smallest indented-line shorter-tail itemization headchar-lower above-blank-line above-double-space above-line-space
I-Body	where λ is a parameter adjusting the strength of regularization, and Ω (w) is a regular-	page=11 xpos=0 ypos=6 single-column full-justified hanged-line longer-tail line-blank-line line-double-space line-space headchar-lower tailchar-hiphen
I-Body	ization term, common choices for which include the L <sub>2</sub> regularizer Ω 2 (w) = 12 k w k 22 =	page=11 xpos=0 ypos=6 single-column full-justified font-largest aligned-line headchar-lower
I-Body	2 <sup>1</sup> w <sup>></sup> w or the L <sub>1</sub> regularizer Ω 1 (w) = k w k 1 = P <sup>M</sup> <sub>m</sub> = <sub>1</sub> | w m | (Tibshirani 1996; Chen and	page=11 xpos=0 ypos=6 single-column centered left-indent font-largest numbered-heading1 year
I-Body	Rosenfeld 1999). Intuitively, if λ is set to a small value, optimization will attempt to	page=11 xpos=0 ypos=6 single-column full-justified year headchar-capital
I-Body	learn a w that effectively minimizes loss on the training data, but there is a risk of over-	page=11 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	fitting reducing generalization capability. On the other hand, if λ is set to a larger value,	page=11 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	optimization will be less aggressive in minimizing loss on the training data, reducing	page=11 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	over-fitting, but also possibly failing to capture useful information that could be used to	page=11 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
E-Body	improve accuracy.	page=11 xpos=0 ypos=7 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SectionHeader	3. Defining a Loss Function	page=11 xpos=0 ypos=8 single-column right-indent aligned-line longer-tail line-blank-line line-double-space line-space itemization above-blank-line above-double-space above-line-space
B-Body	The first step in performing optimization is defining the loss function that we are	page=11 xpos=0 ypos=8 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	interested in optimizing. The choice of a proper loss function is critical in that it effects	page=11 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	the final performance of the optimized MT system, and also the possible choices for op-	page=11 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	timization algorithms. This section describes several common choices for loss functions,	page=11 xpos=0 ypos=9 single-column full-justified aligned-line headchar-lower tailchar-comma
E-Body	and describes their various features.	page=11 xpos=0 ypos=9 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
Page	12	page=11 xpos=0 ypos=9 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Neubig and Watanabe Optimization for Statistical Machine Translation	page=12 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
B-SubsectionHeader	3.1 Error	page=12 xpos=0 ypos=0 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	The first, and most straightforward, loss that we can attempt to optimize is error (Och	page=12 xpos=0 ypos=0 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	2003). We assume that by comparing the decoder’s translation result Ê with the refer-	page=12 xpos=0 ypos=1 single-column full-justified aligned-line year tailchar-hiphen
I-Body	ence E, we are able to calculate a function error(E, Ê) : E <sup>N</sup> × E N → <sub>R</sub> ≥ <sub>0</sub> that describes the	page=12 xpos=0 ypos=1 single-column full-justified font-largest aligned-line headchar-lower
I-Body	extent of error included in the translations. For example, if we use the BLEU described in	page=12 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	Section 2.5 as an evaluation measure for our system, it is natural to use 1 − BLEU as an	page=12 xpos=0 ypos=1 single-column full-justified font-larger aligned-line headchar-capital
I-Body	error function, so that as our evaluation improves, the error decreases. Converting this	page=12 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	to a loss function that is dependent on the model parameters, we obtain the following	page=12 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	loss expressing the error over the 1-best results obtained by decoding in Equation (1):	page=12 xpos=0 ypos=2 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-colon above-blank-line above-double-space above-line-space
B-Equation	 ( ) N <sup></sup>	page=12 xpos=4 ypos=2 single-column left-indent right-indent font-smaller indented-line shorter-tail line-blank-line line-double-space line-space
I-Equation	` error (F, E, C; w) = error <sub></sub> E, arg max w <sup>></sup> h( f (i) , e, d) (17) 	page=12 xpos=1 ypos=2 single-column left-indent font-largest hanged-line longer-tail
I-Equation	h e , d i∈ c <sup>(i)</sup> i = 1	page=12 xpos=5 ypos=2 single-column left-indent right-indent font-largest indented-line shorter-tail itemization headchar-lower above-blank-line above-double-space above-line-space
B-Body	Error has the advantage of being simple, easy to explain, and directly related to	page=12 xpos=0 ypos=3 single-column left-indent hanged-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	translation performance, and these features make it perhaps the most commonly used	page=12 xpos=0 ypos=3 single-column full-justified hanged-line headchar-lower
I-Body	loss in current machine translation systems. On the other hand, it also has a large	page=12 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	disadvantage in that the loss function expressed in Equation (17) is not convex, and	page=12 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	most MT evaluation measures used in the calculation of the error function error( · )	page=12 xpos=0 ypos=4 single-column full-justified font-larger aligned-line headchar-lower
I-Body	are not continuously differentiable. This makes direct minimization of error a difficult	page=12 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	optimization problem (particularly for larger feature sets), and thus a number of other,	page=12 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower tailchar-comma
E-Body	easier-to-optimize losses are used as well.	page=12 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	A special instance of error, which is worth mentioning because of its relation to the	page=12 xpos=0 ypos=4 single-column left-indent indented-line longer-tail headchar-capital
I-Body	methods we will introduce in the following sections, is zero–one loss. Zero–one loss	page=12 xpos=0 ypos=5 single-column full-justified hanged-line headchar-lower
I-Body	focuses on whether an oracle translation is chosen as the system output. Oracle trans-	page=12 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	lations can be vaguely defined as “good” translations, such as the reference translation	page=12 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	e <sup>(i)</sup> , or perhaps the best translation in the k-best list (described in detail in Section 4). If	page=12 xpos=0 ypos=5 single-column full-justified font-largest aligned-line itemization headchar-lower
I-Body	we define the set of oracle translations for sentence i as o <sup>(i)</sup> , zero–one loss is defined by	page=12 xpos=0 ypos=5 single-column full-justified font-largest aligned-line headchar-lower
I-Body	plugging the following zero–one error function into Equation (17):	page=12 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-colon above-double-space above-line-space
B-Equation	error(E, Ê) = 1 X <sup>N</sup> 1 − δ (ê (i) ∈ o (i) )  (18)	page=12 xpos=2 ypos=6 single-column left-indent font-largest indented-line longer-tail line-double-space line-space headchar-lower
I-Equation	N	page=12 xpos=4 ypos=6 single-column left-indent right-indent indented-line shorter-tail headchar-capital
I-Equation	i = 1	page=12 xpos=4 ypos=6 single-column left-indent right-indent font-smallest indented-line longer-tail itemization headchar-lower above-blank-line above-double-space above-line-space
I-Body	where ê <sup>(i)</sup> is the one-best translation candidate, and δ (ê (i) ∈ o (i) ) is one if ê (i) is a member	page=12 xpos=0 ypos=7 single-column full-justified font-largest hanged-line longer-tail line-blank-line line-double-space line-space headchar-lower
E-Body	of o <sup>(i)</sup> and zero otherwise.	page=12 xpos=0 ypos=7 single-column right-indent font-largest aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	3.2 Softmax Loss	page=12 xpos=0 ypos=7 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	One thing to note about error is that there is no concept of “probability” of each	page=12 xpos=0 ypos=8 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	translation candidate incorporated in its calculation. Being able to define a well-scaled	page=12 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	probability of candidates can be useful, however, for estimation of confidence measures	page=12 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	or incorporation with downstream applications. Softmax loss is a loss that is similar to	page=12 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	the zero–one loss, but directly defines a probabilistic model and attempts to maximize	page=12 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	the probability of the oracle translations (Berger, Della Pietra, and Della Pietra 1996; Och	page=12 xpos=0 ypos=9 single-column full-justified aligned-line year headchar-lower
E-Body	and Ney 2002; Blunsom, Cohn, and Osborne 2008).	page=12 xpos=0 ypos=9 single-column right-indent aligned-line shorter-tail year headchar-lower tailchar-period above-blank-line above-double-space above-line-space
Page	13	page=12 xpos=9 ypos=9 single-column left-indent indented-line longer-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Computational Linguistics Volume 42, Number 1	page=13 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
B-Body	In particular, if we assume that MT is modeled according to the log-linear model	page=13 xpos=0 ypos=0 single-column left-indent right-indent indented-line shorter-tail line-blank-line line-double-space line-space headchar-capital above-double-space above-line-space
B-Equation	p <sub>w</sub> (e, d | f , c) = <sub>P</sub> <sub>h</sub> <sub>e</sub> 0 <sub>,</sub> d <sup>exp(w</sup> 0 i∈ c exp(w <sup>></sup> h( f > , e, h( d)) f , e 0 , d 0 ))	page=13 xpos=2 ypos=0 single-column centered left-indent right-indent font-largest indented-line shorter-tail line-double-space line-space itemization headchar-lower
I-Equation	(19)	page=13 xpos=9 ypos=1 single-column left-indent indented-line longer-tail above-blank-line above-double-space above-line-space
I-Body	we can define softmax loss ` <sub>softmax</sub> ( · ) as follows:	page=13 xpos=0 ypos=1 single-column right-indent font-largest hanged-line shorter-tail line-blank-line line-double-space line-space headchar-lower tailchar-colon above-double-space above-line-space
B-Equation	` softmax (F, E, C; w) = − N <sup>1</sup> Y <sub>i</sub> = <sup>N</sup> <sub>1</sub> <sub>h</sub> <sub>e</sub> <sub>,</sub> X d i∈ o (i) p w (e, d | f (i) , c (i) )	page=13 xpos=1 ypos=1 single-column left-indent right-indent box indented-line longer-tail line-double-space line-space
I-Equation	(20)	page=13 xpos=9 ypos=2 single-column left-indent indented-line longer-tail above-double-space above-line-space
I-Equation	= − 1 Y <sup>h</sup> e , d i∈ o <sup>(i)</sup> <sup>exp(w</sup> <sup>></sup> h( f <sup>(i)</sup> , e, d))	page=13 xpos=3 ypos=2 single-column left-indent right-indent font-largest hanged-line shorter-tail line-double-space line-space
I-Equation	N P	page=13 xpos=4 ypos=2 single-column left-indent right-indent font-smallest indented-line shorter-tail headchar-capital above-line-space
I-Equation	(21)	page=13 xpos=9 ypos=2 single-column left-indent indented-line longer-tail line-space
I-Equation	i = 1 h e , d i∈ c <sup>(i)</sup> exp(w <sup>></sup> h( f <sup>(i)</sup> , e, d))	page=13 xpos=4 ypos=2 single-column left-indent right-indent font-largest hanged-line shorter-tail itemization headchar-lower
I-Equation	N P	page=13 xpos=4 ypos=2 single-column left-indent right-indent hanged-line shorter-tail headchar-capital above-blank-line above-double-space above-line-space
B-Body	From Equation (21) we can see that only the oracle translations contribute to the	page=13 xpos=0 ypos=3 single-column left-indent hanged-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	numerator, and all candidates in c <sup>(i)</sup> contributes to the denominator. Thus, intuitively,	page=13 xpos=0 ypos=3 single-column full-justified font-largest hanged-line headchar-lower tailchar-comma
I-Body	the softmax objective prefers parameter settings that assign high scores to the oracle	page=13 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
E-Body	translations, and lower scores to any other members of c <sup>(i)</sup> that are not oracles.	page=13 xpos=0 ypos=3 single-column right-indent font-largest aligned-line shorter-tail headchar-lower tailchar-period
B-Body	It should be noted that this loss can be calculated from a k-best list by iterating over	page=13 xpos=0 ypos=4 single-column left-indent indented-line longer-tail headchar-capital
I-Body	the entire list and calculating the numerators and denominators in Equation (19). It is	page=13 xpos=0 ypos=4 single-column full-justified hanged-line headchar-lower
I-Body	also possible, but more involved, to calculate over lattices or forests by using dynamic	page=13 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	programming algorithms such as the forward–backward or inside–outside algorithms	page=13 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
E-Body	(Blunsom, Cohn, and Osborne 2008; Gimpel and Smith 2009).	page=13 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail year tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	3.3 Risk-Based Loss	page=13 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	In contrast to softmax loss, which can be viewed as a probabilistic version of zero–one	page=13 xpos=0 ypos=5 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	loss, risk defines a probabilistic version of the translation error (Smith and Eisner 2006;	page=13 xpos=0 ypos=5 single-column full-justified aligned-line year headchar-lower tailchar-semicolon
I-Body	Zens, Hasan, and Ney 2007; Li and Eisner 2009; He and Deng 2012). Specifically, risk	page=13 xpos=0 ypos=5 single-column full-justified aligned-line year headchar-capital
I-Body	is based on the expected error incurred by a probabilistic model parameterized by w.	page=13 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower tailchar-period
I-Body	This combines the advantages of the probabilistic model in softmax loss with the direct	page=13 xpos=0 ypos=6 single-column full-justified aligned-line headchar-capital
I-Body	consideration of translation accuracy afforded by using error directly. In comparison to	page=13 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
E-Body	error, it also has the advantage of being differentiable, allowing for easier optimization.	page=13 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower tailchar-period
B-Body	To define this error, we define a scaling parameter γ ≥ 0 and use it in the calculation	page=13 xpos=0 ypos=6 single-column left-indent font-larger indented-line headchar-capital
I-Body	of each hypothesis’s probability	page=13 xpos=0 ypos=6 single-column right-indent hanged-line shorter-tail headchar-lower above-double-space above-line-space
B-Equation	p <sub>γ</sub> <sub>,</sub> w (e, d | f , c) = <sub>P</sub> <sub>h</sub> <sub>e</sub> 0 <sub>,</sub> d <sup>exp(</sup> 0 i∈ c exp( γ <sup>w</sup> <sup>></sup> γ h( w f > , h( e, d)) f , e 0 , d 0 ))	page=13 xpos=2 ypos=7 single-column centered left-indent right-indent font-largest indented-line longer-tail line-double-space line-space itemization headchar-lower
I-Equation	(22)	page=13 xpos=9 ypos=7 single-column left-indent indented-line longer-tail above-blank-line above-double-space above-line-space
B-Body	Given this probability, we then calculate the expected loss as follows:	page=13 xpos=0 ypos=7 single-column right-indent hanged-line shorter-tail line-blank-line line-double-space line-space headchar-capital tailchar-colon above-double-space above-line-space
B-Equation	` risk (F, E, C; γ , w) = N <sup>1</sup> X <sub>i</sub> = <sup>N</sup> <sub>1</sub> E p γ , w ( e , d | f (i) , c (i) ) [err(e (i) , e)]	page=13 xpos=1 ypos=8 single-column left-indent right-indent box indented-line shorter-tail line-double-space line-space
I-Equation	(23)	page=13 xpos=9 ypos=8 single-column left-indent indented-line longer-tail above-blank-line above-double-space above-line-space
I-Equation	N	page=13 xpos=4 ypos=8 single-column left-indent right-indent font-smallest hanged-line shorter-tail line-blank-line line-double-space line-space headchar-capital
I-Equation	= 1 X X <sub>err(e</sub> (i) , e)p <sub>γ</sub> <sub>,</sub> w (e, d | f (i) , c (i) ) (24)	page=13 xpos=3 ypos=9 single-column left-indent font-largest hanged-line longer-tail
I-Equation	N	page=13 xpos=3 ypos=9 single-column left-indent right-indent indented-line shorter-tail headchar-capital
I-Equation	i = 1 h e , d i∈ c (i)	page=13 xpos=4 ypos=9 single-column left-indent right-indent font-smallest indented-line longer-tail itemization headchar-lower above-blank-line above-double-space above-line-space
Page	14	page=13 xpos=0 ypos=9 single-column right-indent hanged-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Neubig and Watanabe Optimization for Statistical Machine Translation	page=14 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Body	In Equation (22), when γ = 0 regardless of parameters w every hypothesis h e, d i will	page=14 xpos=0 ypos=0 single-column full-justified font-larger aligned-line line-blank-line line-double-space line-space headchar-capital
I-Body	be assigned a uniform probability, and when γ = 1 the probabilities are equivalent to	page=14 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower
I-Body	those in the log-linear model of Equation (19). When γ → ∞ , the probability of the	page=14 xpos=0 ypos=0 single-column full-justified font-larger aligned-line headchar-lower
I-Body	highest-scored hypothesis will approach 1, and thus our objective will approach the	page=14 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	error defined in Equation (17). This γ can be adjusted in a way that allows for more	page=14 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
E-Body	effective search of the parameter space, as described in more detail in Section 5.5.	page=14 xpos=0 ypos=1 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	3.4 Margin-Based Loss	page=14 xpos=0 ypos=1 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	The zero–one loss in Section 3.1 was based on whether the oracle received a higher	page=14 xpos=0 ypos=2 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	score than other hypotheses. The idea of margin, which is behind the classification	page=14 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	paradigm of support vector machines (SVMs) (Joachims 1998), takes this a step further,	page=14 xpos=0 ypos=2 single-column full-justified aligned-line year headchar-lower tailchar-comma
I-Body	finding parameters that explicitly maximize the distance, or margin, between correct	page=14 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	and incorrect candidates. The main advantage of margin-based methods is that they are	page=14 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	able to consider the error function, and often achieve high accuracy. These advantages	page=14 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	make margin-based methods perhaps the second most popular loss used in current MT	page=14 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
E-Body	systems after direct minimization of error.	page=14 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	This margin-based objective can be defined as the loss:	page=14 xpos=0 ypos=3 single-column left-indent right-indent indented-line longer-tail headchar-capital tailchar-colon above-double-space above-line-space
B-Listitem	` margin (F, E, C; w) = N <sub>(C)</sub> <sup>1</sup> X <sub>i</sub> = <sup>N</sup> <sub>1</sub> <sub>h</sub> <sub>e</sub> ∗ <sub>,</sub> X d ∗ i∈ o (i) h e , d i∈ X c (i) \ o (i)	page=14 xpos=0 ypos=4 single-column left-indent right-indent box hanged-line shorter-tail line-double-space line-space above-double-space above-line-space
I-Listitem	max <sup>n</sup> 0, ∆ err(e (i) , e ∗ , e) − w > ∆ h( f (i) , e ∗ , d ∗ , e, d) o (25)	page=14 xpos=3 ypos=4 single-column left-indent font-largest indented-line longer-tail line-double-space line-space headchar-lower above-blank-line above-double-space above-line-space
I-Body	where we define	page=14 xpos=0 ypos=5 single-column right-indent hanged-line shorter-tail line-blank-line line-double-space line-space headchar-lower above-double-space above-line-space
B-Equation	∆ err(e, e <sup>∗</sup> , e 0 ) = err(e, e 0 ) − err(e, e ∗ ) (26)	page=14 xpos=2 ypos=5 single-column left-indent font-largest indented-line longer-tail line-double-space line-space above-line-space
I-Equation	∆ h( f , e <sup>∗</sup> , d <sup>∗</sup> , e 0 , d 0 ) = h( f , e ∗ , d ∗ ) − h( f , e 0 , d 0 ) (27)	page=14 xpos=2 ypos=5 single-column left-indent font-largest hanged-line line-space above-double-space above-line-space
B-Body	In Equation (25), we first specify that for each pair of oracle candidates o <sup>(i)</sup> , and non-	page=14 xpos=0 ypos=6 single-column full-justified font-largest hanged-line line-double-space line-space headchar-capital tailchar-hiphen
I-Body	oracle candidates c <sup>(i)</sup> \ o (i) , the margin w <sup>></sup> ∆ h( · ) between oracle e ∗ and non-oracle e	page=14 xpos=0 ypos=6 single-column full-justified font-largest aligned-line headchar-lower
I-Body	should be greater than the difference in the error ∆ err( · ). <sup>4</sup> We then define the loss as the	page=14 xpos=0 ypos=6 single-column full-justified font-largest aligned-line headchar-lower
I-Body	total amount that this margin is violated. In this loss calculation, the number of pairs is	page=14 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	N (C) = <sup>P</sup> <sup>N</sup> <sub>i</sub> = <sub>1</sub> | c (i) \ o (i) | · | o (i) | . Note that here err( · ) is not calculated on the corpus level,	page=14 xpos=0 ypos=6 single-column full-justified font-largest aligned-line headchar-capital tailchar-comma
I-Body	but on the sentence level, and may not directly correspond to our corpus-level error	page=14 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
E-Body	error( · ).	page=14 xpos=0 ypos=7 single-column right-indent font-larger aligned-line shorter-tail headchar-lower tailchar-period
B-Body	It is also common to consider the case where we calculate this loss with regards to	page=14 xpos=0 ypos=7 single-column left-indent indented-line longer-tail headchar-capital
I-Body	only a single translation candidate and oracle, and this is often called hinge loss. If we	page=14 xpos=0 ypos=7 single-column full-justified hanged-line headchar-lower
I-Body	define h ê <sup>(i)</sup> , d̂ <sup>(i)</sup> i ∈ c (i) \ o (i) as the 1-best translation candidate	page=14 xpos=0 ypos=7 single-column right-indent font-largest aligned-line shorter-tail headchar-lower above-blank-line above-double-space above-line-space
B-Equation	h ê <sup>(i)</sup> , d̂ <sup>(i)</sup> i = arg max w > h( f (i) , e, d) (28)	page=14 xpos=2 ypos=8 single-column left-indent font-largest indented-line longer-tail line-blank-line line-double-space line-space itemization headchar-lower
I-Equation	h e , d i∈ c <sup>(i)</sup> \ o (i)	page=14 xpos=4 ypos=8 single-column left-indent right-indent font-smallest indented-line shorter-tail itemization headchar-lower above-blank-line above-double-space above-line-space
B-Footnote	4 Equation (25) can be regarded as an instance of ranking loss described in Section 3.5 in which better	page=14 xpos=0 ypos=9 single-column left-indent right-indent font-smallest hanged-line longer-tail line-blank-line line-double-space line-space numbered-heading1
I-Footnote	translations are selected only from a set of oracle translations.	page=14 xpos=0 ypos=9 single-column left-indent right-indent font-smallest indented-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
Page	15	page=14 xpos=9 ypos=9 single-column left-indent indented-line longer-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Computational Linguistics Volume 42, Number 1	page=15 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Body	and h e <sup>∗</sup> (i) , d <sup>∗</sup> (i) i ∈ o (i) as the oracle translation	page=15 xpos=0 ypos=0 single-column right-indent font-largest aligned-line shorter-tail line-blank-line line-double-space line-space headchar-lower above-blank-line above-double-space above-line-space
B-Equation	h e <sup>∗</sup> (i) , d <sup>∗</sup> (i) i = arg min err(e (i) , e) (29)	page=15 xpos=3 ypos=1 single-column left-indent font-largest indented-line longer-tail line-blank-line line-double-space line-space itemization headchar-lower
I-Equation	h e , d i∈ o <sup>(i)</sup>	page=15 xpos=4 ypos=1 single-column left-indent right-indent font-smallest indented-line shorter-tail itemization headchar-lower above-blank-line above-double-space above-line-space
I-Body	the hinge loss can be defined as follows	page=15 xpos=0 ypos=1 single-column left-indent right-indent hanged-line shorter-tail line-blank-line line-double-space line-space headchar-lower above-double-space above-line-space
I-Equation	` hinge (F, E, C; w) = N <sup>1</sup> X <sub>i</sub> = <sup>N</sup> <sub>1</sub>	page=15 xpos=0 ypos=2 single-column left-indent right-indent box hanged-line shorter-tail line-double-space line-space above-double-space above-line-space
I-Equation	max <sup>n</sup> 0, ∆ err(e ∗ (i) , e ∗ , e) − w > ∆ h( f (i) , e ∗ (i) , d ∗ (i) , ê (i) , d̂ (i) ) o (30)	page=15 xpos=2 ypos=2 single-column left-indent font-largest indented-line longer-tail line-double-space line-space headchar-lower above-blank-line above-double-space above-line-space
B-Body	A special instance of this hinge loss that is widely used in machine translation, and	page=15 xpos=0 ypos=3 single-column full-justified hanged-line line-blank-line line-double-space line-space headchar-capital
I-Body	machine learning in general, is perceptron loss (Liang et al. 2006), which further re-	page=15 xpos=0 ypos=3 single-column full-justified aligned-line year headchar-lower tailchar-hiphen
I-Body	moves the term considering the error, and simply incurs a penalty if the 1-best candidate	page=15 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	receives a higher score than the oracle	page=15 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail headchar-lower above-double-space above-line-space
B-Equation	` perceptron (F, E, C; w) = N <sup>1</sup> X <sub>i</sub> = <sup>N</sup> <sub>1</sub> max n 0, − w > ∆ h( f (i) , e ∗ (i) , d ∗ (i) , ê (i) , d̂ (i) ) o	page=15 xpos=1 ypos=4 single-column centered left-indent right-indent box indented-line longer-tail line-double-space line-space
I-Equation	(31)	page=15 xpos=9 ypos=4 single-column left-indent indented-line longer-tail above-blank-line above-double-space above-line-space
B-Body	In addition to maximizing the margin itself, there has also been work on maximiz-	page=15 xpos=0 ypos=5 single-column left-indent hanged-line line-blank-line line-double-space line-space headchar-capital tailchar-hiphen
I-Body	ing the relative margin (Eidelman, Marton, and Resnik 2013). To explain the relative	page=15 xpos=0 ypos=5 single-column full-justified hanged-line year headchar-lower
I-Body	margin, we first define the worst hypothesis as	page=15 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail headchar-lower above-blank-line above-double-space above-line-space
B-Equation	h ĕ <sup>(i)</sup> , d̆ <sup>(i)</sup> i = arg max err(e (i) , e) (32)	page=15 xpos=3 ypos=5 single-column left-indent font-largest indented-line longer-tail line-blank-line line-double-space line-space itemization headchar-lower
I-Equation	h e , d i∈ c <sup>(i)</sup>	page=15 xpos=4 ypos=6 single-column left-indent right-indent font-smallest indented-line shorter-tail itemization headchar-lower above-double-space above-line-space
I-Body	and then calculate the spread ∆ err(e <sup>(i)</sup> , ė (i) , ĕ <sup>(i)</sup> ), which is the difference of errors be-	page=15 xpos=0 ypos=6 single-column full-justified font-largest hanged-line longer-tail line-double-space line-space headchar-lower tailchar-hiphen
I-Body	tween the oracle hypothesis h ė <sup>(i)</sup> , ḋ <sup>(i)</sup> i and worst hypothesis h ĕ (i) , d̆ <sup>(i)</sup> i . An additional	page=15 xpos=0 ypos=6 single-column full-justified font-largest aligned-line headchar-lower
I-Body	term can then be added to the objective function to penalize parameter settings with	page=15 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	large spreads. The intuition behind the relative margin criterion is that in addition	page=15 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	to increasing the margin, considering the spread reduces the variance between the	page=15 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	non-oracle hypotheses. Given an identical margin, having a smaller variance indi-	page=15 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	cates that an unseen hypothesis will be less likely to pass over the margin and be	page=15 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
E-Body	misclassified.	page=15 xpos=0 ypos=7 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	3.5 Ranking Loss	page=15 xpos=0 ypos=8 single-column right-indent aligned-line longer-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	Perceptron and margin losses attempted to distinguish between oracle and non-oracle	page=15 xpos=0 ypos=8 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	hypotheses. This can be considered a specific instance of the more general ranking	page=15 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	framework (Herbrich, Graepel, and Obermayer 1999; Freund et al. 2003; Burges et al.	page=15 xpos=0 ypos=8 single-column full-justified aligned-line year headchar-lower tailchar-period
I-Body	2005; Cao et al. 2007), where, for an arbitrary pair of translation candidates, a binary	page=15 xpos=0 ypos=9 single-column full-justified aligned-line year
I-Body	classifier is trained to distinguish which of the two candidates has the lower error. If a	page=15 xpos=0 ypos=9 single-column full-justified aligned-line headchar-lower above-blank-line above-double-space above-line-space
Page	16	page=15 xpos=0 ypos=9 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Neubig and Watanabe Optimization for Statistical Machine Translation	page=16 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Body	particular pair of candidates in the training data h e <sub>k</sub> , d k i and h e <sub>k</sub> 0 , d k 0 i is ranked in the	page=16 xpos=0 ypos=0 single-column full-justified font-largest aligned-line line-blank-line line-double-space line-space headchar-lower
I-Body	correct order, the following condition is satisfied:	page=16 xpos=0 ypos=0 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-colon above-blank-line above-double-space above-line-space
B-Equation	err(e <sup>(i)</sup> , e <sub>k</sub> ) < err(e (i) , e <sub>k</sub> 0 )	page=16 xpos=2 ypos=1 single-column left-indent right-indent font-largest indented-line line-blank-line line-double-space line-space headchar-lower above-line-space
I-Equation	⇐⇒ w <sup>></sup> h( f (i) , e <sub>k</sub> , d k ) > w > h( f (i) , e <sub>k</sub> 0 , d k 0 )	page=16 xpos=2 ypos=1 single-column left-indent right-indent font-largest indented-line longer-tail line-space above-blank-line above-double-space above-line-space
B-Body	This can be expressed as	page=16 xpos=0 ypos=1 single-column right-indent hanged-line shorter-tail line-blank-line line-double-space line-space headchar-capital above-blank-line above-double-space above-line-space
B-Equation	err(e <sup>(i)</sup> , e <sub>k</sub> ) < err(e (i) , e <sub>k</sub> 0 )	page=16 xpos=2 ypos=2 single-column left-indent right-indent font-largest indented-line longer-tail line-blank-line line-double-space line-space headchar-lower above-line-space
I-Equation	⇐⇒ w <sup>></sup> h( f (i) , e <sub>k</sub> , d k ) > w > h( f (i) , e <sub>k</sub> 0 , d k 0 )	page=16 xpos=2 ypos=2 single-column left-indent right-indent font-largest indented-line longer-tail line-space above-double-space above-line-space
I-Equation	⇐⇒ w <sup>></sup> h( f (i) , e <sub>k</sub> , d k ) − w > h( f (i) , e <sub>k</sub> 0 , d k 0 ) > 0	page=16 xpos=2 ypos=2 single-column left-indent right-indent font-largest aligned-line longer-tail line-double-space line-space above-double-space above-line-space
I-Equation	⇐⇒ w <sup>></sup> <sup></sup> h( f (i) <sub>,</sub> e <sub>k</sub> , d k ) − h( f (i) , e <sub>k</sub> 0 , d k 0 )  > 0	page=16 xpos=2 ypos=3 single-column left-indent right-indent font-largest aligned-line line-double-space line-space above-double-space above-line-space
I-Equation	⇐⇒ w <sup>></sup> ∆ h( f (i) , e <sub>k</sub> , d k , e <sub>k</sub> 0 , d k 0 ) > 0	page=16 xpos=2 ypos=3 single-column left-indent right-indent font-largest aligned-line shorter-tail line-double-space line-space above-double-space above-line-space
I-Body	where ∆ h( f <sup>(i)</sup> , e <sub>k</sub> , d k , e <sub>k</sub> 0 , d k 0 ) can be treated as training data to be classified using any	page=16 xpos=0 ypos=3 single-column full-justified font-largest hanged-line longer-tail line-double-space line-space headchar-lower
I-Body	variety of binary classifier. Each binary decision made by this classifier becomes an	page=16 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	individual choice, and thus the ranking loss is the sum of these individual losses. As the	page=16 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	binary classifier, it is possible to use perceptron, hinge, or softmax losses between the	page=16 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
E-Body	correct and incorrect answers.	page=16 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	It should be noted that standard ranking techniques make a hard decision between	page=16 xpos=0 ypos=4 single-column left-indent indented-line longer-tail headchar-capital
I-Body	candidates with higher and lower error, which can cause problems when the ranking	page=16 xpos=0 ypos=5 single-column full-justified hanged-line headchar-lower
I-Body	by error does not correlate well with the ranking measured by the model. The cross-	page=16 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	entropy ranking loss solves this problem by softly fitting the model distribution to the	page=16 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
E-Body	distribution of ranking measured by errors (Green et al. 2014).	page=16 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail year headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	3.6 Mean Squared Error Loss	page=16 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	Finally, mean squared error loss is another method that does not make a hard zero–	page=16 xpos=0 ypos=6 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	one decision between the better and worse candidates, but instead attempts to directly	page=16 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	estimate the difference in scores (Bazrafshan, Chung, and Gildea 2012). This is done	page=16 xpos=0 ypos=6 single-column full-justified aligned-line year headchar-lower
I-Body	by first finding the difference in errors between the two candidates ∆ err(e <sup>(i)</sup> , e <sup>∗</sup> , e) and	page=16 xpos=0 ypos=6 single-column full-justified font-largest aligned-line headchar-lower
I-Body	defining the loss as the mean squared error of the difference between the inverse of the	page=16 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	difference in the errors and the difference in the model scores <sup>5</sup> :	page=16 xpos=0 ypos=7 single-column right-indent font-largest aligned-line shorter-tail headchar-lower tailchar-colon above-double-space above-line-space
B-Equation	` mse (F, E, C; w) = N <sup>1</sup> <sub>(C)</sub> X <sub>i</sub> = <sup>N</sup> <sub>1</sub> <sub>h</sub> <sub>e</sub> ∗ <sub>,</sub> X d ∗ i∈ o (i) h e , d X i∈ c (i)	page=16 xpos=0 ypos=7 single-column left-indent right-indent box indented-line shorter-tail line-double-space line-space above-double-space above-line-space
I-Equation	<sup></sup> −∆ <sub>err(e</sub> (i) , e ∗ , e) − w > ∆ h( f (i) , e ∗ , d ∗ , e, d)  <sup>2</sup> (33)	page=16 xpos=4 ypos=8 single-column left-indent font-largest indented-line longer-tail line-double-space line-space headchar-super above-blank-line above-double-space above-line-space
B-Footnote	5 We take the inverse because we would like model scores and errors to be inversely correlated.	page=16 xpos=0 ypos=9 single-column left-indent right-indent font-smallest hanged-line shorter-tail line-blank-line line-double-space line-space numbered-heading1 tailchar-period above-blank-line above-double-space above-line-space
Page	17	page=16 xpos=9 ypos=9 single-column left-indent indented-line longer-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Computational Linguistics Volume 42, Number 1	page=17 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
B-SectionHeader	4. Choosing Oracles	page=17 xpos=0 ypos=0 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space itemization above-blank-line above-double-space above-line-space
B-Body	In the previous section, many loss functions used oracle translations, which are defined	page=17 xpos=0 ypos=0 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	as a set of translations for any sentence that are “good.” Choosing oracle translations is	page=17 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
E-Body	not a trivial task, and in this section we describe the details involved.	page=17 xpos=0 ypos=1 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-SubsectionHeader	4.1 Bold vs. Local Updates	page=17 xpos=0 ypos=1 single-column right-indent aligned-line shorter-tail line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	In other structured learning tasks such as part-of-speech tagging or parsing, it is com-	page=17 xpos=0 ypos=1 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital tailchar-hiphen
I-Body	mon to simply use the correct answer as an oracle. In translation, this is equivalent	page=17 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	to optimizing towards an actual human reference, which is called bold update (Liang	page=17 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	et al. 2006). It should be noted that even if we know the reference e, we still need to	page=17 xpos=0 ypos=2 single-column full-justified aligned-line year headchar-lower
I-Body	obtain a derivation d, and thus it is necessary to perform forced decoding (described in	page=17 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
E-Body	Section 2.4) to obtain this derivation.	page=17 xpos=0 ypos=2 single-column right-indent aligned-line shorter-tail headchar-capital tailchar-period
B-Body	However, bold update has a number of practical difficulties. For example, we are	page=17 xpos=0 ypos=3 single-column left-indent indented-line longer-tail headchar-capital
I-Body	not guaranteed that the decoder is able to actually produce the reference (for example,	page=17 xpos=0 ypos=3 single-column full-justified hanged-line headchar-lower tailchar-comma
I-Body	in the case of unknown words), in which case forced decoding will fail. In addition, even	page=17 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	if the hypothesis exists in the search space, it might require a large change in parameters	page=17 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	w to ensure that the reference gets a higher score than all other hypotheses. This is true	page=17 xpos=0 ypos=3 single-column full-justified aligned-line itemization headchar-lower
I-Body	in the case of non-literal translations, for example, which may be producible by the	page=17 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	decoder, but only by using a derivation that would normally receive an extremely low	page=17 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
E-Body	probability.	page=17 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	Local update is an alternative method that selects an oracle from a set of hypoth-	page=17 xpos=0 ypos=4 single-column left-indent indented-line longer-tail headchar-capital tailchar-hiphen
I-Body	eses produced during the normal decoding process. The space of hypotheses used to	page=17 xpos=0 ypos=4 single-column full-justified hanged-line headchar-lower
I-Body	select oracles is usually based on k-best lists, but can also include lattices or forests	page=17 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	output by the decoder as described in Section 2.4. Because of the previously mentioned	page=17 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	difficulties with bold update, it has been empirically observed that local update tends	page=17 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	to outperform bold update in online optimization (Liang et al. 2006). However, it also	page=17 xpos=0 ypos=5 single-column full-justified aligned-line year headchar-lower
I-Body	makes it necessary to select oracle translations from a set of imperfect decoder outputs,	page=17 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower tailchar-comma
E-Body	and we will describe this process in more detail in the following section.	page=17 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-SubsectionHeader	4.2 Selecting Oracles and Approximating Corpus-Level Errors	page=17 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail line-double-space line-space numbered-heading2 above-double-space above-line-space
B-Body	First, we define o <sup>(i)</sup> ⊆ c (i) as the set of oracle translations, derivation-translation pairs in	page=17 xpos=0 ypos=6 single-column full-justified font-largest aligned-line longer-tail line-double-space line-space headchar-capital
I-Body	c <sup>(i)</sup> that minimize the error function	page=17 xpos=0 ypos=6 single-column right-indent font-largest aligned-line shorter-tail itemization headchar-lower above-double-space above-line-space
B-Equation	O = arg min error <sup></sup> E,  o (i) ⊆ c (i)   <sup>N</sup> <sub>i</sub> = <sub>1</sub>  (34)	page=17 xpos=2 ypos=6 single-column left-indent font-largest indented-line longer-tail line-double-space line-space headchar-capital
I-Equation	{ o (i) ⊆ c (i) } <sup>N</sup> <sub>i</sub> = <sub>1</sub>	page=17 xpos=3 ypos=7 single-column left-indent right-indent font-largest indented-line shorter-tail above-double-space above-line-space
B-Body	One thing to note here is that error( · ) is a corpus-level error function. As mentioned	page=17 xpos=0 ypos=7 single-column full-justified font-larger hanged-line longer-tail line-double-space line-space headchar-capital
I-Body	in Section 2.5, evaluation measures for MT can be classified into those that are	page=17 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	decomposable on the sentence level, and those that are not. If this error function can	page=17 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	be composed as the sum of sentence-level errors, such as BLEU+1, choosing the oracle	page=17 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	is simple; we simply need to find the set of candidates that have the lowest error	page=17 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	independently sentence by sentence. <sup>6</sup>	page=17 xpos=0 ypos=8 single-column right-indent font-largest aligned-line shorter-tail headchar-lower above-blank-line above-double-space above-line-space
B-Footnote	6 More accurately, finding the oracle in the k-best list by enumeration of the hypotheses is easy, but finding	page=17 xpos=0 ypos=8 single-column left-indent right-indent font-smallest indented-line longer-tail line-blank-line line-double-space line-space numbered-heading1
I-Footnote	the oracle in a compressed data structure such as a lattice is computationally difficult, and approximation	page=17 xpos=0 ypos=9 single-column left-indent font-smallest indented-line headchar-lower
I-Footnote	algorithms are necessary (Leusch, Matusov, and Ney 2008; Li and Khudanpur 2009; Sokolov, Wisniewski,	page=17 xpos=0 ypos=9 single-column left-indent right-indent font-smallest aligned-line year headchar-lower tailchar-comma
I-Footnote	and Yvon 2012a).	page=17 xpos=0 ypos=9 single-column left-indent right-indent font-smallest aligned-line shorter-tail year headchar-lower tailchar-period above-blank-line above-double-space above-line-space
Page	18	page=17 xpos=0 ypos=9 single-column right-indent hanged-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
Figure	__Figure 2__	page=18 xpos=-1 ypos=-1 Figure-column left-over right-indent box page-top figure-area column-bottom
B-Header	Optimization for Statistical Machine Translation	page=18 xpos=5 ypos=0 single-column left-indent font-smallest column-top headchar-capital above-blank-line above-double-space above-line-space
B-Figure	. Random order	page=18 xpos=5 ypos=1 single-column left-indent right-indent indented-line shorter-tail line-blank-line line-double-space line-space above-blank-line above-double-space above-line-space
I-Figure	<sup>−</sup> <sup>1)</sup> , c <sub>k</sub> (i) , o (i 1 + 1) , . . . , o (N) 1 } <sup></sup>	page=18 xpos=4 ypos=2 single-column left-indent right-indent font-largest hanged-line longer-tail line-blank-line line-double-space line-space headchar-super above-double-space above-line-space
I-Figure	the oracle	page=18 xpos=5 ypos=2 single-column left-indent right-indent indented-line shorter-tail line-double-space line-space headchar-lower above-blank-line above-double-space above-line-space
I-Figure	Same error value	page=18 xpos=5 ypos=3 single-column left-indent right-indent hanged-line line-blank-line line-double-space line-space headchar-capital above-blank-line above-double-space above-line-space
I-Figure	converged	page=18 xpos=5 ypos=3 single-column left-indent right-indent indented-line line-blank-line line-double-space line-space headchar-lower above-blank-line above-double-space above-line-space
B-Caption	Figure 2	page=18 xpos=0 ypos=4 single-column right-indent font-smallest hanged-line shorter-tail line-blank-line line-double-space line-space string-figure headchar-capital
E-Caption	Greedy search for an oracle.	page=18 xpos=0 ypos=4 single-column right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-period above-blank-line above-double-space above-line-space
B-Body	However, when using a corpus-level error function we need a slightly more	page=18 xpos=0 ypos=5 single-column left-indent indented-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	sophisticated method, such as the greedy method of Venugopal and Vogel (2005).	page=18 xpos=0 ypos=6 single-column full-justified hanged-line year headchar-lower tailchar-period
I-Body	In this method (Figure 2), the oracle is first initialized either as an empty set or by	page=18 xpos=0 ypos=6 single-column full-justified aligned-line headchar-capital
I-Body	randomly picking from the candidates. Next, we iterate randomly through the	page=18 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	translation candidates in c <sup>(i)</sup> , try replacing the current oracle o (i) with the candidate,	page=18 xpos=0 ypos=6 single-column full-justified font-largest aligned-line headchar-lower tailchar-comma
I-Body	and check the change in the error function (Line 9), and if the error decreases, replace	page=18 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	the oracle with the tested candidate. This process is repeated until there is no change	page=18 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
E-Body	in O.	page=18 xpos=0 ypos=7 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	4.3 Selecting Oracles for Margin-Based Methods	page=18 xpos=0 ypos=7 single-column right-indent aligned-line longer-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	Considering the hinge loss of Equation (30), the 1-best and oracle candidates are	page=18 xpos=0 ypos=8 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	acquired according to Equation (28) and Equation (29), respectively, and the loss	page=18 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	is calculated according to Equation (31). In order to minimize this loss, we would like to	page=18 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	select the pair with the largest loss, and update so that the loss gets smaller. However, it	page=18 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	is not necessarily the case that the candidates with the maximum model score h ê <sup>(i)</sup> , d̂ <sup>(i)</sup> i	page=18 xpos=0 ypos=8 single-column full-justified font-largest aligned-line headchar-lower
I-Body	and minimum loss h e <sup>∗</sup> (i) , d <sup>∗</sup> (i) i form the pair with the minimal margin. Thus, when using	page=18 xpos=0 ypos=9 single-column full-justified font-largest aligned-line headchar-lower
I-Body	margin-based objectives, it is common to modify the criterion for selecting candidates	page=18 xpos=0 ypos=9 single-column full-justified aligned-line headchar-lower above-blank-line above-double-space above-line-space
Page	19	page=18 xpos=9 ypos=9 single-column left-indent indented-line line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Computational Linguistics Volume 42, Number 1	page=19 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Body	to use in the update as follows (Chiang, Marton, and Resnik 2008; Chiang, Knight, and	page=19 xpos=0 ypos=0 single-column full-justified aligned-line line-blank-line line-double-space line-space year headchar-lower
I-Body	Wang 2009):	page=19 xpos=0 ypos=0 single-column right-indent aligned-line shorter-tail year headchar-capital tailchar-colon above-double-space above-line-space
B-Equation	h ē <sup>(i)</sup> , d̄ <sup>(i)</sup> i = arg max w > h( f (i) , e, d) + err(e (i) , e) (35)	page=19 xpos=2 ypos=1 single-column left-indent font-largest indented-line longer-tail line-double-space line-space itemization headchar-lower
I-Equation	h e , d i∈ c <sup>(i)</sup>	page=19 xpos=3 ypos=1 single-column left-indent right-indent font-smallest indented-line shorter-tail itemization headchar-lower above-double-space above-line-space
I-Equation	h ė <sup>(i)</sup> , ḋ <sup>(i)</sup> i = arg max w > h( f (i) , e, d) − err(e (i) , e) (36)	page=19 xpos=2 ypos=1 single-column left-indent font-largest hanged-line longer-tail line-double-space line-space itemization headchar-lower
I-Equation	h e , d i∈ c <sup>(i)</sup>	page=19 xpos=3 ypos=1 single-column left-indent right-indent font-smallest indented-line shorter-tail itemization headchar-lower above-double-space above-line-space
B-Body	Thus, we can replace h ê <sup>(i)</sup> , d̂ <sup>(i)</sup> i and h e ∗ (i) , d ∗ (i) i with h ē (i) , d̄ (i) i and h ė (i) , ḋ (i) i , resulting in	page=19 xpos=0 ypos=2 single-column full-justified font-largest hanged-line longer-tail line-double-space line-space headchar-capital
I-Body	a margin of	page=19 xpos=0 ypos=2 single-column right-indent aligned-line shorter-tail itemization headchar-lower above-double-space above-line-space
B-Equation	∆ err(e <sup>(i)</sup> , ė (i) , ē (i) ) − w <sup>></sup> ∆ h( f (i) , ė (i) , ḋ <sup>(i)</sup> , ē (i) , d̄ (i) ) (37)	page=19 xpos=2 ypos=2 single-column left-indent font-largest indented-line longer-tail line-double-space line-space above-blank-line above-double-space above-line-space
I-Body	which is the largest margin in the k-best list. Explaining more intuitively, this criterion	page=19 xpos=0 ypos=3 single-column full-justified hanged-line line-blank-line line-double-space line-space headchar-lower
I-Body	provides a bias towards selecting hypotheses with high error, making the learning	page=19 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	algorithm work harder to correctly classify very bad hypotheses than it does for hy-	page=19 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	potheses that are only slightly worse than the oracle. Inference methods that consider	page=19 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	the loss as in Equations (35) and (36) are called loss-augmented inference (Taskar	page=19 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	et al. 2005) methods, and can minimize losses with respect to the candidate with	page=19 xpos=0 ypos=4 single-column full-justified aligned-line year headchar-lower
I-Body	the largest violation. Gimpel and Smith (2012) take this a step further, defining a	page=19 xpos=0 ypos=4 single-column full-justified aligned-line year headchar-lower
I-Body	structured ramp loss that additionally considers Equations (28) and (29) within this	page=19 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
E-Body	framework.	page=19 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SectionHeader	5. Batch Methods	page=19 xpos=0 ypos=5 single-column right-indent aligned-line longer-tail line-blank-line line-double-space line-space itemization above-blank-line above-double-space above-line-space
B-Body	Now that we have explained the details of calculating loss functions used in ma-	page=19 xpos=0 ypos=5 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital tailchar-hiphen
I-Body	chine translation, we turn to the actual algorithms used in optimizing using these	page=19 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	loss functions. In this section, we cover batch learning approaches to MT optimiza-	page=19 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tion. Batch learning works by considering the entire training data on every update	page=19 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	of the parameters, in contrast to online learning (covered in the following section),	page=19 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	which considers only part of the data at any one time. In standard approaches to	page=19 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	batch learning, for every training example h f <sup>(i)</sup> , e (i) i we enumerate every translation	page=19 xpos=0 ypos=6 single-column full-justified font-largest aligned-line headchar-lower
I-Body	and derivation in the respective sets E ( f <sup>(i)</sup> ) and D( f (i) ), and attempt to adjust the	page=19 xpos=0 ypos=6 single-column full-justified font-largest aligned-line headchar-lower
I-Body	parameters so we can achieve the translations with the lowest error for the entire	page=19 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
E-Body	data.	page=19 xpos=0 ypos=7 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	However, as mentioned previously, the entire space of derivations is too large to	page=19 xpos=0 ypos=7 single-column left-indent indented-line longer-tail headchar-capital
I-Body	handle in practice. To resolve this problem, most batch learning algorithms for MT	page=19 xpos=0 ypos=7 single-column full-justified hanged-line headchar-lower
I-Body	follow the general procedure shown in Figure 3, performing iterations that alternate	page=19 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	between decoding and optimization (Och and Ney 2002). In line 6, GEN( f <sup>(i)</sup> , w (t) ) =	page=19 xpos=0 ypos=7 single-column full-justified font-largest aligned-line year headchar-lower
I-Body	<sup>n</sup> h <sub>e</sub> <sub>k</sub> (i) , d k (i) i o <sup>K</sup> <sub>k</sub> = <sub>1</sub> indicates that we use the current parameters w (t) to perform decoding	page=19 xpos=0 ypos=8 single-column full-justified font-largest aligned-line headchar-super
I-Body	of sentence f <sup>(i)</sup> , and obtain a subset of all derivations. For convenience, we will assume	page=19 xpos=0 ypos=8 single-column full-justified font-largest aligned-line headchar-lower
I-Body	that this subset is expressed using a k-best list kbest <sup>(i)</sup> , but it is also possible to use lattices	page=19 xpos=0 ypos=8 single-column full-justified font-largest aligned-line headchar-lower
E-Body	or forests, as explained in Section 2.4.	page=19 xpos=0 ypos=8 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	A k-best list with scores for each hypothesis can be used as an approximation	page=19 xpos=0 ypos=8 single-column left-indent indented-line longer-tail headchar-capital
I-Body	for the distribution over potential translations of f <sup>(i)</sup> according to the parameters w.	page=19 xpos=0 ypos=9 single-column full-justified font-largest hanged-line headchar-lower tailchar-period
I-Body	However, because the size of the k-best list is limited, and the presence of search	page=19 xpos=0 ypos=9 single-column full-justified aligned-line headchar-capital above-blank-line above-double-space above-line-space
Page	20	page=19 xpos=0 ypos=9 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
Figure	__Figure 3__	page=20 xpos=-1 ypos=-1 Figure-column left-over right-indent box page-top figure-area column-bottom
B-Header	Optimization for Statistical Machine Translation	page=20 xpos=5 ypos=0 single-column left-indent font-smallest column-top headchar-capital above-blank-line above-double-space above-line-space
B-Figure	, e <sup>(i)</sup> i <sup>o</sup> <sup>N</sup> <sub>i</sub> = <sub>1</sub> )	page=20 xpos=5 ypos=0 single-column left-indent right-indent font-largest aligned-line shorter-tail line-blank-line line-double-space line-space above-double-space above-line-space
I-Figure	. k-best List	page=20 xpos=5 ypos=1 single-column left-indent right-indent indented-line longer-tail line-double-space line-space above-blank-line above-double-space above-line-space
I-Figure	with w <sup>(t)</sup>	page=20 xpos=5 ypos=1 single-column left-indent right-indent font-largest indented-line line-blank-line line-double-space line-space headchar-lower
I-Figure	. k-best Merging	page=20 xpos=5 ypos=1 single-column left-indent right-indent hanged-line above-blank-line above-double-space above-line-space
I-Figure	+ λΩ (w)	page=20 xpos=5 ypos=2 single-column left-indent right-indent indented-line shorter-tail line-blank-line line-double-space line-space
I-Figure	. Optimization	page=20 xpos=5 ypos=2 single-column left-indent right-indent longer-tail above-blank-line above-double-space above-line-space
B-Caption	Figure 3	page=20 xpos=0 ypos=3 single-column right-indent font-smallest hanged-line shorter-tail line-blank-line line-double-space line-space string-figure headchar-capital
E-Caption	Batch optimization.	page=20 xpos=0 ypos=3 single-column right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-period above-blank-line above-double-space above-line-space
I-Body	errors in decoding means that we are not even guaranteed to find the highest-scoring	page=20 xpos=0 ypos=3 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-lower
I-Body	hypotheses, this approximation is far from perfect. The effect of this approximation is	page=20 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	particularly obvious if the lack of coverage of the k-best list is systematic. For example, if	page=20 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	the hypotheses in the k-best list are all much too short, optimization may attempt to fix	page=20 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	this by adjusting the parameters to heavily favor very long hypotheses, far overshooting	page=20 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
E-Body	the actual optimal parameters. <sup>7</sup>	page=20 xpos=0 ypos=4 single-column right-indent font-largest aligned-line shorter-tail headchar-lower
B-Body	As a way to alleviate the problems caused by this approximation, in line 7 we merge	page=20 xpos=0 ypos=4 single-column left-indent indented-line longer-tail headchar-capital
I-Body	the k-best lists from multiple decoding iterations, finding a larger and more accurate set	page=20 xpos=0 ypos=5 single-column full-justified hanged-line headchar-lower
I-Body	C of derivations. Given C and the training data h F, E i , we perform minimization of the	page=20 xpos=0 ypos=5 single-column full-justified font-larger aligned-line headchar-capital
I-Body	Ω (w) regularized loss function ` ( · ) and obtain new parameters w <sup>(t</sup> + 1) (line 9). Gener-	page=20 xpos=0 ypos=5 single-column full-justified font-largest aligned-line tailchar-hiphen
I-Body	ation of k-best lists and optimization is performed until a hard limit of T iterations is	page=20 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	reached, or until training has converged. In this setting, usually convergence is defined	page=20 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	as any iteration in which the merged k-best list does not change, or when the parameters	page=20 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
E-Body	w do not change (Och 2003).	page=20 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail itemization year headchar-lower tailchar-period
B-Body	Within this batch optimization framework, the most critical challenge is to find	page=20 xpos=0 ypos=6 single-column left-indent indented-line longer-tail headchar-capital
I-Body	an effective way to solve the optimization problem in line 9 of Figure 3. Section 5.1	page=20 xpos=0 ypos=6 single-column full-justified hanged-line headchar-lower
I-Body	describes methods for directly optimizing the error function. There are also methods	page=20 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	for optimizing other losses such as those based on probabilistic models (Section 5.2),	page=20 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower tailchar-comma
E-Body	error margins (Section 5.3), ranking (Section 5.4), and risk (Section 5.5).	page=20 xpos=0 ypos=7 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	5.1 Error Minimization	page=20 xpos=0 ypos=7 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numbered-heading2
B-Body	5.1.1 Minimum Error Rate Training Overview. Minimum error rate training (MERT) (Och	page=20 xpos=0 ypos=7 single-column full-justified aligned-line longer-tail numbered-heading3
I-Body	2003) is one of the first, and is currently the most widely used, method for MT	page=20 xpos=0 ypos=7 single-column full-justified aligned-line year
I-Body	optimization, and focuses mainly on direct minimization of the error described in	page=20 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	Section 3.1. Because error is not continuously differentiable, MERT uses optimization	page=20 xpos=0 ypos=8 single-column full-justified aligned-line headchar-capital
I-Body	methods that do not require the calculation of a gradient, such as iterative line search	page=20 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower above-blank-line above-double-space above-line-space
B-Footnote	7 Liu et al. (2012) propose a method to avoid over-aggressive moves in parameter space by considering the	page=20 xpos=0 ypos=9 single-column left-indent right-indent font-smallest indented-line line-blank-line line-double-space line-space numbered-heading1 year
I-Footnote	balance between increase in the evaluation score and the similarity with the parameters on the previous	page=20 xpos=0 ypos=9 single-column left-indent right-indent font-smallest indented-line shorter-tail headchar-lower
I-Footnote	iteration.	page=20 xpos=0 ypos=9 single-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
Page	21	page=20 xpos=9 ypos=9 single-column left-indent indented-line longer-tail line-blank-line line-double-space line-space numeric-only page-bottom
Figure	__Figure 4__	page=21 xpos=-1 ypos=-1 Figure-column left-over right-indent box page-top figure-area column-bottom
B-Header	Volume 42, Number 1	page=21 xpos=7 ypos=0 single-column left-indent font-smallest column-top headchar-capital above-blank-line above-double-space above-line-space
B-Figure	. Initialize randomly	page=21 xpos=5 ypos=1 single-column left-indent right-indent hanged-line shorter-tail line-blank-line line-double-space line-space
I-Figure	. Until convergence	page=21 xpos=5 ypos=1 single-column left-indent right-indent indented-line
I-Figure	. For each dimension	page=21 xpos=5 ypos=1 single-column left-indent right-indent hanged-line
I-Figure	w <sup>(t)</sup> + γ b <sup>m</sup> ) . Search	page=21 xpos=5 ypos=1 single-column left-indent right-indent font-largest hanged-line itemization headchar-lower above-double-space above-line-space
I-Figure	+ γ̂ m b <sup>m</sup> ) . Descent	page=21 xpos=5 ypos=1 single-column left-indent right-indent font-largest line-double-space line-space above-line-space
I-Figure	. Update	page=21 xpos=6 ypos=2 single-column left-indent right-indent indented-line line-space above-double-space above-line-space
I-Figure	C; ŵ) then	page=21 xpos=5 ypos=2 single-column left-indent right-indent hanged-line shorter-tail line-double-space line-space headchar-capital above-blank-line above-double-space above-line-space
B-Caption	Figure 4	page=21 xpos=0 ypos=3 single-column right-indent font-smallest hanged-line shorter-tail line-blank-line line-double-space line-space string-figure headchar-capital
E-Caption	Minimum error rate training (MERT).	page=21 xpos=0 ypos=3 single-column right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-period above-blank-line above-double-space above-line-space
I-Body	inspired by Powell’s method (Och 2003; Press et al. 2007), or the Downhill-Simplex	page=21 xpos=0 ypos=4 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space year headchar-lower
I-Body	method (Nelder-Mead method) (Press et al. 2007; Zens, Hasan, and Ney 2007; Zhao	page=21 xpos=0 ypos=4 single-column full-justified aligned-line year headchar-lower
E-Body	and Chen 2009).	page=21 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail year headchar-lower tailchar-period
B-Body	The algorithm for MERT using line search is shown in Figure 4. Here, we assume	page=21 xpos=0 ypos=4 single-column left-indent indented-line longer-tail headchar-capital
I-Body	that w and h( · ) are M-dimensional, and b <sup>m</sup> is an M-dimensional vector where the	page=21 xpos=0 ypos=5 single-column full-justified font-largest hanged-line headchar-lower
I-Body	m-th element is 1 and the rest of the elements are zero. For the T iterations, we decide	page=21 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	the dimension m of the feature vector (line 6), and for each possible weight vector	page=21 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	w <sup>(j)</sup> + γ b <sup>m</sup> choose the γ ∈ <sub>R</sub> that minimizes ` <sub>error</sub> ( · ) using line search (line 7). Then,	page=21 xpos=0 ypos=5 single-column full-justified font-largest aligned-line itemization headchar-lower tailchar-comma
I-Body	among the γ for each of the M search dimensions, we perform an update using γ̂ that	page=21 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	affords the largest reduction in error (lines 9 and 10). This algorithm can be deemed	page=21 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	a variety of steepest descent, which is a standard method used in most implemen-	page=21 xpos=0 ypos=6 single-column full-justified aligned-line itemization headchar-lower tailchar-hiphen
I-Body	tations of MERT (Koehn et al. 2007). Another alternative is a variant of coordinate	page=21 xpos=0 ypos=6 single-column full-justified aligned-line year headchar-lower
I-Body	descent (e.g., Powell’s method), in which search and update is performed in each	page=21 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
E-Body	dimension.	page=21 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	One feature of MERT is that it is known to easily fall into local optima of the	page=21 xpos=0 ypos=6 single-column left-indent indented-line longer-tail headchar-capital
I-Body	error function. Because of this, it is standard to choose R starting points (line 4), perform	page=21 xpos=0 ypos=7 single-column full-justified hanged-line headchar-lower
I-Body	optimization starting at each of these starting points, and finally choose the ŵ that	page=21 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	minimizes the loss from the weights acquired from each of the R random restarts.	page=21 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower tailchar-period
I-Body	The R starting points are generally chosen so that one of the points is the best w	page=21 xpos=0 ypos=7 single-column full-justified aligned-line headchar-capital
I-Body	from the previous iteration, and the remaining R − 1 have each element of w chosen	page=21 xpos=0 ypos=7 single-column full-justified font-larger aligned-line headchar-lower
I-Body	randomly and uniformly from some interval, although it has also been shown that more	page=21 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	intelligent choice of initial points can result in better final scores (Moore and Quirk	page=21 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
E-Body	2008).	page=21 xpos=0 ypos=8 single-column right-indent aligned-line shorter-tail year tailchar-period above-blank-line above-double-space above-line-space
B-Body	5.1.2 Line Search for MERT. Although the majority of this process is relatively straight-	page=21 xpos=0 ypos=8 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space numbered-heading3 tailchar-hiphen
I-Body	forward, the line search in Line 7 of Figure 4 requires a bit more explanation. In this	page=21 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	step, we would like to choose the γ that results in the ordering of hypotheses in c <sup>(i)</sup> that	page=21 xpos=0 ypos=9 single-column full-justified font-largest aligned-line headchar-lower
I-Body	achieves the lowest error. In order to do so, MERT uses an algorithm that allows for	page=21 xpos=0 ypos=9 single-column full-justified aligned-line headchar-lower above-blank-line above-double-space above-line-space
Page	22	page=21 xpos=0 ypos=9 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Neubig and Watanabe Optimization for Statistical Machine Translation	page=22 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Body	exact enumeration of which of the K candidates in c <sup>(i)</sup> will be chosen for each value of	page=22 xpos=0 ypos=0 single-column full-justified font-largest aligned-line line-blank-line line-double-space line-space headchar-lower
I-Body	γ . Concretely, we define	page=22 xpos=0 ypos=0 single-column right-indent aligned-line shorter-tail above-blank-line above-double-space above-line-space
B-Equation	arg max <sup></sup> w <sup>(j)</sup> + γ b <sup>m</sup>   <sup>></sup> h( f (i) , e, d) (38)	page=22 xpos=2 ypos=1 single-column left-indent font-largest indented-line longer-tail line-blank-line line-double-space line-space headchar-lower
I-Equation	h e , d i∈ c <sup>(i)</sup>	page=22 xpos=2 ypos=1 single-column left-indent right-indent font-smallest shorter-tail itemization headchar-lower above-double-space above-line-space
I-Equation	= arg max w <sup>(j)</sup> <sup>></sup> h( f (i) , e, d) + γ · h <sub>m</sub> ( f (i) , e, d) (39)	page=22 xpos=2 ypos=1 single-column left-indent font-largest longer-tail line-double-space line-space
I-Equation	h e , d i∈ c <sup>(i)</sup> | {z } | {z }	page=22 xpos=3 ypos=1 single-column left-indent right-indent font-smallest indented-line shorter-tail itemization headchar-lower
I-Equation	intercept slope	page=22 xpos=4 ypos=2 single-column left-indent right-indent font-smallest indented-line shorter-tail headchar-lower above-double-space above-line-space
I-Equation	= arg max a( f <sup>(i)</sup> , e, d) + γ · b( f (i) , e, d) (40)	page=22 xpos=2 ypos=2 single-column left-indent font-largest hanged-line longer-tail line-double-space line-space
I-Equation	h e , d i∈ c <sup>(i)</sup>	page=22 xpos=3 ypos=2 single-column left-indent right-indent font-smallest indented-line shorter-tail itemization headchar-lower above-double-space above-line-space
I-Body	where each hypothesis h e, d i in c <sup>(i)</sup> of Equation (40) is expressed as a line with intercept	page=22 xpos=0 ypos=2 single-column full-justified font-largest hanged-line longer-tail line-double-space line-space headchar-lower
I-Body	a( f <sup>(i)</sup> , e, d)( = w (j) <sup>></sup> h( f (i) , e, d)) and slope b( f (i) , e, d)( = h <sub>m</sub> ( f (i) , e, d)) with γ as a parame-	page=22 xpos=0 ypos=3 single-column full-justified font-largest aligned-line headchar-lower tailchar-hiphen
I-Body	ter. Equation (40) is a function that returns the translation candidate with the highest	page=22 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	score. We can define a function g( γ ; f <sup>(i)</sup> ) that corresponds to the score of this highest-	page=22 xpos=0 ypos=3 single-column full-justified font-largest aligned-line headchar-lower tailchar-hiphen
I-Body	scoring candidate as follows:	page=22 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-colon above-double-space above-line-space
B-Equation	g( γ ; f <sup>(i)</sup> ) = max a( f (i) , e, d) + γ · b( f (i) , e, d) (41)	page=22 xpos=2 ypos=3 single-column left-indent font-largest indented-line longer-tail line-double-space line-space headchar-lower
I-Equation	h e , d i∈ c <sup>(i)</sup>	page=22 xpos=3 ypos=4 single-column left-indent right-indent font-smallest indented-line shorter-tail itemization headchar-lower above-blank-line above-double-space above-line-space
B-Body	We can see that Equation (41) is a piecewise linear function (Papineni 1999; Och 2003),	page=22 xpos=0 ypos=4 single-column full-justified hanged-line longer-tail line-blank-line line-double-space line-space year headchar-capital tailchar-comma
I-Body	as at any given γ ∈ R the translation candidate with the highest score a( · ) + γ · b( · )	page=22 xpos=0 ypos=4 single-column full-justified font-largest aligned-line headchar-lower
I-Body	will be selected, and this score corresponds to the line that is in the highest position at	page=22 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	that particular γ . In Figure 5, we show an example with the following four translation	page=22 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	candidates:	page=22 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-colon above-blank-line above-double-space above-line-space
B-Equation	c <sub>1</sub> <sup>(i)</sup> : 2 . 5 + γ · ( − 0 . 8), c 2 (i) : 1 + γ · ( − 0 . 2)	page=22 xpos=2 ypos=5 single-column centered left-indent right-indent font-largest indented-line longer-tail line-blank-line line-double-space line-space itemization headchar-lower
I-Equation	(42)	page=22 xpos=9 ypos=5 single-column left-indent indented-line longer-tail
I-Equation	c <sup>(i)</sup> <sub>3</sub> : 2 + γ · ( − 0 . 5), c (i) <sub>4</sub> : − 0 . 5 + γ · 0 . 2	page=22 xpos=2 ypos=5 single-column centered left-indent right-indent font-largest hanged-line shorter-tail itemization headchar-lower above-blank-line above-double-space above-line-space
B-Body	If we set γ to a very small value such as −∞ , the candidate with the smallest slope, in	page=22 xpos=0 ypos=6 single-column full-justified font-larger hanged-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	this example c <sup>(i)</sup> <sub>1</sub> , will be chosen. Furthermore, if we make γ gradually larger, we will	page=22 xpos=0 ypos=6 single-column full-justified font-largest aligned-line headchar-lower above-blank-line above-double-space above-line-space
B-Caption	Figure 5	page=22 xpos=0 ypos=9 single-column right-indent font-smallest aligned-line shorter-tail line-blank-line line-double-space line-space string-figure headchar-capital
E-Caption	An example of hypotheses as lines.	page=22 xpos=0 ypos=9 single-column right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-period above-blank-line above-double-space above-line-space
Page	23	page=22 xpos=9 ypos=9 single-column left-indent indented-line longer-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Computational Linguistics Volume 42, Number 1	page=23 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Body	see that c <sup>(i)</sup> <sub>1</sub> continues to be the highest scoring candidate until we reach the intersection	page=23 xpos=0 ypos=0 single-column full-justified font-largest aligned-line line-blank-line line-double-space line-space headchar-lower
I-Body	of c <sup>(i)</sup> <sub>1</sub> and c (i) 3 at	page=23 xpos=0 ypos=0 single-column right-indent font-largest aligned-line shorter-tail headchar-lower above-double-space above-line-space
B-Equation	2 . 5 − 2	page=23 xpos=4 ypos=1 single-column left-indent right-indent font-larger indented-line longer-tail line-double-space line-space numbered-heading1
I-Equation	≈ 1 . 667 (43)	page=23 xpos=5 ypos=1 single-column left-indent font-larger indented-line longer-tail
I-Equation	( − 0 . 5) − ( − 0 . 8)	page=23 xpos=3 ypos=1 single-column left-indent right-indent font-larger hanged-line shorter-tail above-double-space above-line-space
I-Body	after which c <sup>(i)</sup> <sub>3</sub> will be the highest scoring candidate. If we continue increasing γ , we	page=23 xpos=0 ypos=1 single-column full-justified font-largest hanged-line longer-tail line-double-space line-space headchar-lower
E-Body	will continue by selecting c <sub>2</sub> <sup>(i)</sup> and c <sub>4</sub> (i) starting at their corresponding intersections.	page=23 xpos=0 ypos=1 single-column right-indent font-largest aligned-line shorter-tail headchar-lower tailchar-period
B-Body	A function like Equation (41) that chooses the highest-scoring line for each span	page=23 xpos=0 ypos=2 single-column left-indent indented-line longer-tail headchar-capital
I-Body	over γ is called an envelope, and can be used to compactly express the results we	page=23 xpos=0 ypos=2 single-column full-justified hanged-line headchar-lower
I-Body	will obtain by rescoring c <sup>(i)</sup> according to a particular γ (Figure 6a). After finding the	page=23 xpos=0 ypos=2 single-column full-justified font-largest aligned-line headchar-lower
I-Body	envelope, for each line that participates in the envelope, we can calculate the sufficient	page=23 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	statistics necessary for calculating the loss ` <sub>error</sub> ( · ) and error error( · ). For example, given	page=23 xpos=0 ypos=2 single-column full-justified font-largest aligned-line headchar-lower
I-Body	the envelope in Figure 6a, Figure 6b is an example of the sentence-wise loss with	page=23 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
E-Body	respect to γ .	page=23 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	The envelope shown in Equation (41) can also be viewed as the problem of	page=23 xpos=0 ypos=3 single-column left-indent indented-line longer-tail headchar-capital
I-Body	finding a convex hull in computational geometry. A standard and efficient algorithm	page=23 xpos=0 ypos=3 single-column full-justified hanged-line headchar-lower
I-Body	for finding a convex hull of multiple lines is the sweep line algorithm (Bentley and	page=23 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	Ottmann 1979; Macherey et al. 2008) (see Figure 7). Here, we assume L is a set of the	page=23 xpos=0 ypos=3 single-column full-justified aligned-line year headchar-capital
I-Body	lines corresponding to the K translation candidates in c <sup>(i)</sup> , each line l ∈ L is expressed as	page=23 xpos=0 ypos=4 single-column full-justified font-largest aligned-line headchar-lower
I-Body	h a(l), b(l), γ (l) i with intercept a(l) = a( f <sup>(i)</sup> , e, d), and slope b(l) = b( f (i) , e, d). Furthermore,	page=23 xpos=0 ypos=4 single-column full-justified font-largest aligned-line itemization headchar-lower tailchar-comma
I-Body	we define γ (l) as an intersection initialized to −∞ . S ORT L INES (L) in Figure 3 sorts the	page=23 xpos=0 ypos=4 single-column full-justified font-larger aligned-line headchar-lower
I-Body	lines in the order of their slope b(l), and if two lines l <sub>k</sub> <sub>1</sub> have the same slope, l k 2 chooses	page=23 xpos=0 ypos=4 single-column full-justified font-largest aligned-line headchar-lower
I-Body	the one with the larger intercept a(l <sub>k</sub> <sub>1</sub> ) > a(l k 2 ) and deletes the other. We next process the	page=23 xpos=0 ypos=4 single-column full-justified font-largest aligned-line headchar-lower
I-Body	sorted set of lines L <sup>0</sup> ( | L 0 | ≤ K) in order of ascending slope (lines 4–18). If we assume H	page=23 xpos=0 ypos=4 single-column full-justified font-largest aligned-line headchar-lower above-blank-line above-double-space above-line-space
B-Figure	)	page=23 xpos=0 ypos=5 single-column left-indent right-indent font-smallest shorter-tail line-blank-line line-double-space line-space
I-Figure	(i)	page=23 xpos=0 ypos=5 single-column right-indent font-smallest shorter-tail
I-Figure	f	page=23 xpos=0 ypos=6 single-column left-indent right-indent font-smallest longer-tail headchar-lower
I-Figure	<sup>;</sup> γ	page=23 xpos=0 ypos=6 single-column left-indent right-indent font-larger aligned-line headchar-super
I-Figure	g(	page=23 xpos=0 ypos=6 single-column left-indent right-indent font-smallest aligned-line headchar-lower above-blank-line above-double-space above-line-space
I-Figure	γ	page=23 xpos=3 ypos=6 single-column left-indent right-indent font-smaller indented-line longer-tail line-blank-line line-double-space line-space above-double-space above-line-space
I-Figure	(a) Envelope	page=23 xpos=2 ypos=7 single-column left-indent right-indent font-smallest hanged-line shorter-tail line-double-space line-space itemization above-blank-line above-double-space above-line-space
I-Figure	)	page=23 xpos=0 ypos=7 single-column left-indent right-indent font-smallest hanged-line shorter-tail line-blank-line line-double-space line-space
I-Figure	· (	page=23 xpos=0 ypos=7 single-column left-indent right-indent font-smallest aligned-line
I-Figure	` error	page=23 xpos=0 ypos=7 single-column left-indent right-indent font-largest aligned-line above-blank-line above-double-space above-line-space
I-Figure	γ	page=23 xpos=4 ypos=8 single-column left-indent right-indent font-smaller indented-line longer-tail line-blank-line line-double-space line-space above-double-space above-line-space
I-Figure	(b) Loss	page=23 xpos=3 ypos=9 single-column left-indent right-indent font-smallest hanged-line shorter-tail line-double-space line-space itemization above-double-space above-line-space
B-Caption	Figure 6	page=23 xpos=0 ypos=9 single-column right-indent font-smallest hanged-line shorter-tail line-double-space line-space string-figure headchar-capital
E-Caption	An example of line search in MERT.	page=23 xpos=0 ypos=9 single-column right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-period above-blank-line above-double-space above-line-space
Page	24	page=23 xpos=0 ypos=9 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
Figure	__Figure 7__	page=24 xpos=-1 ypos=-1 Figure-column left-over right-indent box page-top figure-area column-bottom
B-Header	Optimization for Statistical Machine Translation	page=24 xpos=5 ypos=0 single-column left-indent font-smallest column-top headchar-capital above-blank-line above-double-space above-line-space
B-Figure	), γ (l <sup>(k)</sup> ) i} Kk = <sub>1</sub> )	page=24 xpos=5 ypos=0 single-column left-indent right-indent font-largest indented-line shorter-tail line-blank-line line-double-space line-space above-blank-line above-double-space above-line-space
I-Figure	Intersection of H <sub>j</sub> − <sub>1</sub> and l	page=24 xpos=5 ypos=1 single-column left-indent right-indent font-largest longer-tail line-blank-line line-double-space line-space headchar-capital above-line-space
I-Figure	. l is in the envelope	page=24 xpos=5 ypos=1 single-column left-indent right-indent indented-line line-space above-blank-line above-double-space above-line-space
I-Figure	. The leftmost line	page=24 xpos=5 ypos=2 single-column left-indent right-indent indented-line line-blank-line line-double-space line-space above-blank-line above-double-space above-line-space
B-Caption	Figure 7	page=24 xpos=0 ypos=4 single-column right-indent font-smallest hanged-line shorter-tail line-blank-line line-double-space line-space string-figure headchar-capital
E-Caption	The “sweep line” algorithm.	page=24 xpos=0 ypos=4 single-column right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-period above-blank-line above-double-space above-line-space
I-Body	is the envelope expressed as the set of lines it contains, we find the line that intersects	page=24 xpos=0 ypos=5 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-lower
I-Body	with line under consideration at the highest point (lines 6–12), and update the envelope	page=24 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
E-Body	H. As L contains at most K lines, H’s size is also at most K.	page=24 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail headchar-capital tailchar-period
B-Body	Given a particular input sentence f <sup>(i)</sup> , its set of translation candidates c (i) , and	page=24 xpos=0 ypos=5 single-column left-indent font-largest indented-line longer-tail headchar-capital
I-Body	the resulting envelope H <sup>(i)</sup> , we can also define the set of intersections between lines	page=24 xpos=0 ypos=5 single-column full-justified font-largest hanged-line headchar-lower
I-Body	in the envelope as γ <sub>1</sub> <sup>(i)</sup> < · · · < γ j (i) < · · · < γ <sub>|</sub> (i) <sub>H</sub> (i) | . We also define ∆` (i) j to be the change	page=24 xpos=0 ypos=5 single-column full-justified font-largest aligned-line headchar-lower
I-Body	in the loss function that occurs when we move from one span [ γ <sup>(i)</sup> <sub>j</sub> − <sub>1</sub> , γ (i) j ) to the next	page=24 xpos=0 ypos=6 single-column full-justified font-largest aligned-line headchar-lower
I-Body	[ γ <sub>j</sub> <sup>(i)</sup> , γ (i) j + <sub>1</sub> ). If we first calculate the loss incurred when setting γ = −∞ , then process the	page=24 xpos=0 ypos=6 single-column full-justified font-largest aligned-line
I-Body	spans in increasing order, keeping track of the difference ∆` <sub>j</sub> <sup>(i)</sup> incurred at each span	page=24 xpos=0 ypos=6 single-column full-justified font-largest aligned-line headchar-lower
E-Body	boundary, it is possible to efficiently calculate the loss curve over all spans of γ .	page=24 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	In addition, whereas all explanation of line search to this point has focused on	page=24 xpos=0 ypos=7 single-column left-indent indented-line longer-tail headchar-capital
I-Body	the procedure for a single sentence, by calculating the envelopes for each sentence in	page=24 xpos=0 ypos=7 single-column full-justified hanged-line headchar-lower
I-Body	the data 1 ≤ i ≤ N, and combining these envelopes into a single plane, it is relatively	page=24 xpos=0 ypos=7 single-column full-justified font-larger aligned-line headchar-lower
I-Body	simple to perform this processing on the corpus level as well. It should be noted that	page=24 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	for corpus-based evaluation measures such as BLEU, when performing corpus-level	page=24 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	processing, we do not keep track of the change in the loss, but the change in the	page=24 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	sufficient statistics required to calculate the loss for each sentence. In the case of BLEU,	page=24 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	the sufficient statistics amount to n-gram counts c <sub>n</sub> , n-gram matches m n , and reference	page=24 xpos=0 ypos=8 single-column full-justified font-largest aligned-line headchar-lower
I-Body	lengths r. We then calculate the loss curve ` <sub>error</sub> ( · ) for the entire corpus based on these	page=24 xpos=0 ypos=8 single-column full-justified font-largest aligned-line headchar-lower
I-Body	sufficient statistics, and find a γ that minimizes Equation (17) based on this curve. By	page=24 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	repeating this line search for each parameter until we can no longer obtain a decrease,	page=24 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	it is possible to find a local minimum in the loss function, even for non-convex or non-	page=24 xpos=0 ypos=9 single-column full-justified aligned-line headchar-lower tailchar-hiphen
E-Body	differential functions.	page=24 xpos=0 ypos=9 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
Page	25	page=24 xpos=9 ypos=9 single-column left-indent indented-line longer-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Computational Linguistics Volume 42, Number 1	page=25 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Body	5.1.3 MERT’s Weaknesses and Extensions. Although MERT is widely used as the standard	page=25 xpos=0 ypos=0 single-column full-justified aligned-line line-blank-line line-double-space line-space numbered-heading3
I-Body	optimization procedure for MT, it also has a number of weaknesses, and a number of	page=25 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower
E-Body	extensions to the MERT framework have been proposed to resolve these problems.	page=25 xpos=0 ypos=0 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	The first weakness of MERT is the randomness in the optimization process. Because	page=25 xpos=0 ypos=1 single-column left-indent indented-line longer-tail headchar-capital
I-Body	each iteration of the training algorithm generally involves a number of random restarts,	page=25 xpos=0 ypos=1 single-column full-justified hanged-line headchar-lower tailchar-comma
I-Body	the results will generally change over multiple training runs, with the changes often	page=25 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	being quite significant. Some research has shown that this randomness can be stabilized	page=25 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	somewhat by improving the ability of the line-search algorithm to find a globally good	page=25 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	solution by choosing random seeds more intelligently (Moore and Quirk 2008; Foster	page=25 xpos=0 ypos=1 single-column full-justified aligned-line year headchar-lower
I-Body	and Kuhn 2009) or by searching in directions that consider multiple features at once,	page=25 xpos=0 ypos=2 single-column full-justified aligned-line year headchar-lower tailchar-comma
I-Body	instead of using the simple coordinate ascent as described in Figure 4 (Cer, Jurafsky, and	page=25 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	Manning 2008). Orthogonally to actual improvement of the results, Clark et al. (2011)	page=25 xpos=0 ypos=2 single-column full-justified aligned-line year headchar-capital
I-Body	suggest that because randomness is a fundamental feature of MERT and other opti-	page=25 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	mization algorithms for MT, it is better experimental practice to perform optimization	page=25 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	multiple times, and report the resulting means and standard deviations over various	page=25 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
E-Body	optimization runs.	page=25 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	It is also possible to optimize the MERT objective using other optimization al-	page=25 xpos=0 ypos=3 single-column left-indent indented-line longer-tail headchar-capital tailchar-hiphen
I-Body	gorithms. For example, Suzuki, Duh, and Nagata (2011) present a method for using	page=25 xpos=0 ypos=3 single-column full-justified hanged-line year headchar-lower
I-Body	particle swarm optimization, a distributed algorithm where many “particles” are each	page=25 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	associated with a parameter vector, and the particle updates its vector in a way such that	page=25 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	it moves towards the current local and global optima. Another alternative optimization	page=25 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	algorithm is Galley and Quirk’s (2011) method for using linear programming to per-	page=25 xpos=0 ypos=4 single-column full-justified aligned-line year headchar-lower tailchar-hiphen
I-Body	form search for optimal parameters over more than one dimension, or all dimensions	page=25 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	at a single time. However, as MERT remains a fundamentally computationally hard	page=25 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	problem, this method takes large amounts of time for larger training sets or feature	page=25 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
E-Body	spaces.	page=25 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	It should be noted that instability in MERT is not entirely due to the fact that search	page=25 xpos=0 ypos=5 single-column left-indent indented-line longer-tail headchar-capital
I-Body	is random, but also due to the fact that k-best lists are poor approximations of the whole	page=25 xpos=0 ypos=5 single-column full-justified hanged-line headchar-lower
I-Body	space of possible translations. One way to improve this approximation is by performing	page=25 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	MERT over an exponentially large number of hypotheses encoded in a translation lattice	page=25 xpos=0 ypos=5 single-column full-justified aligned-line headchar-capital
I-Body	(Macherey et al. 2008) or hypergraph (Kumar et al. 2009). It is possible to perform MERT	page=25 xpos=0 ypos=5 single-column full-justified aligned-line year
I-Body	over these sorts of packed data structures by observing the fact that the envelopes	page=25 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	used in MERT can be expressed as a semiring (Dyer 2010a; Sokolov and Yvon 2011),	page=25 xpos=0 ypos=6 single-column full-justified aligned-line year headchar-lower tailchar-comma
I-Body	allowing for exact calculation of the full envelope for all hypotheses in a lattice or	page=25 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	hypergraph using polynomial-time dynamic programming (the forward algorithm or	page=25 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	inside algorithm, respectively). There has also been work to improve the accuracy of the	page=25 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	k-best approximation by either sampling k-best candidates from the translation lattice	page=25 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	(Chatterjee and Cancedda 2010), or performing forced decoding to find derivations that	page=25 xpos=0 ypos=7 single-column full-justified aligned-line year
I-Body	achieve the reference translation, and adding them to the k-best list (Liang, Zhang, and	page=25 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
E-Body	Zhao 2012).	page=25 xpos=0 ypos=7 single-column right-indent aligned-line shorter-tail year headchar-capital tailchar-period
B-Body	The second weakness of MERT is that it has no concept of regularization, causing	page=25 xpos=0 ypos=7 single-column left-indent indented-line longer-tail headchar-capital
I-Body	it to overfit the training data if there are too many features, and there have been several	page=25 xpos=0 ypos=7 single-column full-justified hanged-line headchar-lower
I-Body	attempts to incorporate regularization to ameliorate this problem. Cer, Jurafsky, and	page=25 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	Manning (2008) propose a method to incorporate regularization by not choosing the	page=25 xpos=0 ypos=8 single-column full-justified aligned-line year headchar-capital
I-Body	plateau in the loss curve that minimizes the loss itself, but choosing the point con-	page=25 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	sidering the loss values for a few surrounding plateaus, helping to avoid points that	page=25 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	have a low loss but are surrounded by plateaus with higher loss. It is also possible to	page=25 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	incorporate regularization into MERT-style line search using an SVM-inspired margin-	page=25 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	based objective (Hayashi et al. 2009) or by using scale-invariant regularization methods	page=25 xpos=0 ypos=9 single-column full-justified aligned-line year headchar-lower
E-Body	such as L <sub>0</sub> or a scaled version of L 2 (Galley et al. 2013).	page=25 xpos=0 ypos=9 single-column right-indent font-largest aligned-line shorter-tail year headchar-lower tailchar-period above-blank-line above-double-space above-line-space
Page	26	page=25 xpos=0 ypos=9 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Neubig and Watanabe Optimization for Statistical Machine Translation	page=26 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
B-Body	The final weakness of MERT is that it has computational problems when scaling	page=26 xpos=0 ypos=0 single-column left-indent indented-line line-blank-line line-double-space line-space headchar-capital
I-Body	to large numbers of features. When using only a standard set of 20 or so features, MERT	page=26 xpos=0 ypos=0 single-column full-justified hanged-line headchar-lower
I-Body	is able to perform training in reasonable time, but the number of line searches, and thus	page=26 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower
I-Body	time, required in Algorithm 4 scales linearly with the number of features. Thus train-	page=26 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ing of hundreds of features is time-consuming, and there are no published results	page=26 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	training standard MERT on thousands or millions of features. It should be noted, how-	page=26 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ever, that Galley et al. (2013) report results for thousands of features by choosing	page=26 xpos=0 ypos=1 single-column full-justified aligned-line year headchar-lower
I-Body	intelligent search directions by calculating the gradient of expected BLEU, as explained	page=26 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
E-Body	in Section 5.5.2.	page=26 xpos=0 ypos=1 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	5.2 Gradient-Based Batch Optimization	page=26 xpos=0 ypos=2 single-column right-indent aligned-line longer-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	In the previous section, MERT optimized a loss function that was exactly equiv-	page=26 xpos=0 ypos=2 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital tailchar-hiphen
I-Body	alent to the error function, which is not continuously differentiable and thus precludes	page=26 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	the use of standard convex optimization algorithms used in other optimization prob-	page=26 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	lems. In contrast, other losses such as the softmax loss described in Section 3.2 and	page=26 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	risk-based losses described in Section 3.3 are differentiable, allowing for the use of	page=26 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	these algorithms for MT optimization (Smith and Eisner 2006; Blunsom and Osborne	page=26 xpos=0 ypos=3 single-column full-justified aligned-line year headchar-lower
E-Body	2008).	page=26 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail year tailchar-period
B-Body	Convex optimization is well covered in the standard machine learning literature,	page=26 xpos=0 ypos=4 single-column left-indent indented-line longer-tail headchar-capital tailchar-comma
I-Body	so we do not cover it in depth, but methods such as conjugate gradient (using first-	page=26 xpos=0 ypos=4 single-column full-justified hanged-line headchar-lower tailchar-hiphen
I-Body	order statistics) (Nocedal and Wright 2006) and the limited-memory Broyden-Fletcher-	page=26 xpos=0 ypos=4 single-column full-justified aligned-line year headchar-lower tailchar-hiphen
I-Body	Goldfarb-Shanno method (using second-order statistics) (Liu and Nocedal 1989) are	page=26 xpos=0 ypos=4 single-column full-justified aligned-line year headchar-capital
I-Body	standard options for optimizing these losses. These methods are equally applicable	page=26 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	when the loss is combined with a differentiable regularizer Ω (w), such as L <sub>2</sub> regu-	page=26 xpos=0 ypos=4 single-column full-justified font-largest aligned-line headchar-lower tailchar-hiphen
I-Body	larization. Using a non-differentiable regularizer such as L <sub>1</sub> makes optimization more	page=26 xpos=0 ypos=5 single-column full-justified font-largest aligned-line headchar-lower
I-Body	difficult, but can be handled by other algorithms such as orthant-wise limited-memory	page=26 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
E-Body	Quasi-Newton (Andrew and Gao 2007).	page=26 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail year headchar-capital tailchar-period
B-Body	In addition to the function being differentiable, if it is also convex we can be	page=26 xpos=0 ypos=5 single-column left-indent indented-line longer-tail headchar-capital
I-Body	guaranteed that these algorithms will not get stuck in local optima and instead they	page=26 xpos=0 ypos=5 single-column full-justified hanged-line headchar-lower
I-Body	will reach a globally optimal solution. In general, the softmax objective is convex if	page=26 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	there is only one element in the oracle set o <sup>(i)</sup> , and not necessarily convex if there are	page=26 xpos=0 ypos=6 single-column full-justified font-largest aligned-line headchar-lower
I-Body	multiple oracles. In the case of MT, as there are usually multiple translations e that	page=26 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	minimize error( · ), and multiple derivations d that result in the same translation e, o <sup>(i)</sup>	page=26 xpos=0 ypos=6 single-column full-justified font-largest aligned-line headchar-lower
I-Body	will generally contain multiple members. Thus, we cannot be entirely certain that we	page=26 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
E-Body	will reach a global optimum.	page=26 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	5.3 Margin-Based Optimization	page=26 xpos=0 ypos=7 single-column right-indent aligned-line longer-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	Minimizing the margin-based loss described in Section 3.4, possibly with the addition	page=26 xpos=0 ypos=7 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	of a regularizer, is also a relatively standard problem in the machine learning literature.	page=26 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower tailchar-period
I-Body	Methods to solve Equation (25) include sequential minimization optimization (Platt	page=26 xpos=0 ypos=8 single-column full-justified aligned-line headchar-capital
I-Body	1999), dual coordinate descent (Hsieh et al. 2008), as well as the quadratic program	page=26 xpos=0 ypos=8 single-column full-justified aligned-line year
E-Body	solvers used in standard SVMs (Joachims 1998).	page=26 xpos=0 ypos=8 single-column right-indent aligned-line shorter-tail year headchar-lower tailchar-period
B-Body	It should also be noted that there have also been several attempts to apply margin-	page=26 xpos=0 ypos=8 single-column left-indent indented-line longer-tail headchar-capital tailchar-hiphen
I-Body	based online learning algorithms explained in Section 6.3, but in a batch setting where	page=26 xpos=0 ypos=8 single-column full-justified hanged-line headchar-lower
I-Body	the whole training corpus is decoded before each iteration of optimization (Cherry and	page=26 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	Foster 2012; Gimpel and Smith 2012). We will explain these methods in more detail	page=26 xpos=0 ypos=9 single-column full-justified aligned-line year headchar-capital
I-Body	later, but it should be noted that the advantage of using these methods in a batch	page=26 xpos=0 ypos=9 single-column full-justified aligned-line headchar-lower above-blank-line above-double-space above-line-space
Page	27	page=26 xpos=9 ypos=9 single-column left-indent indented-line line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Computational Linguistics Volume 42, Number 1	page=27 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Body	setting mainly lies in simplicity; for online learning it is often necessary to directly	page=27 xpos=0 ypos=0 single-column full-justified aligned-line line-blank-line line-double-space line-space headchar-lower
I-Body	implement the optimization procedure within the decoder, whereas in a batch setting	page=27 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower
I-Body	the implementation of the decoding and optimization algorithm can be performed	page=27 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower
E-Body	separately.	page=27 xpos=0 ypos=1 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	5.4 Ranking and Linear Regression Optimization	page=27 xpos=0 ypos=1 single-column right-indent aligned-line longer-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	The rank-based loss described in Section 3.5 is essentially the combination of multiple	page=27 xpos=0 ypos=1 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	losses over binary decisions. These binary decisions can be solved using gradient-	page=27 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	based or margin-based methods, and thus optimization itself can be solved with the	page=27 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	algorithms described in the previous two sections. However, one important concern	page=27 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	in this setting is training time. At the worst, the number of pairwise comparisons for	page=27 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	any particular k-best list is k(k − 1) / 2, leading to unmanageably large amounts of time	page=27 xpos=0 ypos=2 single-column full-justified font-larger aligned-line headchar-lower
E-Body	required for training.	page=27 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	One way to alleviate this problem is by randomly sampling a small number of	page=27 xpos=0 ypos=3 single-column left-indent indented-line longer-tail headchar-capital
I-Body	these k(k − 1) / 2 hypotheses for use in optimization, which has been shown empirically	page=27 xpos=0 ypos=3 single-column full-justified font-larger hanged-line headchar-lower
I-Body	to allow for increases in training speed without decreases in accuracy. For example,	page=27 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	Hopkins and May (2011) describe a method dubbed pairwise ranking optimization	page=27 xpos=0 ypos=3 single-column full-justified aligned-line year headchar-capital
I-Body	that selects 5,000 pairs randomly for each sentence, and among these random pairs	page=27 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	using the 50 with the largest difference in error for training the classifier. Other selection	page=27 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	heuristics—for example, avoiding training on candidate pairs with overly different	page=27 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	scores (Nakov, Guzmán, and Vogel 2013), or performing Monte Carlo sampling (Roth	page=27 xpos=0 ypos=4 single-column full-justified aligned-line year headchar-lower
I-Body	et al. 2010; Haddow, Arun, and Koehn 2011)—are also possible and potentially increase	page=27 xpos=0 ypos=4 single-column full-justified aligned-line year headchar-lower
I-Body	accuracy. Recently, there has also been a method proposed that uses an efficient ranking	page=27 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	SVM formulation that alleviates the need for this sampling and explicitly performs	page=27 xpos=0 ypos=4 single-column full-justified aligned-line headchar-capital
E-Body	ranking over all pairs (Dreyer and Dong 2015).	page=27 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail year headchar-lower tailchar-period
B-Body	The mean squared error loss described in Section 3.6, which is similar to ranking	page=27 xpos=0 ypos=5 single-column left-indent indented-line longer-tail headchar-capital
I-Body	loss in that it will prefer a proper ordering of the k-best list, is much easier to optimize.	page=27 xpos=0 ypos=5 single-column full-justified hanged-line headchar-lower tailchar-period
I-Body	This loss can be minimized using standard techniques for solving least-squared-error	page=27 xpos=0 ypos=5 single-column full-justified aligned-line headchar-capital
E-Body	linear regression (Press et al. 2007).	page=27 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail year headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	5.5 Risk Minimization	page=27 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	The risk-based loss in Section 3.3 is also differentiable and thus can be optimized	page=27 xpos=0 ypos=6 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	by gradient-based methods. One thing to note here is that risk objective is highly	page=27 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	non-convex, and thus some care must be taken to ensure that optimization does not	page=27 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	fall into a local optimum. The motivation behind introducing the scaling parameter	page=27 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	γ in Equation (22) is that it allows us to control the “peakiness” of the distribution,	page=27 xpos=0 ypos=7 single-column full-justified aligned-line tailchar-comma
I-Body	which can be useful for optimization. Putting this formally, we first define the entropy	page=27 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	of p <sub>γ</sub> <sub>,</sub> w ( · ) as	page=27 xpos=0 ypos=7 single-column right-indent font-largest aligned-line shorter-tail headchar-lower above-double-space above-line-space
B-Equation	H(p <sub>γ</sub> <sub>,</sub> w ) = − N <sup>1</sup> X <sub>i</sub> = <sup>N</sup> <sub>1</sub> <sub>h</sub> <sub>e</sub> <sub>,</sub> X d i∈ c (i) p γ , w (e, d | f (i) , c (i) ) log p γ , w (e, d | f (i) , c (i) )	page=27 xpos=1 ypos=8 single-column centered left-indent right-indent box indented-line longer-tail line-double-space line-space headchar-capital
I-Equation	(44)	page=27 xpos=9 ypos=8 single-column left-indent indented-line longer-tail above-blank-line above-double-space above-line-space
I-Body	When γ takes a small value this entropy will be high, indicating that the loss function	page=27 xpos=0 ypos=8 single-column full-justified hanged-line line-blank-line line-double-space line-space headchar-capital
I-Body	is relatively smooth and less sensitive to local optima. Conversely, when γ → ∞ , the	page=27 xpos=0 ypos=9 single-column full-justified font-larger aligned-line headchar-lower
I-Body	entropy becomes lower, and the loss function becomes more peaky with more local	page=27 xpos=0 ypos=9 single-column full-justified aligned-line headchar-lower above-blank-line above-double-space above-line-space
Page	28	page=27 xpos=0 ypos=9 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Neubig and Watanabe Optimization for Statistical Machine Translation	page=28 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Body	optima. It has been noted that this fact can be used for effective optimization through	page=28 xpos=0 ypos=0 single-column full-justified aligned-line line-blank-line line-double-space line-space headchar-lower
I-Body	the process of deterministic annealing (Sindhwani, Keerthi, and Chapelle 2006). In	page=28 xpos=0 ypos=0 single-column full-justified aligned-line year headchar-lower
I-Body	deterministic annealing, the parameter γ is not set as a hyperparameter, and instead the	page=28 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower
I-Body	entropy H(p <sub>γ</sub> <sub>,</sub> w ) is directly used as a regularization function during the optimization	page=28 xpos=0 ypos=1 single-column full-justified font-largest aligned-line headchar-lower
I-Body	process (Smith and Eisner 2006; Li and Eisner 2009):	page=28 xpos=0 ypos=1 single-column right-indent aligned-line shorter-tail year headchar-lower tailchar-colon above-blank-line above-double-space above-line-space
B-Equation	arg min ` <sub>risk</sub> (F, E, C; γ , w) − T · H(p γ <sub>,</sub> w ) + λΩ (w) (45)	page=28 xpos=2 ypos=1 single-column left-indent font-largest indented-line longer-tail line-blank-line line-double-space line-space headchar-lower
I-Equation	γ∈ R <sub>≥</sub> <sub>0</sub> , w ∈ R <sup>M</sup>	page=28 xpos=2 ypos=1 single-column left-indent right-indent font-smaller hanged-line shorter-tail above-blank-line above-double-space above-line-space
B-Body	In Equation (45), T is the temperature, which can either be set as a hyperparameter,	page=28 xpos=0 ypos=2 single-column full-justified hanged-line longer-tail line-blank-line line-double-space line-space headchar-capital tailchar-comma
I-Body	or gradually decreased from ∞ to −∞ (or 0) through a process of cooling (Smith and	page=28 xpos=0 ypos=2 single-column full-justified font-larger aligned-line headchar-lower
I-Body	Eisner 2006). The motivation for cooling is that if we start with a large T, the earlier	page=28 xpos=0 ypos=2 single-column full-justified aligned-line year headchar-capital
I-Body	steps using a smoother function will allow us to approach the global optimum, and the	page=28 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
E-Body	later steps will allow us to approach the actual error function.	page=28 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	It should be noted that in Equation (24), and the discussion up to this point, we	page=28 xpos=0 ypos=3 single-column left-indent indented-line longer-tail headchar-capital
I-Body	have been using not the corpus-based error, but the sentence-based error err(e <sup>(i)</sup> , e).	page=28 xpos=0 ypos=3 single-column full-justified font-largest hanged-line headchar-lower tailchar-period
I-Body	There have also been attempts to make the risk minimization framework appli-	page=28 xpos=0 ypos=3 single-column full-justified aligned-line headchar-capital tailchar-hiphen
I-Body	cable to corpus-level error error( · ), specifically BLEU. We will discuss two such	page=28 xpos=0 ypos=3 single-column full-justified font-larger aligned-line headchar-lower
E-Body	methods.	page=28 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-Body	5.5.1 Linear BLEU. Linear BLEU (Tromble et al. 2008) provides an approximation for	page=28 xpos=0 ypos=4 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space numbered-heading3 year
I-Body	corpus-level BLEU that can be divided among sentences. Linear BLEU uses a Taylor	page=28 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	expansion to approximate the effect that the sufficient statistics of any particular sen-	page=28 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tence will have on corpus-level BLEU. We define r as the total length of the reference	page=28 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	translations, c as the total length of the candidates, and c <sub>n</sub> and m n (1 ≤ n ≤ 4) as the	page=28 xpos=0 ypos=5 single-column full-justified font-largest aligned-line headchar-lower
I-Body	translation candidate’s number of n-grams, and number of n-grams that match the	page=28 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	reference respectively. Taking the equation for corpus-level BLEU (Papineni et al. 2002)	page=28 xpos=0 ypos=5 single-column full-justified aligned-line year headchar-lower
I-Body	and assuming that the n-gram counts are approximately equal for 1 ≤ n ≤ 4, we get the	page=28 xpos=0 ypos=5 single-column full-justified font-larger aligned-line headchar-lower
I-Body	following approximation:	page=28 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-colon above-double-space above-line-space
B-Equation	log BLEU = min <sup></sup> 0, 1 − c <sup>r</sup>   + 1 <sub>4</sub> <sub>n</sub> X = <sup>4</sup> <sub>1</sub> log m c n n	page=28 xpos=2 ypos=6 single-column left-indent right-indent box indented-line longer-tail line-double-space line-space headchar-lower
I-Equation	(46)	page=28 xpos=9 ypos=6 single-column left-indent indented-line longer-tail above-double-space above-line-space
I-Equation	≈ min <sup></sup> 0, 1 − r <sub>c</sub>   + 1 <sub>4</sub> <sub>n</sub> X = <sup>4</sup> <sub>1</sub> log m c n	page=28 xpos=3 ypos=7 single-column left-indent right-indent box hanged-line shorter-tail line-double-space line-space
I-Equation	(47)	page=28 xpos=9 ypos=7 single-column left-indent indented-line longer-tail above-blank-line above-double-space above-line-space
I-Body	If we assume that when we add the sufficient statistics of a particular sentence e, the	page=28 xpos=0 ypos=7 single-column full-justified hanged-line line-blank-line line-double-space line-space headchar-capital
I-Body	corpus-level statistics change to r <sup>0</sup> , c 0 , c 0 <sub>n</sub> , and m 0 n , then we can express the change in	page=28 xpos=0 ypos=8 single-column full-justified font-largest aligned-line headchar-lower
I-Body	BLEU in the logarithm domain as follows	page=28 xpos=0 ypos=8 single-column right-indent aligned-line shorter-tail headchar-capital above-blank-line above-double-space above-line-space
B-Equation	∆ log BLEU = log BLEU <sup>0</sup> − log BLEU (48)	page=28 xpos=2 ypos=8 single-column left-indent font-largest indented-line longer-tail line-blank-line line-double-space line-space above-double-space above-line-space
I-Equation	≈ 1 X <sub>log</sub> <sup>m</sup> <sub>c</sub> 0 <sup>0</sup> n − 1 4 X <sub>n</sub> = <sup>4</sup> <sub>1</sub> log m c n	page=28 xpos=3 ypos=9 single-column left-indent right-indent box indented-line shorter-tail line-double-space line-space
I-Equation	4	page=28 xpos=4 ypos=9 single-column left-indent right-indent font-smallest indented-line shorter-tail numeric-only above-line-space
I-Equation	(49)	page=28 xpos=9 ypos=9 single-column left-indent indented-line longer-tail line-space
I-Equation	4	page=28 xpos=4 ypos=9 single-column left-indent right-indent hanged-line shorter-tail numeric-only
I-Equation	n = 1	page=28 xpos=4 ypos=9 single-column left-indent right-indent font-smallest indented-line longer-tail itemization headchar-lower above-blank-line above-double-space above-line-space
Page	29	page=28 xpos=9 ypos=9 single-column left-indent indented-line longer-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Computational Linguistics Volume 42, Number 1	page=29 xpos=0 ypos=0 single-column right-over font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
B-Body	If we make the assumption that there is no change in the brevity penalty, ∆ log BLEU	page=29 xpos=0 ypos=0 single-column right-over aligned-line line-blank-line line-double-space line-space headchar-capital
I-Body	relies solely on m <sub>n</sub> and c. ∆ log BLEU can then be approximated using a first-order	page=29 xpos=0 ypos=0 single-column right-over font-largest aligned-line headchar-lower
I-Body	Taylor expansion as follows:	page=29 xpos=0 ypos=0 single-column right-indent aligned-line shorter-tail headchar-capital tailchar-colon above-double-space above-line-space
B-Equation	∆ log BLEU ≈ (c <sup>0</sup> − c) <sup>∂</sup> <sup>log</sup> <sub>∂</sub> BLEU <sub>c</sub> 0 <sup>0</sup>     <sub> </sub> <sub>c</sub> 0 = c + X <sub>n</sub> = <sup>4</sup> <sub>1</sub> (m 0 n − m n ) ∂ log ∂ BLEU m 0 n 0       m 0 n = m n (50)	page=29 xpos=0 ypos=1 single-column left-indent right-over box indented-line longer-tail line-double-space line-space above-double-space above-line-space
I-Equation	= − c <sup>0</sup> − <sub>c</sub> c + 1 <sub>4</sub> X <sub>n</sub> = <sup>4</sup> <sub>1</sub> m n 0 m − n m n	page=29 xpos=2 ypos=1 single-column left-indent right-indent box indented-line shorter-tail line-double-space line-space
I-Equation	(51)	page=29 xpos=9 ypos=2 single-column left-indent right-over indented-line longer-tail above-blank-line above-double-space above-line-space
B-Body	As c <sup>0</sup> − c is the length of e, and m 0 <sub>n</sub> − m n is the number of n-grams (g n ) in e that match	page=29 xpos=0 ypos=2 single-column right-over font-largest hanged-line line-blank-line line-double-space line-space headchar-capital
I-Body	the n-grams (g <sup>(i)</sup> <sub>n</sub> ) in e (i) , the sentence-level error function err <sub>lBLEU</sub> ( · ) for linear BLEU is	page=29 xpos=0 ypos=2 single-column right-over font-largest aligned-line shorter-tail headchar-lower above-double-space above-line-space
B-Equation	err <sub>lBLEU</sub> (e <sup>(i)</sup> , e) = <sup>|</sup> e <sub>c</sub> | − 1 <sub>4</sub> X <sub>n</sub> = <sup>4</sup> <sub>1</sub> |{ g n ∈ e } ∩ m { n g n (i) ∈ e (i) }|	page=29 xpos=2 ypos=3 single-column left-indent right-indent box indented-line shorter-tail line-double-space line-space headchar-lower
I-Equation	(52)	page=29 xpos=9 ypos=3 single-column left-indent right-over indented-line longer-tail above-blank-line above-double-space above-line-space
I-Body	c and m <sub>n</sub> are set to a fixed value (Tromble et al. 2008). For example, in the batch opti-	page=29 xpos=0 ypos=3 single-column right-over font-largest hanged-line line-blank-line line-double-space line-space itemization year headchar-lower tailchar-hiphen
I-Body	mization algorithm of Figure 3 they can be calculated based on the k-best list generated	page=29 xpos=0 ypos=3 single-column right-over aligned-line headchar-lower
E-Body	prior to optimization.	page=29 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-Body	5.5.2 Expectations of Sufficient Statistics. DeNero, Chiang, and Knight (2009) present an	page=29 xpos=0 ypos=4 single-column right-over aligned-line longer-tail line-blank-line line-double-space line-space numbered-heading3 year
I-Body	alternative method that calculates not the expectation of the error itself, but the expecta-	page=29 xpos=0 ypos=4 single-column right-over aligned-line headchar-lower tailchar-hiphen
I-Body	tion of the sufficient statistics used in calculating the error. In contrast to sentence-level	page=29 xpos=0 ypos=4 single-column right-over aligned-line headchar-lower
I-Body	approximations or formulations such as linear BLEU, the expectation of the sufficient	page=29 xpos=0 ypos=5 single-column right-over aligned-line headchar-lower
I-Body	statistics can be calculated directly on the corpus level. Because of this, by maximizing	page=29 xpos=0 ypos=5 single-column right-over aligned-line headchar-lower
I-Body	the evaluation derived by these expected statistics, it is possible to directly optimize for	page=29 xpos=0 ypos=5 single-column right-over aligned-line headchar-lower
E-Body	a corpus-level error, in a manner similar to MERT (Pauls, Denero, and Klein 2009).	page=29 xpos=0 ypos=5 single-column right-over aligned-line shorter-tail itemization year headchar-lower tailchar-period
B-Body	When this is applied to BLEU in particular, this measure is often called xBLEU	page=29 xpos=0 ypos=5 single-column left-indent right-over indented-line longer-tail headchar-capital
I-Body	(Rosti et al. 2010, 2011) and the required sufficient statistics include n-gram counts and	page=29 xpos=0 ypos=5 single-column right-over hanged-line year
I-Body	matched n-gram counts. We define the kth translation candidate in c <sup>(i)</sup> as h e <sub>k</sub> , d k i , its	page=29 xpos=0 ypos=6 single-column right-over font-largest aligned-line headchar-lower
I-Body	score as s <sub>i,k</sub> = γ w <sup>></sup> h( f <sup>(i)</sup> , e k , d k ), and the probability in Equation (22) as	page=29 xpos=0 ypos=6 single-column right-indent font-largest aligned-line shorter-tail headchar-lower above-double-space above-line-space
B-Equation	p <sub>i,k</sub> = <sub>P</sub> K <sub>k</sub> 0 <sup>exp(s</sup> = <sub>1</sub> exp(s i,k ) i,k 0 )	page=29 xpos=3 ypos=6 single-column left-indent right-indent font-largest indented-line shorter-tail line-double-space line-space itemization headchar-lower
I-Equation	(53)	page=29 xpos=9 ypos=6 single-column left-indent right-over indented-line longer-tail above-blank-line above-double-space above-line-space
I-Body	Next, we define the expectation of the n-gram (g <sub>n</sub> ∈ e <sub>k</sub> ) frequency as c n,i,k , the expec-	page=29 xpos=0 ypos=7 single-column right-over font-largest hanged-line line-blank-line line-double-space line-space headchar-capital tailchar-hiphen
I-Body	tation of the number of n-gram matches as m <sub>n,i,k</sub> , and the expectation of the reference	page=29 xpos=0 ypos=7 single-column right-over font-largest aligned-line headchar-lower
I-Body	length as r <sub>i,k</sub> . These values can be calculated as:	page=29 xpos=0 ypos=7 single-column right-indent font-largest aligned-line shorter-tail headchar-lower tailchar-colon above-double-space above-line-space
B-Equation	c <sub>n,i,k</sub> = |{ g n ∈ e k }| · p i,k	page=29 xpos=3 ypos=7 single-column left-indent right-indent font-largest indented-line longer-tail line-double-space line-space itemization headchar-lower above-double-space above-line-space
I-Equation	m <sub>n,i,k</sub> = |{ g n ∈ e k } ∩ { g n <sup>(i)</sup> ∈ e (i) }| · p i,k	page=29 xpos=2 ypos=8 single-column left-indent right-indent font-largest hanged-line longer-tail line-double-space line-space itemization headchar-lower above-double-space above-line-space
I-Equation	r <sub>i,k</sub> = | e <sup>(i)</sup> | · p i,k	page=29 xpos=3 ypos=8 single-column left-indent right-indent font-largest indented-line shorter-tail line-double-space line-space itemization headchar-lower above-double-space above-line-space
B-Body	It should be noted that although these equations apply to k-best lists, it is also possible	page=29 xpos=0 ypos=8 single-column right-over hanged-line longer-tail line-double-space line-space headchar-capital
I-Body	to calculate statistics over lattices or forests using dynamic programming algorithms	page=29 xpos=0 ypos=9 single-column right-over aligned-line headchar-lower
E-Body	and tools such as the expectation semiring (Eisner 2002; Li and Eisner 2009).	page=29 xpos=0 ypos=9 single-column right-over aligned-line shorter-tail year headchar-lower tailchar-period above-blank-line above-double-space above-line-space
Page	30	page=29 xpos=0 ypos=9 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Neubig and Watanabe Optimization for Statistical Machine Translation	page=30 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Body	xBLEU is calculated from these expected sufficient statistics:	page=30 xpos=0 ypos=0 single-column left-indent right-indent indented-line shorter-tail line-blank-line line-double-space line-space headchar-lower tailchar-colon above-double-space above-line-space
B-Equation	xBLEU = <sup>Y</sup> <sub>P</sub> i N = <sub>i</sub> = 1 <sub>1</sub> P k K k = = 1 1 c n,i,k 4 P N P K m <sub>n,i,k</sub> ! <sup>14</sup> <sub>·</sub> φ <sub>1</sub> − <sub>P</sub> P N <sub>i</sub> = N i = <sub>1</sub> 1 P P K k k = K = 1 1 c r 1,i,k i,k !	page=30 xpos=1 ypos=0 single-column centered left-indent right-indent box indented-line longer-tail line-double-space line-space headchar-lower
I-Equation	(54)	page=30 xpos=9 ypos=1 single-column left-indent indented-line longer-tail above-line-space
I-Equation	n = 1	page=30 xpos=2 ypos=1 single-column left-indent right-indent font-smallest hanged-line shorter-tail line-space itemization headchar-lower above-blank-line above-double-space above-line-space
I-Body	where φ (x) is the brevity penalty. Compared to the risk minimization in Equation (24),	page=30 xpos=0 ypos=1 single-column full-justified hanged-line longer-tail line-blank-line line-double-space line-space headchar-lower tailchar-comma
I-Body	we define our optimization problem as the maximization of xBLEU:	page=30 xpos=0 ypos=1 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-colon above-blank-line above-double-space above-line-space
B-Equation	` xBLEU (F, E, C; γ , w) = − xBLEU (55)	page=30 xpos=3 ypos=2 single-column left-indent font-largest indented-line longer-tail line-blank-line line-double-space line-space above-blank-line above-double-space above-line-space
B-Body	It is possible to calculate a gradient for xBLEU, allowing for optimization using	page=30 xpos=0 ypos=2 single-column full-justified hanged-line line-blank-line line-double-space line-space headchar-capital
I-Body	gradient-based optimization methods, and we explain the full (somewhat involved)	page=30 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
E-Body	derivation in Appendix A.	page=30 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail string-appendix headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SectionHeader	6. Online Methods	page=30 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space itemization above-blank-line above-double-space above-line-space
B-Body	In the batch learning methods of Section 5, the steps of decoding and optimization	page=30 xpos=0 ypos=3 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	are performed sequentially over the entire training data. In contrast, online learning	page=30 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	performs updates not after the whole corpus has been processed, but over smaller	page=30 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	subsets of the training data deemed mini-batches. One of the major advantages of	page=30 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	online methods is that updates are performed on a much more fine-grained basis—it	page=30 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	is often the case that online methods converge faster than batch methods, particularly	page=30 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	on larger data sets. On the other hand, online methods have the disadvantage of being	page=30 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	harder to implement (they often must be implemented inside the decoder, whereas	page=30 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	batch methods can be separate), and also generally being less stable (with sensitivity	page=30 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
E-Body	to the order in which the training data is processed or other factors).	page=30 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	In the online learning algorithm in Figure 8, from the training data h F, E i =	page=30 xpos=0 ypos=5 single-column left-indent font-larger indented-line longer-tail headchar-capital
I-Body	<sup>n</sup> h <sub>f</sub> (i) <sub>,</sub> e (i) i o <sub>i</sub> <sup>N</sup> = <sub>1</sub> we first randomly choose a mini-batch consisting of K sentences of	page=30 xpos=0 ypos=5 single-column full-justified font-largest hanged-line headchar-super above-blank-line above-double-space above-line-space
B-Equation	, e <sup>(i)</sup> i <sup>o</sup> <sub>i</sub> <sup>N</sup> = <sub>1</sub> )	page=30 xpos=5 ypos=6 single-column left-indent right-indent font-largest indented-line shorter-tail line-blank-line line-double-space line-space column-bottom
Figure	__Figure 8__	page=30 xpos=0 ypos=6 Figure-column right-indent box column-top figure-area column-bottom
I-Figure	Sample ( |h F̃ <sup>(t)</sup> , Ẽ (t) i| = K)	page=30 xpos=5 ypos=7 single-column left-indent right-indent font-largest column-top headchar-capital
I-Figure	translation candidates	page=30 xpos=5 ypos=7 single-column left-indent right-indent indented-line headchar-lower above-blank-line above-double-space above-line-space
I-Figure	. Decode with w <sup>(t)</sup>	page=30 xpos=5 ypos=7 single-column left-indent right-indent font-largest indented-line line-blank-line line-double-space line-space above-double-space above-line-space
I-Figure	C̃ <sup>(t)</sup> ; w) + λΩ (w) .	page=30 xpos=4 ypos=8 single-column left-indent right-indent font-largest hanged-line line-double-space line-space headchar-capital tailchar-period above-blank-line above-double-space above-line-space
B-Caption	Figure 8	page=30 xpos=0 ypos=9 single-column right-indent font-smallest hanged-line shorter-tail line-blank-line line-double-space line-space string-figure headchar-capital
E-Caption	Online Learning.	page=30 xpos=0 ypos=9 single-column right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-period above-blank-line above-double-space above-line-space
Page	31	page=30 xpos=9 ypos=9 single-column left-indent indented-line longer-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Computational Linguistics Volume 42, Number 1	page=31 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Body	parallel data h F̃ <sup>(t)</sup> , Ẽ (t) i = <sup>n</sup> h f̃ ( j) , ẽ ( j) i o <sup>K</sup> <sub>j</sub> = <sub>1</sub> (line 4). We then decode each source sentence	page=31 xpos=0 ypos=0 single-column full-justified font-largest aligned-line line-blank-line line-double-space line-space headchar-lower
I-Body	f̃ <sup>(</sup> <sup>j)</sup> <sub>of</sub> the mini-batch and generate a k-best list (line 7), which is used in optimization	page=31 xpos=0 ypos=0 single-column full-justified font-largest aligned-line headchar-lower
I-Body	(line 9). In contrast to the batch learning algorithm in Figure 3, we do not merge the	page=31 xpos=0 ypos=1 single-column full-justified aligned-line
I-Body	k-bests from previous iterations. In addition, optimization is performed not over the	page=31 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	entire data, but only the data h F̃ <sup>(t)</sup> , Ẽ (t) i and its corresponding k-best, C̃ (t) . Like batch	page=31 xpos=0 ypos=1 single-column full-justified font-largest aligned-line headchar-lower
I-Body	learning, within the online learning framework, there are a number of optimization	page=31 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
E-Body	algorithms and objective functions that can be used.	page=31 xpos=0 ypos=1 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	The first thing we must consider during online learning is that because we only	page=31 xpos=0 ypos=2 single-column left-indent indented-line longer-tail headchar-capital
I-Body	optimize over the data in the mini-batch, it is not possible to directly optimize a corpus-	page=31 xpos=0 ypos=2 single-column full-justified hanged-line headchar-lower tailchar-hiphen
I-Body	level evaluation measure such as BLEU, and it is necessary to define an error function	page=31 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	that is compatible with the learning framework (see Section 6.1). Once the error has been	page=31 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	set, we can perform parameter updates according to a number of different algorithms	page=31 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	including the perceptron (Section 6.2), MIRA (Section 6.3), AROW (Section 6.4), and	page=31 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
E-Body	stochastic gradient descent (SGD) (Section 6.5).	page=31 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	6.1 Approximating the Error	page=31 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	In online learning, parameters are updated not with respect to the entire training corpus,	page=31 xpos=0 ypos=3 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital tailchar-comma
I-Body	but with respect to a subset of data sampled from the corpus. This has consequences	page=31 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	for the calculation of translation quality when using a corpus-level evaluation measure	page=31 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	such as BLEU. For example, when choosing an oracle for oracle-based optimization	page=31 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	methods, the oracles chosen when considering the entire corpus will be different from	page=31 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	the oracles chosen when considering a mini-batch. In general, the amount of difference	page=31 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	between the corpus-level and mini-batch level oracles will vary depending on the size	page=31 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	of a mini-batch, with larger mini-batches providing a better approximation (Tan et al.	page=31 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower tailchar-period
I-Body	2013; Watanabe 2012). Thus, when using smaller batches, especially single sentences, it	page=31 xpos=0 ypos=5 single-column full-justified aligned-line year
I-Body	is necessary to use methods to approximate the corpus-level error function as covered	page=31 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
E-Body	in the next two sections.	page=31 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-Body	6.1.1 Approximation with a Pseudo-Corpus. The first method to approximate the corpus-	page=31 xpos=0 ypos=6 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space numbered-heading3 tailchar-hiphen
I-Body	level evaluation measure relies on creating a pseudo-corpus, and using it to augment	page=31 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	the statistics used in the mini-batch error calculation (Watanabe et al. 2007). Specifically,	page=31 xpos=0 ypos=6 single-column full-justified aligned-line year headchar-lower tailchar-comma
I-Body	given the training data h F, E i = <sup>n</sup> h f (i) , e (i) i o <sup>N</sup> <sub>i</sub> = <sub>1</sub> , we define its corresponding pseudo-	page=31 xpos=0 ypos=6 single-column full-justified font-largest aligned-line headchar-lower tailchar-hiphen
I-Body	corpus Ē = <sup></sup> ē <sup>(i)</sup>   <sup>N</sup> <sub>i</sub> = <sub>1</sub> . Ē could be, for example, either the 1-best translation candidate	page=31 xpos=0 ypos=6 single-column full-justified font-largest aligned-line headchar-lower
I-Body	or the oracle calculated during the decoding step in line 7 of Figure 8. In the pseudo-	page=31 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	corpus approximation, the sentence-level error for the translation candidate e <sup>0</sup> acquired	page=31 xpos=0 ypos=7 single-column full-justified font-largest aligned-line headchar-lower
I-Body	by decoding the ith source sentence in the training data can be defined as the corpus-	page=31 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	level error acquired when in Ē, the ith sentence ē <sup>(i)</sup> is replaced with e <sup>0</sup>	page=31 xpos=0 ypos=7 single-column right-indent font-largest aligned-line shorter-tail headchar-lower above-double-space above-line-space
B-Equation	err <sub>pseudo</sub> (e <sup>(i)</sup> , e <sup>0</sup> ) = error E, { ē (1) , . . . , ē (i − 1) , e 0 , ē (i + 1) , . . . , ē (N) }  (56)	page=31 xpos=1 ypos=8 single-column left-indent font-largest indented-line longer-tail line-double-space line-space headchar-lower above-blank-line above-double-space above-line-space
I-Body	6.1.2 Approximation with Decay. When approximating the error function using a pseudo-	page=31 xpos=0 ypos=8 single-column full-justified hanged-line line-blank-line line-double-space line-space numbered-heading3 tailchar-hiphen
I-Body	corpus, it is necessary to remember translation candidates for every sentence in the	page=31 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	corpus. In addition, the size of differences in the sentence-level error becomes depen-	page=31 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	dent on the number of other sentences in the corpus, making it necessary to perform	page=31 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	scaling of the error, particularly for max-margin methods (Watanabe et al. 2007). As	page=31 xpos=0 ypos=9 single-column full-justified aligned-line year headchar-lower
I-Body	an alternative method that alleviates these problems, there has also been a method	page=31 xpos=0 ypos=9 single-column full-justified aligned-line headchar-lower above-blank-line above-double-space above-line-space
Page	32	page=31 xpos=0 ypos=9 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Neubig and Watanabe Optimization for Statistical Machine Translation	page=32 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Body	proposed that remembers a single set of sufficient statistics for the whole corpus, and	page=32 xpos=0 ypos=0 single-column full-justified aligned-line line-blank-line line-double-space line-space headchar-lower
I-Body	upon every update forces these statistics to decay according to some criterion (Chiang,	page=32 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower tailchar-comma
E-Body	Marton, and Resnik 2008; Chiang, Knight, and Wang 2009; Chiang 2012).	page=32 xpos=0 ypos=0 single-column right-indent aligned-line shorter-tail year headchar-capital tailchar-period
B-Body	We define s̄ as the sufficient statistics necessary to calculate a particular eval-	page=32 xpos=0 ypos=1 single-column left-indent indented-line longer-tail headchar-capital tailchar-hiphen
I-Body	uation measure. For example, in the case of BLEU, this would be a particular	page=32 xpos=0 ypos=1 single-column full-justified hanged-line headchar-lower
I-Body	translation candidate’s counts of the number of n-grams, the number of matched	page=32 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	n-grams, and the reference length. When evaluating the error for a candidate e <sup>0</sup> of the	page=32 xpos=0 ypos=1 single-column full-justified font-largest aligned-line headchar-lower
I-Body	ith sentence in the training data, we first decay the sufficient statistics	page=32 xpos=0 ypos=1 single-column right-indent aligned-line shorter-tail headchar-lower above-blank-line above-double-space above-line-space
B-Equation	s̄ ← ν × s̄ (57)	page=32 xpos=4 ypos=2 single-column left-indent font-larger indented-line longer-tail line-blank-line line-double-space line-space headchar-lower above-blank-line above-double-space above-line-space
I-Body	where ν < 1 is the amount of decay, taking a value such as 0.9 or 0.99. Next, based on	page=32 xpos=0 ypos=2 single-column full-justified hanged-line line-blank-line line-double-space line-space headchar-lower
I-Body	these sufficient statistics, we calculate the error of each candidate for the sentence by	page=32 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	summing the sufficient statistics of the sentence with the decayed sufficient statistics s̄.	page=32 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower tailchar-period
I-Body	For example, if we want to calculate BLEU using stat <sub>BLEU</sub> (e <sup>(i)</sup> , e <sup>0</sup> ) and the ith training	page=32 xpos=0 ypos=3 single-column full-justified font-largest aligned-line headchar-capital
I-Body	sentence h f <sup>(i)</sup> , e (i) i , if BLEU( · ) is a function calculating BLEU from a particular set of	page=32 xpos=0 ypos=3 single-column full-justified font-largest aligned-line headchar-lower
I-Body	sufficient statistics, we can use the following equation:	page=32 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-colon above-double-space above-line-space
B-Equation	err <sub>bleu</sub> 0 (e <sup>(i)</sup> , e <sup>0</sup> ) = 1 − BLEU(s̄ + stat BLEU (e (i) , e 0 )) (58)	page=32 xpos=2 ypos=3 single-column left-indent font-largest indented-line longer-tail line-double-space line-space headchar-lower above-double-space above-line-space
B-Body	After performing an update for a particular sentence, s̄ is then updated with the statis-	page=32 xpos=0 ypos=4 single-column full-justified hanged-line line-double-space line-space headchar-capital tailchar-hiphen
I-Body	tics from the 1-best hypothesis found during decoding. When the training data is large,	page=32 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	this function will place more emphasis on the recently generated examples, forgetting	page=32 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
E-Body	the older ones.	page=32 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	6.2 The Perceptron	page=32 xpos=0 ypos=5 single-column right-indent aligned-line longer-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	The most simple algorithm for online learning is the perceptron algorithm (shown in	page=32 xpos=0 ypos=5 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	Figure 9), which, as its name suggests, optimizes the perceptron loss of Section 3.4. The	page=32 xpos=0 ypos=5 single-column full-justified aligned-line string-figure headchar-capital
I-Body	most central feature of the algorithm is that when the 1-best and oracle translations	page=32 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower column-bottom above-double-space above-line-space
Figure	__Figure 9__	page=32 xpos=0 ypos=6 Figure-column right-indent box column-top line-double-space line-space figure-area column-bottom
B-Figure	e <sup>(i)</sup> i <sup>o</sup> <sup>N</sup> <sub>i</sub> = <sub>1</sub> )	page=32 xpos=5 ypos=6 single-column left-indent right-indent font-largest column-top itemization headchar-lower above-blank-line above-double-space above-line-space
I-Figure	. Assume K = 1	page=32 xpos=5 ypos=7 single-column left-indent right-indent indented-line longer-tail line-blank-line line-double-space line-space
I-Figure	. Decode with w <sup>(t)</sup>	page=32 xpos=5 ypos=7 single-column left-indent right-indent font-largest hanged-line above-line-space
I-Figure	. 1-best candidate	page=32 xpos=5 ypos=7 single-column left-indent right-indent line-space
I-Figure	. Oracle candidate	page=32 xpos=5 ypos=7 single-column left-indent right-indent above-double-space above-line-space
I-Figure	h( f , ê, d̂) . Update	page=32 xpos=5 ypos=8 single-column left-indent right-indent hanged-line line-double-space line-space headchar-lower above-blank-line above-double-space above-line-space
B-Caption	Figure 9	page=32 xpos=0 ypos=9 single-column right-indent font-smallest hanged-line shorter-tail line-blank-line line-double-space line-space string-figure headchar-capital
E-Caption	The perceptron algorithm.	page=32 xpos=0 ypos=9 single-column right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-period above-blank-line above-double-space above-line-space
Page	33	page=32 xpos=9 ypos=9 single-column left-indent indented-line longer-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Computational Linguistics Volume 42, Number 1	page=33 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Body	differ, we update the parameters at line 9. Because the gradient of the perceptron loss in	page=33 xpos=0 ypos=0 single-column full-justified aligned-line line-blank-line line-double-space line-space headchar-lower
I-Body	Equation (31) with respect to w is	page=33 xpos=0 ypos=0 single-column right-indent aligned-line shorter-tail headchar-capital above-blank-line above-double-space above-line-space
B-Equation	K	page=33 xpos=0 ypos=1 single-column left-indent right-indent font-smallest indented-line shorter-tail line-blank-line line-double-space line-space headchar-capital
I-Equation	1 X −∆ <sub>h(</sub> f̃ <sup>(i)</sup> <sub>,</sub> e ∗ (i) , d ∗ (i) , ê (i) , d̂ <sup>(i)</sup> )	page=33 xpos=0 ypos=1 single-column left-indent right-indent font-largest hanged-line longer-tail numbered-heading1
I-Equation	K	page=33 xpos=0 ypos=1 single-column left-indent right-indent shorter-tail headchar-capital
I-Equation	i = 1	page=33 xpos=0 ypos=1 single-column left-indent right-indent font-smallest indented-line longer-tail itemization headchar-lower above-double-space above-line-space
I-Equation	K	page=33 xpos=4 ypos=1 single-column left-indent right-indent font-smallest indented-line longer-tail line-double-space line-space headchar-capital
I-Equation	= 1 X − <sup></sup> <sub>h(</sub> f̃ (i) <sub>,</sub> e ∗ (i) , d ∗ (i) ) − h( f̃ (i) , ê (i) , d̂ (i) )  (59)	page=33 xpos=4 ypos=1 single-column left-indent font-largest hanged-line longer-tail
I-Equation	K	page=33 xpos=4 ypos=2 single-column left-indent right-indent indented-line shorter-tail headchar-capital
I-Equation	i = 1	page=33 xpos=4 ypos=2 single-column left-indent right-indent font-smallest indented-line longer-tail itemization headchar-lower above-double-space above-line-space
I-Body	this algorithm can also be viewed as updating the parameters based on this gradient,	page=33 xpos=0 ypos=2 single-column full-justified hanged-line longer-tail line-double-space line-space headchar-lower tailchar-comma
I-Body	making the parameters for oracle h e <sup>∗</sup> (i) , d <sup>∗</sup> (i) i stronger, and the parameters for the mis-	page=33 xpos=0 ypos=2 single-column full-justified font-largest aligned-line headchar-lower tailchar-hiphen
E-Body	taken translation h ê <sup>(i)</sup> , d̂ <sup>(i)</sup> i weaker.	page=33 xpos=0 ypos=2 single-column right-indent font-largest aligned-line shorter-tail headchar-lower tailchar-period
B-Body	In line 12, we return the final parameters to be used in translation. The most	page=33 xpos=0 ypos=3 single-column left-indent indented-line longer-tail headchar-capital
I-Body	straightforward approach here is to simply return the parameters resulting from the	page=33 xpos=0 ypos=3 single-column full-justified hanged-line headchar-lower
I-Body	final iteration of the perceptron training, but in a popular variant called the averaged	page=33 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	perceptron, we instead use the average of the parameters over all iterations in training	page=33 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	(Collins 2002). This averaging helps reduce overfitting of sentences that were viewed	page=33 xpos=0 ypos=3 single-column full-justified aligned-line year
I-Body	near the end of the training process, and is known to improve robustness to unknown	page=33 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
E-Body	data, resulting in higher translation accuracy (Liang et al. 2006).	page=33 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail year headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	6.3 MIRA	page=33 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	In line 9 of the perceptron algorithm in Figure 9, the parameters are updated using the	page=33 xpos=0 ypos=4 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	gradient with respect to w <sup>(t)</sup> (see Equation (59)). This update has the advantage of being	page=33 xpos=0 ypos=5 single-column full-justified font-largest aligned-line headchar-lower
I-Body	simple and being guaranteed to converge when the data is linearly separable, but it is	page=33 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	common for MT to handle feature sets that do not allow for linear separation of the	page=33 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	1-best and oracle hypotheses, resulting in instability in learning. The margin infused	page=33 xpos=0 ypos=5 single-column full-justified aligned-line
I-Body	relaxed algorithm (MIRA) (Crammer and Singer 2003; Crammer et al. 2006) is another	page=33 xpos=0 ypos=5 single-column full-justified aligned-line year headchar-lower
I-Body	online learning algorithm designed to help reduce these problems of instability. The	page=33 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	update in MIRA follows the same Equation (59), but also adds an additional term that	page=33 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	prevents the parameters w <sup>(t)</sup> from varying largely from their previous values 8	page=33 xpos=0 ypos=6 single-column right-indent font-largest aligned-line shorter-tail headchar-lower above-double-space above-line-space
B-Equation	` MIRA ( F̃, Ẽ, C̃; w, w <sup>(t)</sup> ) = <sup>1</sup> <sub>2</sub> k w − w (t) k 2 2 + <sup>λ</sup> MIRA K X <sub>i</sub> = <sup>K</sup> <sub>1</sub>	page=33 xpos=0 ypos=6 single-column left-indent right-indent box indented-line shorter-tail line-double-space line-space above-double-space above-line-space
I-Equation	max <sup>n</sup> 0, ∆ err(ẽ (i) , e ∗ (i) , ê (i) ) − w > ∆ h( f̃ (i) , e ∗ (i) , d ∗ (i) , ê (i) , d̂ (i) ) o (60)	page=33 xpos=2 ypos=7 single-column left-indent font-largest indented-line longer-tail line-double-space line-space headchar-lower above-blank-line above-double-space above-line-space
I-Body	It should be noted that although this is defined as a loss, it is dependent on the param-	page=33 xpos=0 ypos=7 single-column full-justified hanged-line line-blank-line line-double-space line-space headchar-capital tailchar-hiphen
E-Body	eters at the previous time step, in contrast to the losses in Section 3.	page=33 xpos=0 ypos=7 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	In line 9 of Figure 8 (or when K = 1, line 9 of Figure 9), the next parameters w <sup>(t</sup> + 1)	page=33 xpos=0 ypos=8 single-column left-indent font-largest indented-line longer-tail headchar-capital
I-Body	are chosen to minimize Equation (60) (Watanabe et al. 2007; Chiang, Marton, and Resnik	page=33 xpos=0 ypos=8 single-column full-justified hanged-line year headchar-lower
I-Body	2008; Chiang, Knight, and Wang 2009). λ <sub>MIRA</sub> is a hyperparameter that controls the	page=33 xpos=0 ypos=8 single-column full-justified font-largest aligned-line year above-blank-line above-double-space above-line-space
B-Footnote	8 The algorithm presented here is actually the PA-I algorithm (Crammer et al. 2006) applied to structured	page=33 xpos=0 ypos=9 single-column left-indent right-indent font-smallest indented-line shorter-tail line-blank-line line-double-space line-space numbered-heading1 year
I-Footnote	learning, and for historical reasons most research in parsing, MT, and other areas refer to this algorithm	page=33 xpos=0 ypos=9 single-column left-indent right-indent font-smallest indented-line headchar-lower
I-Footnote	not as PA-I, but MIRA (McDonald, Crammer, and Pereira 2005; Watanabe et al. 2007).	page=33 xpos=0 ypos=9 single-column left-indent right-indent font-smallest aligned-line shorter-tail year headchar-lower tailchar-period above-blank-line above-double-space above-line-space
Page	34	page=33 xpos=0 ypos=9 single-column right-indent hanged-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Neubig and Watanabe Optimization for Statistical Machine Translation	page=34 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Body	amount of fitting to the data, with larger values indicating a stronger fit. Intuitively, the	page=34 xpos=0 ypos=0 single-column full-justified aligned-line line-blank-line line-double-space line-space headchar-lower
I-Body	MIRA objective contains an error-minimizing term similar to that of the perceptron, but	page=34 xpos=0 ypos=0 single-column full-justified aligned-line headchar-capital
I-Body	also contains a regularization term with respect to the change in parameters compared	page=34 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower
I-Body	to w <sup>(t)</sup> , preferring smaller changes in parameters. When K > 1, this equation can be	page=34 xpos=0 ypos=1 single-column full-justified font-largest aligned-line headchar-lower
I-Body	formulated using Lagrange multipliers and solved using a quadratic programming	page=34 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	solver similar to that used in SVMs. When K = 1, we can simply use the following	page=34 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	update formula	page=34 xpos=0 ypos=1 single-column right-indent aligned-line shorter-tail headchar-lower above-double-space above-line-space
B-Equation	w <sup>(t</sup> + 1) = w (t) + α (t) ∆ h( f̃ , e <sup>∗</sup> , d <sup>∗</sup> , ê, d̂) (61)	page=34 xpos=1 ypos=1 single-column left-indent font-largest indented-line longer-tail line-double-space line-space itemization headchar-lower above-double-space above-line-space
I-Equation	α (t) = min <sup></sup> λ <sub>MIRA</sub> , ∆ err(ẽ, <sub>k∆</sub> e <sup>∗</sup> , <sub>h(</sub> ê) f̃ − <sub>,</sub> e w ∗ > , d ∆ ∗ , h( ê, f̃ d̂) , e k ∗ 2 , d <sup>∗</sup> , ê, d̂) 	page=34 xpos=1 ypos=2 single-column left-indent right-indent font-largest indented-line shorter-tail line-double-space line-space
I-Equation	(62)	page=34 xpos=9 ypos=2 single-column left-indent indented-line longer-tail above-blank-line above-double-space above-line-space
I-Body	The amount of update is proportional to the difference in loss between hypotheses, but	page=34 xpos=0 ypos=2 single-column full-justified hanged-line line-blank-line line-double-space line-space headchar-capital
I-Body	when the difference in the features between the oracle and incorrect hypotheses is small,	page=34 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	the features that are different will be subject to an extremely large update. The function	page=34 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	of the parameter λ <sub>MIRA</sub> is to control these over-aggressive updates. It should also be	page=34 xpos=0 ypos=3 single-column full-justified font-largest aligned-line headchar-lower
E-Body	noted that when we set α <sup>(t)</sup> = 1, MIRA reduces to the perceptron algorithm.	page=34 xpos=0 ypos=3 single-column right-indent font-largest aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	6.4 AROW	page=34 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	One of the problems often pointed out with MIRA is that it is overagressive, mainly	page=34 xpos=0 ypos=4 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	because it attempts to classify the current training example correctly according to	page=34 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	Equation (60), even when the training example is an outlier or includes noise. One way	page=34 xpos=0 ypos=4 single-column full-justified aligned-line headchar-capital
I-Body	to reduce the effect of noise is through the use of adaptive regularization of weights	page=34 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	(AROW) (Crammer, Kulesza, and Dredze 2009; Chiang 2012). AROW is based on a	page=34 xpos=0 ypos=5 single-column full-justified aligned-line year
I-Body	similar concept to MIRA, but instead of working directly on the weight vector w, it	page=34 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	defines a Gaussian distribution N (w, Σ ) over the weights. The covariance matrix Σ	page=34 xpos=0 ypos=5 single-column full-justified font-larger aligned-line headchar-lower
I-Body	is usually assumed to be diagonal, and each variance term in Σ functions as a sort	page=34 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	of learning rate for its corresponding weight, with weights of higher variance being	page=34 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
E-Body	updated more widely, and weights with lower variance being updated less widely.	page=34 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	For notational simplicity, we first define x <sup>(i)</sup> = ∆ h( f̃ <sup>(i)</sup> , e ∗ (i) , d ∗ (i) , ê (i) , d̂ <sup>(i)</sup> ), after	page=34 xpos=0 ypos=6 single-column left-indent font-largest indented-line longer-tail headchar-capital
I-Body	which we can express the AROW loss as follows:	page=34 xpos=0 ypos=6 single-column right-indent hanged-line shorter-tail headchar-lower tailchar-colon above-blank-line above-double-space above-line-space
B-Equation	` AROW ( F̃, Ẽ, C̃; w, Σ , w <sup>(t)</sup> , Σ (t) ) = KL(N (w, Σ ) ||N (w (t) , Σ (t) )) +	page=34 xpos=0 ypos=6 single-column left-indent right-indent font-largest indented-line longer-tail line-blank-line line-double-space line-space above-double-space above-line-space
I-Equation	K 	page=34 xpos=1 ypos=7 single-column left-indent right-indent indented-line shorter-tail line-double-space line-space headchar-capital
I-Equation	K 1 X λ <sub>MIRA</sub> max  0, ∆ err(ẽ (i) , e ∗ (i) , ê (i) ) − w > x (i)   + <sup>λ</sup> var <sub>2</sub> x > (i) Σ x (i) <sup></sup> (63)	page=34 xpos=1 ypos=7 single-column left-indent font-largest hanged-line longer-tail headchar-capital
I-Equation	i = 1	page=34 xpos=1 ypos=7 single-column left-indent right-indent font-smallest indented-line shorter-tail itemization headchar-lower above-double-space above-line-space
I-Body	where KL( · ) is the Kullback-Leibler divergence. The KL divergence term here plays a	page=34 xpos=0 ypos=7 single-column full-justified font-larger hanged-line longer-tail line-double-space line-space headchar-lower
I-Body	similar role to the parameter norm in MIRA, preventing large changes in the weight	page=34 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	distributions from update to update, and the first term within the summation is the	page=34 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	same loss term as MIRA. The main difference lies in the second term within the sum-	page=34 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	mation, which penalizes variance matrices that have large values for features seen in the	page=34 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	hypotheses to be updated. This has the effect of decreasing the variance, and thus the	page=34 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	learning rate, for frequently updated features, preventing large and aggressive moves	page=34 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	of weights for features that already have been seen often and thus can be considered	page=34 xpos=0 ypos=9 single-column full-justified aligned-line headchar-lower
I-Body	relatively reliable. In the case of K = 1 and where λ <sub>MIRA</sub> = λ var , the AROW update that	page=34 xpos=0 ypos=9 single-column full-justified font-largest aligned-line headchar-lower above-blank-line above-double-space above-line-space
Page	35	page=34 xpos=9 ypos=9 single-column left-indent indented-line line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Computational Linguistics Volume 42, Number 1	page=35 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Body	minimizes this loss consists of the following updates of w and Σ (Crammer, Kulesza,	page=35 xpos=0 ypos=0 single-column full-justified aligned-line line-blank-line line-double-space line-space headchar-lower tailchar-comma
I-Body	and Dredze 2009):	page=35 xpos=0 ypos=0 single-column right-indent aligned-line shorter-tail year headchar-lower tailchar-colon above-double-space above-line-space
B-Equation	w <sup>(t</sup> + 1) = w (t) + α (t + 1) Σ (t) x (i) (64)	page=35 xpos=2 ypos=1 single-column left-indent font-largest indented-line longer-tail line-double-space line-space itemization headchar-lower above-line-space
I-Equation	Σ (t + 1) = Σ (t) − β (t + 1) Σ (t) x (i) x <sup>></sup> (i) Σ (t) (65)	page=35 xpos=2 ypos=1 single-column left-indent font-largest line-space above-line-space
I-Equation	α (t + 1) = max 0, 1 − w <sup>></sup> (t) x (i)  β (t + 1) (66)	page=35 xpos=2 ypos=1 single-column left-indent font-largest aligned-line line-space above-double-space above-line-space
I-Equation	β (t + 1) = 1 (67)	page=35 xpos=2 ypos=1 single-column left-indent font-largest aligned-line line-double-space line-space
I-Equation	x <sup>></sup> (i) Σ <sub>t</sub> − <sub>1</sub> x (i) + 1 / 2 λ var <sup>.</sup>	page=35 xpos=4 ypos=2 single-column left-indent right-indent font-largest indented-line shorter-tail itemization headchar-lower above-blank-line above-double-space above-line-space
B-SubsectionHeader	6.5 Stochastic Gradient Descent	page=35 xpos=0 ypos=2 single-column right-indent hanged-line shorter-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	Stochastic gradient descent (SGD) is a gradient-based online algorithm for optimizing	page=35 xpos=0 ypos=2 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	differentiable losses, possibly with the addition of L <sub>2</sub> regularization Ω 2 (w), or L 1 regu-	page=35 xpos=0 ypos=3 single-column full-justified font-largest aligned-line headchar-lower tailchar-hiphen
I-Body	larization Ω <sub>1</sub> (w). As SGD relies on gradients, it can be thought of as an alternative to the	page=35 xpos=0 ypos=3 single-column full-justified font-largest aligned-line headchar-lower
I-Body	gradient-based batch algorithms in Section 5.2. Compared with batch algorithms, SGD	page=35 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	requires less memory and tends to converge faster, but requires more care (particularly	page=35 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	with regard to the selection of a learning rate) to ensure that it converges to a good	page=35 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
E-Body	answer.	page=35 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	In SGD, like with the perceptron algorithm and MIRA, in line 9 of Figure 8 the	page=35 xpos=0 ypos=4 single-column left-indent indented-line longer-tail headchar-capital
I-Body	parameters are updated according to the gradient ∆` ( · ) + λ∆Ω ( · ) with respect to w <sup>(t)</sup>	page=35 xpos=0 ypos=4 single-column full-justified font-largest hanged-line headchar-lower
I-Body	(Bottou 1998):	page=35 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail year tailchar-colon above-double-space above-line-space
B-Equation	η (t + 1) ← update( η (t) ) (68)	page=35 xpos=1 ypos=4 single-column left-indent font-largest indented-line longer-tail line-double-space line-space above-double-space above-line-space
I-Equation	w <sup>(t</sup> + 1) ← w (t) − η (t + 1)  ∆` ( F̃ (t) , Ẽ (t) , C̃ (t) ; w (t) ) + λ∆Ω (w (t) )   (69)	page=35 xpos=1 ypos=5 single-column left-indent font-largest line-double-space line-space itemization headchar-lower above-blank-line above-double-space above-line-space
I-Body	where η <sup>(t)</sup> > 0 is the learning rate, which is generally initialized to a value η (1) and	page=35 xpos=0 ypos=5 single-column full-justified font-largest hanged-line line-blank-line line-double-space line-space headchar-lower
I-Body	gradually reduced according to a function update( · ) as learning progresses. One stan-	page=35 xpos=0 ypos=5 single-column full-justified font-larger aligned-line headchar-lower tailchar-hiphen
I-Body	dard method for updating η <sup>(t)</sup> according to the following formula	page=35 xpos=0 ypos=5 single-column right-indent font-largest aligned-line shorter-tail headchar-lower above-double-space above-line-space
B-Equation	η (t + 1) ← η <sup>(1)</sup> (70)	page=35 xpos=4 ypos=6 single-column left-indent font-largest indented-line longer-tail line-double-space line-space
I-Equation	1 + t / T	page=35 xpos=5 ypos=6 single-column left-indent right-indent indented-line shorter-tail numbered-heading1 above-blank-line above-double-space above-line-space
I-Body	allows for a guarantee of convergence (Collins et al. 2008). In Equation (69), the pa-	page=35 xpos=0 ypos=6 single-column full-justified hanged-line longer-tail line-blank-line line-double-space line-space year headchar-lower tailchar-hiphen
I-Body	rameters are updated and we obtain w <sup>(t</sup> + 1) . Within this framework, in the perceptron	page=35 xpos=0 ypos=7 single-column full-justified font-largest aligned-line headchar-lower
I-Body	algorithm η <sup>(t)</sup> is set to a fixed value, and in MIRA the amount of update changes for	page=35 xpos=0 ypos=7 single-column full-justified font-largest aligned-line headchar-lower
I-Body	every mini-batch. SGD-style online gradient-based methods have been used in transla-	page=35 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tion for optimizing risk-based (Gao and He 2013), ranking-based (Watanabe 2012; Green	page=35 xpos=0 ypos=7 single-column full-justified aligned-line year headchar-lower
I-Body	et al. 2013), and other (Tillmann and Zhang 2006) objectives. When the regularization	page=35 xpos=0 ypos=7 single-column full-justified aligned-line year headchar-lower
I-Body	term Ω (w) is not differentiable, such as L <sub>1</sub> regularization, it is a common practice to	page=35 xpos=0 ypos=8 single-column full-justified font-largest aligned-line headchar-lower
I-Body	use forward-backward splitting (FOBOS) (Duchi and Singer 2009; Green et al. 2013) in	page=35 xpos=0 ypos=8 single-column full-justified aligned-line year headchar-lower
I-Body	which the optimization is performed in two steps:	page=35 xpos=0 ypos=8 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-colon above-double-space above-line-space
B-Equation	w <sup>(t</sup> + <sup>1</sup> 2 ) ← w (t) − η (t + 1) ∆` ( F̃ (t) , Ẽ (t) , C̃ (t) ; w (t) ) (71)	page=35 xpos=2 ypos=8 single-column left-indent font-largest indented-line longer-tail line-double-space line-space itemization headchar-lower above-double-space above-line-space
I-Equation	w <sup>(t</sup> + 1) ← arg min 1 k w − w (t + 2 1 ) k 22 + η (t + 1) λΩ (w) (72)	page=35 xpos=2 ypos=9 single-column left-indent font-largest line-double-space line-space itemization headchar-lower
I-Equation	w 2	page=35 xpos=3 ypos=9 single-column left-indent right-indent font-largest indented-line shorter-tail itemization headchar-lower above-blank-line above-double-space above-line-space
Page	36	page=35 xpos=0 ypos=9 single-column right-indent hanged-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Neubig and Watanabe Optimization for Statistical Machine Translation	page=36 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
B-Body	First, we perform updates without considering the regularization term in Equation (71).	page=36 xpos=0 ypos=0 single-column full-justified aligned-line line-blank-line line-double-space line-space headchar-capital tailchar-period
I-Body	Second, the regularization term is applied in Equation (72), which balances regulariza-	page=36 xpos=0 ypos=0 single-column full-justified aligned-line headchar-capital tailchar-hiphen
I-Body	tion and proximity to w <sup>(t</sup> + <sup>1</sup> 2 ) . As an alternative to FOBOS, it is possible to use dual	page=36 xpos=0 ypos=0 single-column full-justified font-largest aligned-line headchar-lower
I-Body	averaging, which keeps track of the average of previous gradients and optimizes for	page=36 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
E-Body	these along with the full regularization term (Xiao 2010).	page=36 xpos=0 ypos=1 single-column right-indent aligned-line shorter-tail year headchar-lower tailchar-period
B-Body	An alternative to SGD is adaptive gradient (AdaGrad) (Duchi, Hazan, and Singer	page=36 xpos=0 ypos=1 single-column left-indent indented-line longer-tail headchar-capital
I-Body	2011; Green et al. 2013) updates. The motivation behind AdaGrad is similar to that of	page=36 xpos=0 ypos=1 single-column full-justified hanged-line year
I-Body	AROW (Section 6.4), using second-order covariance statistics Σ to adjust the learning	page=36 xpos=0 ypos=1 single-column full-justified aligned-line headchar-capital
I-Body	rate of individual parameters based on their update frequency. If we define the SGD	page=36 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	gradient as x = ∆` ( F̃ <sup>(t)</sup> , Ẽ (t) , C̃ (t) ; w (t) ) + λ∆Ω (w (t) ) for notational simplicity, the update	page=36 xpos=0 ypos=2 single-column full-justified font-largest aligned-line headchar-lower
I-Body	rule for AdaGrad can be expressed as follows	page=36 xpos=0 ypos=2 single-column right-indent aligned-line shorter-tail headchar-lower above-double-space above-line-space
B-Equation	Σ − 1(t + 1) ← Σ − 1(t) + xx > (73)	page=36 xpos=3 ypos=2 single-column left-indent font-largest indented-line longer-tail line-double-space line-space above-double-space above-line-space
I-Equation	w <sup>(t</sup> + 1) ← w (t) − ηΣ 1 / 2(t) x (74)	page=36 xpos=3 ypos=3 single-column left-indent font-largest indented-line line-double-space line-space itemization headchar-lower above-blank-line above-double-space above-line-space
B-Body	Like AROW, it is common to use a diagonal covariance matrix, and each time an update	page=36 xpos=0 ypos=3 single-column full-justified hanged-line line-blank-line line-double-space line-space headchar-capital
I-Body	is performed the variance for the updated features decreases, reducing the overall	page=36 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	learning rate for more commonly updated features. It should be noted that as the update	page=36 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	of each feature is automatically controlled by the covariance matrix, there is no need to	page=36 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
E-Body	decay η as is necessary in SGD.	page=36 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SectionHeader	7. Large-Scale Optimization	page=36 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space itemization above-blank-line above-double-space above-line-space
B-Body	Up to this point, we have generally given a mathematical or algorithmic explanation	page=36 xpos=0 ypos=4 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	of the various optimization methods, and placed a smaller emphasis on factors such	page=36 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	as training efficiency. In traditional optimization settings for MT where we optimize	page=36 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	only a small number of weights for dense features on a training set of around 1,000	page=36 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	sentences, efficiency is often less of a concern. However, when trying to move to larger	page=36 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	sets of sparse features, 1,000 sentences of training data is simply not enough to robustly	page=36 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	estimate the parameters, and larger training sets become essential. When moving to	page=36 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	larger training sets, parallelization of both the decoding process and the optimization	page=36 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	process becomes essential. In this section, we outline the methods that can be used to	page=36 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	perform parallelization, greatly increasing the efficiency of training. As parallelization	page=36 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	in MT has seen wider use with respect to online learning methods, we will start with a	page=36 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
E-Body	description of online methods and touch briefly upon batch methods afterwards.	page=36 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	7.1 Large-Scale Online Optimization	page=36 xpos=0 ypos=7 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	Within the online learning framework, it is possible to improve the efficiency of learning	page=36 xpos=0 ypos=7 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	through parallelization (McDonald, Hall, and Mann 2010). An example of this is shown	page=36 xpos=0 ypos=7 single-column full-justified aligned-line year headchar-lower
I-Body	in Figure 10, where the training data h F, E i = <sup>n</sup> h f (i) , e (i) i o <sup>N</sup> <sub>i</sub> = <sub>1</sub> is split into S shards (line	page=36 xpos=0 ypos=7 single-column full-justified font-largest aligned-line headchar-lower
I-Body	2), learning is performed locally over each shard h F <sub>s</sub> , E s i , and the S sets of parameters w s	page=36 xpos=0 ypos=8 single-column full-justified font-largest aligned-line
I-Body	acquired through local learning are combined according to a function mix( · ), a process	page=36 xpos=0 ypos=8 single-column full-justified font-larger aligned-line headchar-lower
I-Body	called parameter mixing (line 6). This can be considered an instance of the MapReduce	page=36 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	(Dean and Ghemawat 2008) programming model, where each Map assigns shards to S	page=36 xpos=0 ypos=8 single-column full-justified aligned-line year
E-Body	CPUs and performs training, and Reduce combines the resulting parameters w <sub>s</sub> .	page=36 xpos=0 ypos=8 single-column right-indent font-largest aligned-line shorter-tail headchar-capital tailchar-period
B-Body	In the training algorithm of Figure 10, because parameters are learned locally on	page=36 xpos=0 ypos=9 single-column left-indent indented-line longer-tail headchar-capital
I-Body	each shard, it is not necessarily guaranteed that the parameters are optimized for the	page=36 xpos=0 ypos=9 single-column full-justified hanged-line headchar-lower above-blank-line above-double-space above-line-space
Page	37	page=36 xpos=9 ypos=9 single-column left-indent indented-line line-blank-line line-double-space line-space numeric-only page-bottom
Figure	__Figure 10__	page=37 xpos=-1 ypos=-1 Figure-column left-over right-indent box page-top figure-area column-bottom
B-Header	Volume 42, Number 1	page=37 xpos=7 ypos=0 single-column left-indent font-smallest column-top headchar-capital above-blank-line above-double-space above-line-space
B-Figure	h F <sub>S</sub> , E S i}	page=37 xpos=5 ypos=0 single-column left-indent right-indent font-largest hanged-line shorter-tail line-blank-line line-double-space line-space itemization headchar-lower
I-Figure	. Run shards in parallel	page=37 xpos=5 ypos=0 single-column left-indent right-indent longer-tail above-blank-line above-double-space above-line-space
I-Figure	. Local learning	page=37 xpos=6 ypos=1 single-column left-indent right-indent indented-line line-blank-line line-double-space line-space above-blank-line above-double-space above-line-space
I-Figure	. Parameter mixing	page=37 xpos=5 ypos=1 single-column left-indent right-indent hanged-line line-blank-line line-double-space line-space above-blank-line above-double-space above-line-space
B-Caption	Figure 10	page=37 xpos=0 ypos=2 single-column right-indent font-smallest hanged-line shorter-tail line-blank-line line-double-space line-space string-figure headchar-capital
E-Caption	Parallel learning.	page=37 xpos=0 ypos=2 single-column right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-period above-blank-line above-double-space above-line-space
I-Body	data as a whole. In addition, it is also known that some divisions of the data can lead to	page=37 xpos=0 ypos=2 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-lower
I-Body	contradictions between the parameters (McDonald, Hall, and Mann 2010). Because of	page=37 xpos=0 ypos=3 single-column full-justified aligned-line year headchar-lower
I-Body	this, when performing distributed online learning, it is common to perform parameter	page=37 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	mixing several times throughout the training process, which allows the separate shards	page=37 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	to share information and prevents contradiction between the learned parameters. Based	page=37 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	on the timing of the update, these varieties of mixing are called synchronous update	page=37 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
E-Body	and asynchronous update.	page=37 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-Body	7.1.1 Synchronous Update. In the online learning algorithm with synchronous update	page=37 xpos=0 ypos=4 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space numbered-heading3
I-Body	shown in Figure 11, learning is performed independently over each shard h F <sub>s</sub> , E s i (1 ≤	page=37 xpos=0 ypos=4 single-column full-justified font-largest aligned-line headchar-lower
I-Body	s ≤ S). The difference between this and Figure 10 lies in the fact that learning is	page=37 xpos=0 ypos=4 single-column full-justified font-larger aligned-line itemization headchar-lower
I-Body	performed T <sup>0</sup> times, with each iteration initialized with the parameters w (t) from the	page=37 xpos=0 ypos=4 single-column full-justified font-largest aligned-line headchar-lower
E-Body	previous iteration (line 7).	page=37 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	After the local learning finishes, the parameters are mixed in line 8 (Watanabe et al.	page=37 xpos=0 ypos=5 single-column left-indent indented-line longer-tail headchar-capital tailchar-period
I-Body	2007; McDonald, Hall, and Mann 2010; Watanabe 2012). In the mixing function it is most	page=37 xpos=0 ypos=5 single-column full-justified hanged-line year
I-Body	common to use the average of the parameters	page=37 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail headchar-lower column-bottom above-double-space above-line-space
Figure	__Figure 11__	page=37 xpos=0 ypos=5 Figure-column left-indent right-indent box column-top line-double-space line-space figure-area column-bottom
B-Figure	µ s w̄ <sup>(t</sup> s + 1) (75)	page=37 xpos=5 ypos=5 single-column left-indent font-largest column-top above-line-space
I-Figure	1	page=37 xpos=5 ypos=6 single-column left-indent right-indent font-smallest hanged-line shorter-tail line-space numeric-only above-blank-line above-double-space above-line-space
I-Figure	( h F, E i )	page=37 xpos=5 ypos=6 single-column left-indent right-indent font-larger indented-line longer-tail line-blank-line line-double-space line-space
I-Figure	. . . , h F <sub>S</sub> , E S i}	page=37 xpos=5 ypos=6 single-column left-indent right-indent font-largest hanged-line longer-tail
I-Figure	. Initialize parameters	page=37 xpos=5 ypos=7 single-column left-indent right-indent indented-line longer-tail
I-Figure	. Run shards in parallel	page=37 xpos=5 ypos=7 single-column left-indent right-indent hanged-line above-blank-line above-double-space above-line-space
I-Figure	parameters to each shard	page=37 xpos=5 ypos=7 single-column left-indent right-indent hanged-line line-blank-line line-double-space line-space headchar-lower
I-Figure	s i , w̄ <sup>(t)</sup> s , T 0 )	page=37 xpos=5 ypos=7 single-column left-indent right-indent font-largest hanged-line shorter-tail itemization headchar-lower
I-Figure	Local learning on each shard	page=37 xpos=5 ypos=7 single-column left-indent right-indent aligned-line longer-tail headchar-capital
I-Figure	<sup>+</sup> <sup>1)</sup> } <sup></sup> . Mix parameters	page=37 xpos=5 ypos=8 single-column left-indent right-indent font-largest headchar-super above-blank-line above-double-space above-line-space
B-Caption	Figure 11	page=37 xpos=0 ypos=9 single-column right-indent font-smallest hanged-line shorter-tail line-blank-line line-double-space line-space string-figure headchar-capital
E-Caption	Online learning with synchronous update.	page=37 xpos=0 ypos=9 single-column right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-period above-blank-line above-double-space above-line-space
Page	38	page=37 xpos=0 ypos=9 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Neubig and Watanabe Optimization for Statistical Machine Translation	page=38 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Body	where <sup>P</sup> <sup>S</sup> <sub>s</sub> = <sub>1</sub> µ s = 1. As µ s , it is possible to use a uniform distribution, or a weight	page=38 xpos=0 ypos=0 single-column full-justified font-largest aligned-line line-blank-line line-double-space line-space headchar-lower
I-Body	proportional to the number of online updates performed at each shard (McDonald,	page=38 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	Hall, and Mann 2010; Simianer, Riezler, and Dyer 2012). It should be noted that this	page=38 xpos=0 ypos=0 single-column full-justified aligned-line year headchar-capital
I-Body	algorithm can be considered a variety of the MapReduce framework, allowing for	page=38 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	relatively straightforward implementation using parallel processing infrastructure such	page=38 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
E-Body	as Hadoop (Eidelman 2012).	page=38 xpos=0 ypos=1 single-column right-indent aligned-line shorter-tail year headchar-lower tailchar-period
B-Body	Simianer, Riezler, and Dyer (2012) propose another method for mixing parameters	page=38 xpos=0 ypos=1 single-column left-indent indented-line longer-tail year headchar-capital
I-Body	that, instead of averaging at each iteration, chooses to preserve only the parameters	page=38 xpos=0 ypos=1 single-column full-justified hanged-line headchar-lower
I-Body	that have been learned over all shards, and sets all the remaining parameters to zero,	page=38 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	allowing for a simple sort of feature selection. In particular, we define a S × M matrix	page=38 xpos=0 ypos=2 single-column full-justified font-larger aligned-line headchar-lower
I-Body	that combines the parameters w̄ <sub>s</sub> <sup>(t</sup> + 1) at each shard as w̄ (t + 1) = <sup>h</sup> w̄ (t <sub>1</sub> + 1) | . . . | w̄ S (t + 1) i <sup>></sup> ,	page=38 xpos=0 ypos=2 single-column full-justified font-largest aligned-line headchar-lower tailchar-comma
I-Body	takes the L <sub>2</sub> norm of each matrix column, and averages the columns with high norm	page=38 xpos=0 ypos=2 single-column full-justified font-largest aligned-line headchar-lower
E-Body	values while setting the rest to zero.	page=38 xpos=0 ypos=2 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-Body	7.1.2 Asynchronous Update. While parallel learning with synchronous update is guar-	page=38 xpos=0 ypos=3 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space numbered-heading3 tailchar-hiphen
I-Body	anteed to converge (McDonald, Hall, and Mann 2010), parameter mixing only occurs	page=38 xpos=0 ypos=3 single-column full-justified aligned-line year headchar-lower
I-Body	after all the data has been processed, leading to inefficiency over large data sets. To fix	page=38 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	this problem, asynchronous update sends information about parameter updates to each	page=38 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	shard asynchronously, allowing the parameters to be updated more frequently, resulting	page=38 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
E-Body	in faster learning (Chiang, Marton, and Resnik 2008; Chiang, Knight, and Wang 2009).	page=38 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail year headchar-lower tailchar-period
B-Body	The algorithm for learning with asynchronous update is shown in Figure 12. With	page=38 xpos=0 ypos=4 single-column left-indent indented-line longer-tail headchar-capital
I-Body	the data h F <sub>s</sub> , E s i (1 ≤ s ≤ S) split into S pieces, each shard performs T iterations of	page=38 xpos=0 ypos=4 single-column full-justified font-largest hanged-line headchar-lower
I-Body	training by sampling a mini-batch (line 7), translating each sentence (line 10), and	page=38 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
E-Body	performing optimization on the mini-batch level (line 12).	page=38 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period column-bottom above-double-space above-line-space
Figure	__Figure 12__	page=38 xpos=0 ypos=5 Figure-column right-indent box column-top line-double-space line-space figure-area column-bottom
B-Figure	( h F, E i )	page=38 xpos=5 ypos=5 single-column left-indent right-indent font-larger column-top
I-Figure	, . . . , h F <sub>S</sub> , E S i}	page=38 xpos=5 ypos=5 single-column left-indent right-indent font-largest hanged-line longer-tail
I-Figure	. Initialize parameters	page=38 xpos=7 ypos=6 single-column left-indent right-indent indented-line longer-tail
I-Figure	. Run s shards in parallel	page=38 xpos=6 ypos=6 single-column left-indent right-indent hanged-line above-blank-line above-double-space above-line-space
I-Figure	. Copy parameters to each shard	page=38 xpos=6 ypos=6 single-column left-indent right-indent hanged-line line-blank-line line-double-space line-space above-line-space
I-Figure	. Sample ( |h F̃ <sub>s</sub> <sup>(t)</sup> , Ẽ (t) s i| = K)	page=38 xpos=6 ypos=6 single-column left-indent right-indent font-largest indented-line line-space above-line-space
I-Figure	. k-best candidates	page=38 xpos=7 ypos=7 single-column left-indent right-indent indented-line line-space above-blank-line above-double-space above-line-space
I-Figure	. Decode with w̄ <sup>(t)</sup> <sub>s</sub>	page=38 xpos=7 ypos=7 single-column left-indent right-indent font-largest line-blank-line line-double-space line-space above-double-space above-line-space
I-Figure	(t) (t)	page=38 xpos=5 ypos=7 single-column left-indent right-indent font-smallest hanged-line shorter-tail line-double-space line-space itemization
I-Figure	s , C̃ s ; w̄ <sub>s</sub> ) + λΩ ( w̄ s ) . Optimize	page=38 xpos=5 ypos=7 single-column left-indent right-indent font-largest aligned-line longer-tail itemization headchar-lower
I-Figure	. , w̄ <sub>S</sub> <sup>(t</sup> + 1) } <sup></sup> . Mix parameters	page=38 xpos=5 ypos=8 single-column left-indent right-indent font-largest hanged-line above-blank-line above-double-space above-line-space
B-Caption	Figure 12	page=38 xpos=0 ypos=9 single-column right-indent font-smallest hanged-line shorter-tail line-blank-line line-double-space line-space string-figure headchar-capital
E-Caption	Online learning with asynchronous update.	page=38 xpos=0 ypos=9 single-column right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-period above-blank-line above-double-space above-line-space
Page	39	page=38 xpos=9 ypos=9 single-column left-indent indented-line longer-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Computational Linguistics Volume 42, Number 1	page=39 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
B-Body	Compared with the synchronous algorithm in Figure 11, the parameter mixing	page=39 xpos=0 ypos=0 single-column left-indent indented-line line-blank-line line-double-space line-space headchar-capital
I-Body	operator mix <sub>async</sub> ( · ) in line 13 sends the result of the mini-batch level update	page=39 xpos=0 ypos=0 single-column right-indent font-largest hanged-line shorter-tail headchar-lower above-double-space above-line-space
B-Equation	∆ w̄ <sup>(t</sup> <sub>s</sub> + 1) = w̄ s (t + 1) − w̄ s (t) (76)	page=39 xpos=3 ypos=1 single-column left-indent font-largest indented-line longer-tail line-double-space line-space above-double-space above-line-space
I-Body	to each shard s <sup>0</sup> (s 0 = 6 s,1 ≤ s 0 ≤ S). It should be noted that because the number of di-	page=39 xpos=0 ypos=1 single-column full-justified font-largest hanged-line line-double-space line-space headchar-lower tailchar-hiphen
I-Body	mensions M in the parameter vector is extremely large, and each mini-batch will only	page=39 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	update a small fraction of these parameters, by only sending the parameters that have	page=39 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	actually changed in each mini-batch update we can greatly increase the efficiency of the	page=39 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	training. In operator copy <sub>async</sub> ( · ) of line 6, the learner receives the update from all the	page=39 xpos=0 ypos=2 single-column full-justified font-largest aligned-line headchar-lower
I-Body	other shards and mixes them together to acquire the full update vector	page=39 xpos=0 ypos=2 single-column right-indent aligned-line shorter-tail headchar-lower above-blank-line above-double-space above-line-space
B-Equation	S	page=39 xpos=5 ypos=2 single-column left-indent right-indent font-smallest indented-line shorter-tail line-blank-line line-double-space line-space headchar-capital
I-Equation	w̄ <sub>s</sub> <sup>(t)</sup> ← X ∆ w̄ (t) <sub>s</sub> 0 (77)	page=39 xpos=4 ypos=2 single-column left-indent font-largest hanged-line longer-tail headchar-lower
I-Equation	s <sup>0</sup> = 1	page=39 xpos=4 ypos=3 single-column left-indent right-indent font-smallest indented-line shorter-tail itemization headchar-lower above-blank-line above-double-space above-line-space
B-Body	It should be noted that at mixing time, there is no need to wait for the update vectors	page=39 xpos=0 ypos=3 single-column full-justified hanged-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	from all of the other shards; update can be performed with only the update vectors that	page=39 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	have been received at the time. Because of this, each shard will not necessarily be using	page=39 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	parameters that reflect all the most recent updates performed on other shards. However,	page=39 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	as updates are performed much more frequently than in synchronous update, it is easier	page=39 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	to avoid local optima, and learning tends to converge faster. It can also be shown that	page=39 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	(under some conditions) the amount of accuracy lost by this delay in update is bounded	page=39 xpos=0 ypos=4 single-column full-justified aligned-line
E-Body	(Zinkevich, Langford, and Smola 2009; Recht et al. 2011).	page=39 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail year tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	7.2 Large-Scale Batch Optimization	page=39 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	Compared with online learning, within the batch optimization framework, paralleliza-	page=39 xpos=0 ypos=5 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital tailchar-hiphen
I-Body	tion is usually straightforward. Often, decoding takes the majority of time required for	page=39 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	the optimization process, and because the parameters will be identical for each sentence	page=39 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	in the decoding run (Figure 3, line 6), decoding can be parallelized trivially. The process	page=39 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	of parallelizing optimization itself depends slightly on the optimization algorithm, but	page=39 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
E-Body	is generally possible to achieve in a number of ways.	page=39 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	The first, and simplest, method for parallelization is the parallelization of opti-	page=39 xpos=0 ypos=6 single-column left-indent indented-line longer-tail headchar-capital tailchar-hiphen
I-Body	mization runs. The most obvious example of this is MERT, where random restarts are	page=39 xpos=0 ypos=6 single-column full-justified hanged-line headchar-lower
I-Body	required. Each of the random restarts is completely independent, so it is possible to run	page=39 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	these on different nodes, and finally check which run achieved the best accuracy and	page=39 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
E-Body	use that result.	page=39 xpos=0 ypos=7 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	Another more fine-grained method for parallelization, again most useful for MERT,	page=39 xpos=0 ypos=7 single-column left-indent indented-line longer-tail headchar-capital tailchar-comma
I-Body	is the parallelization of search directions. In the loop starting at Figure 4, line 6, MERT	page=39 xpos=0 ypos=7 single-column full-justified hanged-line headchar-lower
I-Body	performs line search in several different directions, each one being independent of	page=39 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
I-Body	the others. Each of these line searches can be performed in parallel, and the direction	page=39 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
E-Body	allowing for the greatest gain in accuracy is chosen when all threads have completed.	page=39 xpos=0 ypos=8 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	A method that is applicable to a much broader array of optimization methods	page=39 xpos=0 ypos=8 single-column left-indent indented-line longer-tail headchar-capital
I-Body	is the parallelization of calculation of sufficient statistics. In this approach, like in	page=39 xpos=0 ypos=8 single-column full-justified hanged-line headchar-lower
I-Body	Section 7.1.1, we first split the data into shards h F <sub>s</sub> , E s i (1 ≤ s ≤ S). Then, over these	page=39 xpos=0 ypos=8 single-column full-justified font-largest aligned-line headchar-capital
I-Body	shards we calculate the sufficient statistics necessary to perform a parameter update. For	page=39 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	example, in MERT these sufficient statistics would consist of the envelope for each of	page=39 xpos=0 ypos=9 single-column full-justified aligned-line headchar-lower
I-Body	the potential search directions. In gradient based methods, the sufficient statistics would	page=39 xpos=0 ypos=9 single-column full-justified aligned-line headchar-lower above-blank-line above-double-space above-line-space
Page	40	page=39 xpos=0 ypos=9 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Neubig and Watanabe Optimization for Statistical Machine Translation	page=40 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Body	consist of the gradient calculated with respect to only the data on the shard. Finally,	page=40 xpos=0 ypos=0 single-column full-justified aligned-line line-blank-line line-double-space line-space headchar-lower tailchar-comma
I-Body	when all threads have finished calculating these statistics, a master thread combines the	page=40 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower
E-Body	statistics from each shard, either by merging the envelopes, or by adding the gradients.	page=40 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SectionHeader	8. Other Topics in MT Optimization	page=40 xpos=0 ypos=1 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space itemization above-blank-line above-double-space above-line-space
B-Body	In this section we cover several additional topics in optimization for MT, including non-	page=40 xpos=0 ypos=1 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital tailchar-hiphen
I-Body	linear models (Section 8.1), optimization for a particular domain or test set (Section 8.2),	page=40 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	and the interaction between evaluation measures (Section 8.3) or search (Section 8.4)	page=40 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
E-Body	and optimization.	page=40 xpos=0 ypos=2 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	8.1 Non-Linear Models	page=40 xpos=0 ypos=2 single-column right-indent aligned-line longer-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	Note that up until this point, all models that we have considered calculate the scores	page=40 xpos=0 ypos=2 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	for translation hypotheses according to a linear model, where the score is calculated	page=40 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	according to the dot product of the features and weights shown in Equation (1). How-	page=40 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ever, linear models are obviously limited in their expressive power, and a number of	page=40 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	works have attempted to move beyond linear combinations of features to nonlinear	page=40 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
E-Body	combinations.	page=40 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	In general, most nonlinear models for machine learning can be applied to MT as	page=40 xpos=0 ypos=3 single-column left-indent indented-line longer-tail headchar-capital
I-Body	well, with one major caveat. Specifically, the efficiency of the decoding process largely	page=40 xpos=0 ypos=4 single-column full-justified hanged-line headchar-lower
I-Body	relies on the feature locality assumption mentioned in Section 2.3. Unfortunately, the lo-	page=40 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	cality assumption breaks down when moving beyond a simple linear scoring function,	page=40 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	and overcoming this problem is the main obstacle to applying nonlinear models to MT	page=40 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	(or structured learning in general). A number of countermeasures to this problem exist:	page=40 xpos=0 ypos=4 single-column full-justified aligned-line tailchar-colon above-double-space above-line-space
B-Listitem	Reranking: The most simple and commonly used method for incorporating non-	page=40 xpos=0 ypos=5 single-column full-justified aligned-line line-double-space line-space headchar-capital tailchar-hiphen
I-Listitem	linearity, or other highly nonlocal features that cannot be easily incorporated in	page=40 xpos=0 ypos=5 single-column left-indent indented-line headchar-lower
I-Listitem	search, is through the use of reranking (Shen, Sarkar, and Och 2004). In this case,	page=40 xpos=0 ypos=5 single-column left-indent aligned-line year headchar-lower tailchar-comma
I-Listitem	a system optimized using a standard linear model is used to create a k-best list of	page=40 xpos=0 ypos=5 single-column left-indent aligned-line itemization headchar-lower
I-Listitem	outputs, and this k-best list is then reranked using the nonlinear model (Nguyen,	page=40 xpos=0 ypos=5 single-column left-indent aligned-line headchar-lower tailchar-comma
I-Listitem	Mahajan, and He 2007; Duh and Kirchhoff 2008). Because we are now only dealing	page=40 xpos=0 ypos=6 single-column left-indent aligned-line year headchar-capital
I-Listitem	with fully expanded hypotheses, scoring becomes trivial, but reranking also has	page=40 xpos=0 ypos=6 single-column left-indent aligned-line headchar-lower
I-Listitem	the major downsides of potentially missing useful hypotheses not included in the	page=40 xpos=0 ypos=6 single-column left-indent aligned-line headchar-lower
I-Listitem	k-best list, <sup>9</sup> and requiring time directly proportional to the size of the k-best list.	page=40 xpos=0 ypos=6 single-column left-indent right-indent font-largest aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-Listitem	Local Nonlinearity: Another possibility is to first use a nonlinear function to calculate	page=40 xpos=0 ypos=6 single-column full-justified hanged-line longer-tail line-double-space line-space headchar-capital
I-Listitem	local features, which are then used as part of the standard linear model (Liu	page=40 xpos=0 ypos=6 single-column left-indent indented-line headchar-lower
I-Listitem	et al. 2013). Alternatively, it is possible to treat feature-value pairs as new binary	page=40 xpos=0 ypos=7 single-column left-indent aligned-line year headchar-lower
I-Listitem	features (Clark, Dyer, and Lavie 2014). In this case, all effects of nonlinearity are	page=40 xpos=0 ypos=7 single-column left-indent aligned-line year headchar-lower
I-Listitem	resolved before the search actually begins, allowing for the use of standard and	page=40 xpos=0 ypos=7 single-column left-indent aligned-line headchar-lower
I-Listitem	efficient search algorithms. On the other hand, it is not possible to incorporate	page=40 xpos=0 ypos=7 single-column left-indent aligned-line headchar-lower
I-Listitem	non-local features into the nonlinear model.	page=40 xpos=0 ypos=7 single-column left-indent right-indent aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-Listitem	Improved Search Techniques: Although there is no general-purpose solution to incor-	page=40 xpos=0 ypos=8 single-column full-justified hanged-line longer-tail line-double-space line-space headchar-capital tailchar-hiphen
I-Listitem	porating nonlinear models into search, for some particular models it is possible	page=40 xpos=0 ypos=8 single-column left-indent indented-line headchar-lower
I-Listitem	to perform search in a way that allows for incorporation of nonlinearities. For ex-	page=40 xpos=0 ypos=8 single-column left-indent aligned-line headchar-lower tailchar-hiphen
I-Listitem	ample, ensemble decoding has been used with stacking-based models (Razmara	page=40 xpos=0 ypos=8 single-column left-indent aligned-line headchar-lower above-blank-line above-double-space above-line-space
B-Footnote	9 This problem can be ameliorated somewhat by ensuring that there is sufficient diversity in the n-best list	page=40 xpos=0 ypos=9 single-column centered left-indent right-indent font-smallest hanged-line line-blank-line line-double-space line-space numbered-heading1
I-Footnote	(Gimpel et al. 2013).	page=40 xpos=0 ypos=9 single-column left-indent right-indent font-smallest indented-line shorter-tail year tailchar-period above-blank-line above-double-space above-line-space
Page	41	page=40 xpos=9 ypos=9 single-column left-indent indented-line longer-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Computational Linguistics Volume 42, Number 1	page=41 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Listitem	and Sarkar 2013), and it has been shown that the search space can be simplified to	page=41 xpos=0 ypos=0 single-column left-indent indented-line line-blank-line line-double-space line-space year headchar-lower
I-Listitem	the extent that kernel functions can be calculated efficiently (Wang, Shawe-Taylor,	page=41 xpos=0 ypos=0 single-column left-indent aligned-line headchar-lower tailchar-comma
I-Listitem	and Szedmak 2007).	page=41 xpos=0 ypos=0 single-column left-indent right-indent aligned-line shorter-tail year headchar-lower tailchar-period above-double-space above-line-space
B-Body	Once the problems of search have been solved, a number of actual learning tech-	page=41 xpos=0 ypos=1 single-column left-indent hanged-line longer-tail line-double-space line-space headchar-capital tailchar-hiphen
I-Body	niques can be used to model nonlinear scoring functions. One of the most popular	page=41 xpos=0 ypos=1 single-column full-justified hanged-line headchar-lower
I-Body	examples of nonlinear functions are those utilizing kernels, and methods applied to MT	page=41 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	include kernel-like functions over the feature space such as the Parzen window, binning,	page=41 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	and Gaussian kernels (Nguyen, Mahajan, and He 2007), or the n-spectrum string kernel	page=41 xpos=0 ypos=1 single-column full-justified aligned-line year headchar-lower
I-Body	for finding associations between the source and target strings (Wang, Shawe-Taylor,	page=41 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	and Szedmak 2007). Neural networks are another popular method for modeling non-	page=41 xpos=0 ypos=2 single-column full-justified aligned-line year headchar-lower tailchar-hiphen
I-Body	linearities, and it has been shown that neural networks can effectively be used to calcu-	page=41 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	late new local features for MT (Liu et al. 2013). Methods such as boosting or stacking,	page=41 xpos=0 ypos=2 single-column full-justified aligned-line year headchar-lower tailchar-comma
I-Body	which combine together multiple parameterizations of the translation model, have been	page=41 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
I-Body	incorporated through reranking (Duh and Kirchhoff 2008; Lagarda and Casacuberta	page=41 xpos=0 ypos=3 single-column full-justified aligned-line year headchar-lower
I-Body	2008; Duan et al. 2009; Sokolov, Wisniewski, and Yvon 2012b), or ensemble decoding	page=41 xpos=0 ypos=3 single-column full-justified aligned-line year
I-Body	(Razmara and Sarkar 2013). Regression decision trees have also been introduced as a	page=41 xpos=0 ypos=3 single-column full-justified aligned-line year
I-Body	method for inducing nonlinear functions, incorporated through history-based search	page=41 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	algorithms (Turian, Wellington, and Melamed 2006), or by using the trees to induce	page=41 xpos=0 ypos=3 single-column full-justified aligned-line year headchar-lower
E-Body	features local to the search state (Toutanova and Ahn 2013).	page=41 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail year headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	8.2 Domain-Dependent Optimization	page=41 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	One widely acknowledged feature of machine learning problems in general is that the	page=41 xpos=0 ypos=4 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	parameters are sensitive to the domain of the data, and by optimizing the parameters	page=41 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	with data from the target domain it is possible to achieve gains in accuracy. In machine	page=41 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	translation, this is also very true, although much of the work on domain adaptation has	page=41 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	focused on adapting the model learning process prior to explicit optimization towards	page=41 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	an evaluation measure (Koehn and Schroeder 2007). However, there are a few works on	page=41 xpos=0 ypos=5 single-column full-justified aligned-line year headchar-lower
E-Body	optimization-based domain adaptation in MT, as we will summarize subsequently.	page=41 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	One relatively simple way of performing domain adaptation is by selecting a subset	page=41 xpos=0 ypos=5 single-column left-indent indented-line longer-tail headchar-capital
I-Body	of the training data that is similar to the data that we want to translate (Li et al. 2010).	page=41 xpos=0 ypos=6 single-column full-justified hanged-line year headchar-lower tailchar-period
I-Body	This can be done by selecting sentences that are similar to our test corpus, or even	page=41 xpos=0 ypos=6 single-column full-justified aligned-line headchar-capital
I-Body	selecting adaptation data for each individual test sentence (Liu et al. 2012). If no parallel	page=41 xpos=0 ypos=6 single-column full-justified aligned-line year headchar-lower
I-Body	data exist in the target domain, it has also been shown that first automatically translating	page=41 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	data from the source to the target language or vice versa, then using this data for	page=41 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	optimization and model training is also helpful (Ueffing, Haffari, and Sarkar 2007; Li	page=41 xpos=0 ypos=6 single-column full-justified aligned-line year headchar-lower
I-Body	et al. 2011; Zhao et al. 2011) In addition, in a computer-assisted translation scenario, it is	page=41 xpos=0 ypos=7 single-column full-justified aligned-line year headchar-lower
I-Body	possible to reflect post-edited translations back into the optimization process as new in-	page=41 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	domain training data (Mathur, Mauro, and Federico 2013; Denkowski, Dyer, and Lavie	page=41 xpos=0 ypos=7 single-column full-justified aligned-line year headchar-lower
E-Body	2014).	page=41 xpos=0 ypos=7 single-column right-indent aligned-line shorter-tail year tailchar-period
B-Body	Once adaptation data have been chosen, it is necessary to decide how to use the	page=41 xpos=0 ypos=7 single-column left-indent indented-line longer-tail headchar-capital
I-Body	data. The most straightforward way is to simply use these in-domain data in opti-	page=41 xpos=0 ypos=8 single-column full-justified hanged-line headchar-lower tailchar-hiphen
I-Body	mization, but if the data set is small it is preferable to combine both in- and out-of-	page=41 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	domain data to achieve more robust parameter estimates. This is essentially equivalent	page=41 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	to the standard domain-adaptation problem in machine learning, and in the context	page=41 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	of MT there have been methods proposed to perform Bayesian adaptation of proba-	page=41 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	bilistic models (Sanchis-Trilles and Casacuberta 2010), and online update using ultra-	page=41 xpos=0 ypos=8 single-column full-justified aligned-line year headchar-lower tailchar-hiphen
I-Body	conservative algorithms (Liu et al. 2012). This can be extended to cover multiple target	page=41 xpos=0 ypos=9 single-column full-justified aligned-line year headchar-lower
E-Body	domains using multi-task learning (Cui et al. 2013).	page=41 xpos=0 ypos=9 single-column right-indent aligned-line shorter-tail year headchar-lower tailchar-period above-blank-line above-double-space above-line-space
Page	42	page=41 xpos=0 ypos=9 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Neubig and Watanabe Optimization for Statistical Machine Translation	page=42 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
B-Body	Finally, it has been noted that when optimizing a few dense parameters, it is useful	page=42 xpos=0 ypos=0 single-column left-indent indented-line line-blank-line line-double-space line-space headchar-capital
I-Body	to make the distinction between in-domain translation (when the model training data	page=42 xpos=0 ypos=0 single-column full-justified hanged-line headchar-lower
I-Body	matches the test domain) and cross-domain translation (when the model training data	page=42 xpos=0 ypos=0 single-column full-justified aligned-line headchar-lower
I-Body	mismatches the test domain). In cross-domain translation, fewer long rules will be	page=42 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	used, and translation probabilities will be less reliable, and the parameters must change	page=42 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	accordingly to account for this (Pecina, Toral, and van Genabith 2012). It has also been	page=42 xpos=0 ypos=1 single-column full-justified aligned-line year headchar-lower
I-Body	shown that building TMs for several domains and tuning the parameters to maximize	page=42 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	translation accuracy can improve MT accuracy on the target domain (Haddow 2013).	page=42 xpos=0 ypos=1 single-column full-justified aligned-line year headchar-lower tailchar-period
I-Body	Another option for making the distinction between in-domain and out-of-domain data	page=42 xpos=0 ypos=1 single-column full-justified aligned-line headchar-capital
I-Body	is by firing different features for in-domain and out-of-domain training data, allow-	page=42 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ing for the learning of different weights for different domains (Clark, Lavie, and Dyer	page=42 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower
E-Body	2012).	page=42 xpos=0 ypos=2 single-column right-indent aligned-line shorter-tail year tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	8.3 Evaluation Measures and Optimization	page=42 xpos=0 ypos=2 single-column right-indent aligned-line longer-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	In the entirety of this article, we have assumed that optimization for MT aims to	page=42 xpos=0 ypos=3 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	reduce MT error defined using an evaluation measure, generally BLEU. However, as	page=42 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	mentioned in Section 2.5, evaluation of MT is an active research field, and there are	page=42 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	many alternatives in addition to BLEU. Thus, it is of interest whether changing the	page=42 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	measure used in optimization can affect the overall quality of the translations achieved,	page=42 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower tailchar-comma
E-Body	as measured by human evaluators.	page=42 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	There have been a few comprehensive studies on the effect of the metric used in	page=42 xpos=0 ypos=4 single-column left-indent indented-line longer-tail headchar-capital
I-Body	optimization on human assessments of the generated translations (Cer, Manning, and	page=42 xpos=0 ypos=4 single-column full-justified hanged-line headchar-lower
I-Body	Jurafsky 2010; Callison-Burch et al. 2011). These studies showed the rather surprising	page=42 xpos=0 ypos=4 single-column full-justified aligned-line year headchar-capital
I-Body	result that despite the fact that other evaluation measures had proven superior to BLEU	page=42 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	with regards to post facto correlation with human evaluation, a BLEU-optimized system	page=42 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	proved superior to systems tuned using other metrics. Since this result, however, there	page=42 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	have been other reports stating that systems optimized using other metrics such as	page=42 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
I-Body	TESLA (Liu, Dahlmeier, and Ng 2011) and MEANT (Lo et al. 2013) achieve superior	page=42 xpos=0 ypos=5 single-column full-justified aligned-line year headchar-capital
E-Body	results to BLEU-optimized systems.	page=42 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	There have also been attempts to directly optimize not automatic, but human	page=42 xpos=0 ypos=5 single-column left-indent indented-line longer-tail headchar-capital
I-Body	evaluation measures of translation quality (Zaidan and Callison-Burch 2009). However,	page=42 xpos=0 ypos=6 single-column full-justified hanged-line year headchar-lower tailchar-comma
I-Body	the cost of performing this sort of human-in-the-loop optimization is prohibitive, so	page=42 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	Zaidan and Callison-Burch (2009) propose a method that re-uses partial hypotheses	page=42 xpos=0 ypos=6 single-column full-justified aligned-line year headchar-capital
I-Body	in evaluation. Saluja, Lane, and Zhang (2012) also propose a method for incorporat-	page=42 xpos=0 ypos=6 single-column full-justified aligned-line year headchar-lower tailchar-hiphen
I-Body	ing binary good/bad input into optimization, with the motivation that this sort of	page=42 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	feedback is easier for human annotators to provide than generating new reference	page=42 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
E-Body	sentences.	page=42 xpos=0 ypos=7 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	Finally, there is also some work on optimizing multiple evaluation metrics at one	page=42 xpos=0 ypos=7 single-column left-indent indented-line longer-tail headchar-capital
I-Body	time. The easiest way to do so is to simply use the linear interpolation of two or more	page=42 xpos=0 ypos=7 single-column full-justified hanged-line headchar-lower
I-Body	metrics as the error function (Dyer et al. 2009; He and Way 2009; Servan and Schwenk	page=42 xpos=0 ypos=7 single-column full-justified aligned-line year headchar-lower
I-Body	2011):	page=42 xpos=0 ypos=7 single-column right-indent aligned-line shorter-tail year tailchar-colon above-blank-line above-double-space above-line-space
B-Equation	L	page=42 xpos=4 ypos=8 single-column centered left-indent right-indent font-smallest indented-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Equation	error(E, Ê) = <sup>X</sup> ρ <sub>i</sub> error i (E, Ê) (78)	page=42 xpos=3 ypos=8 single-column left-indent font-largest hanged-line longer-tail headchar-lower above-line-space
I-Equation	i = 1	page=42 xpos=4 ypos=8 single-column centered left-indent right-indent font-smallest indented-line shorter-tail line-space itemization headchar-lower above-double-space above-line-space
I-Body	where L is the number of error functions, and ρ <sub>i</sub> is a manually set interpolation	page=42 xpos=0 ypos=8 single-column full-justified font-largest hanged-line longer-tail line-double-space line-space headchar-lower
I-Body	coefficient for its respective error function. There are also more sophisticated meth-	page=42 xpos=0 ypos=9 single-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ods based on the idea of optimizing towards Pareto-optimal hypotheses (Duh et al.	page=42 xpos=0 ypos=9 single-column full-justified aligned-line headchar-lower tailchar-period above-blank-line above-double-space above-line-space
Page	43	page=42 xpos=9 ypos=9 single-column left-indent indented-line line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Computational Linguistics Volume 42, Number 1	page=43 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Body	2012), which achieve errors lower than all other hypotheses on at least one evaluation	page=43 xpos=0 ypos=0 single-column full-justified aligned-line line-blank-line line-double-space line-space year
I-Body	measure,	page=43 xpos=0 ypos=0 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-comma above-double-space above-line-space
B-Equation	pareto(E, E ) = { Ê ∈ E : ∀ <sub>E</sub> 0 ∈ E ∃ i error i (E, Ê) < error i (E, E <sup>0</sup> ) } (79)	page=43 xpos=1 ypos=1 single-column left-indent font-largest indented-line longer-tail line-double-space line-space headchar-lower above-blank-line above-double-space above-line-space
B-Body	To incorporate this concept of Pareto optimality into optimization, the Pareto-optimal	page=43 xpos=0 ypos=1 single-column full-justified hanged-line line-blank-line line-double-space line-space headchar-capital
I-Body	set is defined on the sentence level, and ranking loss (Section 3.5) is used to ensure	page=43 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	that the Pareto-optimal hypotheses achieve a higher score than those that are not Pareto	page=43 xpos=0 ypos=1 single-column full-justified aligned-line headchar-lower
I-Body	optimal. This method has also been extended to take advantage of ensemble decoding,	page=43 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	where multiple parameter settings are used simultaneously in decoding (Sankaran,	page=43 xpos=0 ypos=2 single-column full-justified aligned-line headchar-lower tailchar-comma
E-Body	Sarkar, and Duh 2013).	page=43 xpos=0 ypos=2 single-column right-indent aligned-line shorter-tail year headchar-capital tailchar-period above-blank-line above-double-space above-line-space
B-SubsectionHeader	8.4 Search and Optimization	page=43 xpos=0 ypos=2 single-column right-indent aligned-line longer-tail line-blank-line line-double-space line-space numbered-heading2 above-blank-line above-double-space above-line-space
B-Body	As mentioned in Section 2.4, because MT decoders perform approximate search, they	page=43 xpos=0 ypos=3 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	may make search errors and not find the hypothesis that achieves the highest model	page=43 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
E-Body	score. There have been a few attempts to consider this fact in the optimization process.	page=43 xpos=0 ypos=3 single-column right-indent aligned-line headchar-lower tailchar-period
B-Body	For example, in the perceptron algorithm of Section 6.2 it is known that the conver-	page=43 xpos=0 ypos=3 single-column left-indent indented-line headchar-capital tailchar-hiphen
I-Body	gence guarantees of the structured perceptron no longer hold when using approximate	page=43 xpos=0 ypos=3 single-column full-justified hanged-line headchar-lower
I-Body	search. The first method that can be used to resolve this problem is the early updating	page=43 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	strategy (Collins and Roark 2004; Cowan, Kuc̆erová, and Collins 2006). The early updat-	page=43 xpos=0 ypos=4 single-column full-justified aligned-line year headchar-lower tailchar-hiphen
I-Body	ing strategy is a variety of bold updates, where the decoder output e <sup>∗</sup> (i) must be exactly	page=43 xpos=0 ypos=4 single-column full-justified font-largest aligned-line headchar-lower
I-Body	equal to the reference e <sup>(i)</sup> . Decoding proceeds as normal, but the moment the correct	page=43 xpos=0 ypos=4 single-column full-justified font-largest aligned-line headchar-lower
I-Body	hypothesis e <sup>(i)</sup> can no longer be produced by any hypothesis in the search space (i.e.,	page=43 xpos=0 ypos=4 single-column full-justified font-largest aligned-line headchar-lower tailchar-comma
I-Body	a search error has occurred), search is stopped and update is performed using only	page=43 xpos=0 ypos=4 single-column full-justified aligned-line itemization headchar-lower
I-Body	the partial derivation. The second method is the max-violation perceptron (Huang,	page=43 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	Fayong, and Guo 2012; Yu et al. 2013). In the max-violation perceptron, forced decoding	page=43 xpos=0 ypos=5 single-column full-justified aligned-line year headchar-capital
I-Body	is performed to acquire a derivation h e <sup>∗</sup> (i) , d <sup>∗</sup> (i) i that can exactly reproduce the correct	page=43 xpos=0 ypos=5 single-column full-justified font-largest aligned-line headchar-lower
I-Body	output e <sup>(i)</sup> , and update is performed at the point when the score of a partial hypothesis	page=43 xpos=0 ypos=5 single-column full-justified font-largest aligned-line headchar-lower
I-Body	h e <sup>(i)</sup> ˆ <sub>,</sub> d <sup>ˆ</sup> (i) i exceeds that of the partial hypothesis h e ∗ (i) , d ∗ (i) i by the greatest margin (the	page=43 xpos=0 ypos=5 single-column full-justified font-largest aligned-line itemization headchar-lower
E-Body	point of “maximum violation”).	page=43 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	Search-aware tuning (Liu and Huang 2014) is a method that is able to consider	page=43 xpos=0 ypos=6 single-column left-indent indented-line longer-tail year headchar-capital
I-Body	search errors using an arbitrary optimization method. It does so by defining an evalu-	page=43 xpos=0 ypos=6 single-column full-justified hanged-line headchar-lower tailchar-hiphen
I-Body	ation measure for not only full sentences, but also partial derivations that occur during	page=43 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
E-Body	the search process, and optimizes parameters for k-best lists of partial derivations.	page=43 xpos=0 ypos=6 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	Finally, there has also been some work on optimizing features not of the model itself,	page=43 xpos=0 ypos=6 single-column left-indent indented-line longer-tail headchar-capital tailchar-comma
I-Body	but parameters of the search process, using the downhill simplex algorithm (Chung	page=43 xpos=0 ypos=7 single-column full-justified hanged-line headchar-lower
I-Body	and Galley 2012). Using this method, it is possible to adjust the beam width, distortion	page=43 xpos=0 ypos=7 single-column full-justified aligned-line year headchar-lower
I-Body	penalty, or other parameters that actually affect the size and shape of the derivation	page=43 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
E-Body	space, as opposed to simply rescoring hypotheses within it.	page=43 xpos=0 ypos=7 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SectionHeader	9. Conclusion	page=43 xpos=0 ypos=8 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space itemization above-blank-line above-double-space above-line-space
B-Body	In this survey article, we have provided a review of the current state-of-the-art in	page=43 xpos=0 ypos=8 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	machine translation optimization, covering batch optimization, online optimization,	page=43 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	expansions to large scale data, and a number of other topics. While these optimization	page=43 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	algorithms have already led to large improvements in machine translation accuracy, the	page=43 xpos=0 ypos=8 single-column full-justified aligned-line headchar-lower
I-Body	task of MT optimization is, as stated in the Introduction, an extremely hard one that is	page=43 xpos=0 ypos=9 single-column full-justified aligned-line headchar-lower
E-Body	far from solved.	page=43 xpos=0 ypos=9 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
Page	44	page=43 xpos=0 ypos=9 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Neubig and Watanabe Optimization for Statistical Machine Translation	page=44 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
B-Caption	Table 2	page=44 xpos=0 ypos=0 single-column right-indent font-smallest aligned-line shorter-tail line-blank-line line-double-space line-space string-table headchar-capital
I-Caption	Percent of WMT systems using each optimization method, including all systems, or systems that	page=44 xpos=0 ypos=0 single-column full-justified font-smallest aligned-line longer-tail headchar-capital
E-Caption	achieved the best results on at least one language pair.	page=44 xpos=0 ypos=1 single-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period column-bottom
Table	__Table 2__	page=44 xpos=-1 ypos=1 Table-column centered left-over right-over box column-top table-area column-bottom above-blank-line above-double-space above-line-space
B-Body	The utility of an optimization algorithm can be viewed from a number of perspec-	page=44 xpos=0 ypos=3 single-column left-indent column-top line-blank-line line-double-space line-space headchar-capital tailchar-hiphen
I-Body	tives. The final accuracy achieved is, of course, one of the most important factors, but	page=44 xpos=0 ypos=3 single-column full-justified hanged-line headchar-lower
I-Body	speed, scalability, ease of implementation, final resulting model size, and many other	page=44 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	factors play an important role. We can assume that the algorithms being used outside of	page=44 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	the context of research on optimization itself are those that satisfy these criteria in some	page=44 xpos=0 ypos=3 single-column full-justified aligned-line headchar-lower
I-Body	way. Although it is difficult to exactly discern exactly which algorithms are seeing the	page=44 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	largest amount of use (industrial SMT systems rarely disclose this sort of information	page=44 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	publicly), one proxy for this is to look at systems that performed well on shared tasks	page=44 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	such as the Workshop on Machine Translation (WMT) (Bojar et al. 2014). In Table 2	page=44 xpos=0 ypos=4 single-column full-justified aligned-line year headchar-lower
I-Body	we show the percentage of WMT systems using each optimization algorithm for the	page=44 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	past four years, both including all systems, and systems that achieved the highest level	page=44 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
I-Body	of human evaluation in the resource-constrained setting for at least one language pair.	page=44 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower tailchar-period
I-Body	From these statistics we can see that even after over ten years, MERT is still the dominant	page=44 xpos=0 ypos=5 single-column full-justified aligned-line headchar-capital
I-Body	optimization algorithm. However, starting in WMT 2013, we can see a move to systems	page=44 xpos=0 ypos=5 single-column full-justified aligned-line year headchar-lower
I-Body	based on MIRA, and to a lesser extent ranking, particularly in the most competitive	page=44 xpos=0 ypos=5 single-column full-justified aligned-line headchar-lower
E-Body	systems.	page=44 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	In these systems, the preferred choice of an optimization algorithm seems to be	page=44 xpos=0 ypos=6 single-column left-indent indented-line longer-tail headchar-capital
I-Body	MERT when using up to 20 features, and MIRA when using a large number of features	page=44 xpos=0 ypos=6 single-column full-justified hanged-line headchar-capital
I-Body	(up to several hundred). There are fewer examples of systems using large numbers	page=44 xpos=0 ypos=6 single-column full-justified aligned-line
I-Body	of features (tens of thousands, or millions) in actual competitive systems, with a few	page=44 xpos=0 ypos=6 single-column full-justified aligned-line headchar-lower
I-Body	exceptions (Dyer et al. 2009; Neidert et al. 2014; Wuebker et al. 2014). In the case when	page=44 xpos=0 ypos=6 single-column full-justified aligned-line year headchar-lower
I-Body	a large number of sparse features are used, it is most common to use a softmax or	page=44 xpos=0 ypos=6 single-column full-justified aligned-line itemization headchar-lower
I-Body	risk-based objective and gradient-based optimization algorithms, often combining the	page=44 xpos=0 ypos=7 single-column full-justified aligned-line headchar-lower
E-Body	features into summary features and performing a final tuning pass with MERT.	page=44 xpos=0 ypos=7 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	The fact that algorithms other than MERT are seeing adoption in competitive sys-	page=44 xpos=0 ypos=7 single-column left-indent indented-line longer-tail headchar-capital tailchar-hiphen
I-Body	tems for shared tasks is a welcome sign for the future of MT optimization research.	page=44 xpos=0 ypos=7 single-column full-justified hanged-line headchar-lower tailchar-period
I-Body	However, there are still many open questions in the field, a few of which can be outlined	page=44 xpos=0 ypos=7 single-column full-justified aligned-line headchar-capital
I-Body	here:	page=44 xpos=0 ypos=7 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-colon above-double-space above-line-space
B-Listitem	Stable Training with Millions of Features: At the moment, there is still no stable train-	page=44 xpos=0 ypos=8 single-column full-justified aligned-line longer-tail line-double-space line-space headchar-capital tailchar-hiphen
I-Listitem	ing recipe that has been widely proven to effectively optimize millions of features.	page=44 xpos=0 ypos=8 single-column left-indent indented-line headchar-lower tailchar-period
I-Listitem	Finding an algorithm that gives consistent improvements in this setting is perhaps	page=44 xpos=0 ypos=8 single-column left-indent aligned-line headchar-capital
I-Listitem	the largest open problem in MT optimization.	page=44 xpos=0 ypos=8 single-column left-indent right-indent aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-Listitem	Evaluation Measures for Optimization: Although many evaluation measures show	page=44 xpos=0 ypos=9 single-column full-justified hanged-line longer-tail line-double-space line-space headchar-capital
I-Listitem	consistent improvements in correlation with human evaluation scores over BLEU	page=44 xpos=0 ypos=9 single-column left-indent indented-line headchar-lower above-blank-line above-double-space above-line-space
Page	45	page=44 xpos=9 ypos=9 single-column left-indent indented-line line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Computational Linguistics Volume 42, Number 1	page=45 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Listitem	when used to evaluate the output of existing MT systems, there are few results that	page=45 xpos=0 ypos=0 single-column left-indent indented-line line-blank-line line-double-space line-space headchar-lower
I-Listitem	show that systems optimized with evaluation measures other than BLEU achieve	page=45 xpos=0 ypos=0 single-column left-indent aligned-line headchar-lower
I-Listitem	consistent improvements in human evaluation scores.	page=45 xpos=0 ypos=0 single-column left-indent right-indent aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-Listitem	Better Training/Utilization of Nonlinear Scoring Functions: Nonlinear functions us-	page=45 xpos=0 ypos=1 single-column full-justified hanged-line longer-tail line-double-space line-space headchar-capital tailchar-hiphen
I-Listitem	ing neural networks have recently achieved large improvements in a number of	page=45 xpos=0 ypos=1 single-column left-indent indented-line headchar-lower
I-Listitem	areas of natural language processing and machine learning. Better methods to	page=45 xpos=0 ypos=1 single-column left-indent aligned-line headchar-lower
I-Listitem	incorporate these sort of nonlinear scoring functions into MT is a highly promising	page=45 xpos=0 ypos=1 single-column left-indent aligned-line headchar-lower
I-Listitem	direction, but will require improvements in both learning the scoring functions	page=45 xpos=0 ypos=1 single-column left-indent aligned-line headchar-lower
I-Listitem	and correctly incorporating these functions into MT decoding.	page=45 xpos=0 ypos=2 single-column left-indent right-indent aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-Body	Resolving these, and many other, open questions will likely be among the top priorities	page=45 xpos=0 ypos=2 single-column full-justified hanged-line longer-tail line-double-space line-space headchar-capital
E-Body	of MT optimization research in the years to come.	page=45 xpos=0 ypos=2 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
AppendixHeader	Appendix A: Derivation for xBLEU Gradients	page=45 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail line-blank-line line-double-space line-space string-appendix headchar-capital above-blank-line above-double-space above-line-space
B-Body	In this appendix, we explain in detail how to derive a gradient for the xBLEU objective	page=45 xpos=0 ypos=3 single-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital
E-Body	in Equation (54), which has not been described completely in previous work.	page=45 xpos=0 ypos=3 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	First, we consider the brevity penalty	page=45 xpos=0 ypos=3 single-column left-indent right-indent indented-line shorter-tail headchar-capital above-blank-line above-double-space above-line-space
B-Equation	φ (x) = min { 1, exp(x) } (A.1)	page=45 xpos=3 ypos=4 single-column left-indent font-larger indented-line longer-tail line-blank-line line-double-space line-space above-blank-line above-double-space above-line-space
B-Body	We can see that it is not differentiable, which precludes the use of gradient-based	page=45 xpos=0 ypos=4 single-column full-justified hanged-line line-blank-line line-double-space line-space headchar-capital
I-Body	algorithms. To remedy this problem, Rosti et al. (2010) propose the use of the following	page=45 xpos=0 ypos=4 single-column full-justified aligned-line year headchar-lower
E-Body	differentiable approximation for the brevity penalty.	page=45 xpos=0 ypos=4 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-Equation	exp(x) − 1	page=45 xpos=4 ypos=5 single-column left-indent right-indent font-larger indented-line shorter-tail line-blank-line line-double-space line-space headchar-lower
I-Equation	ϕ (x) = + 1 (A.2)	page=45 xpos=3 ypos=5 single-column left-indent hanged-line longer-tail
I-Equation	1 + exp(10000x)	page=45 xpos=4 ypos=5 single-column left-indent right-indent indented-line shorter-tail numbered-heading1 above-blank-line above-double-space above-line-space
B-Body	Using Equation (54) and the approximated brevity penalty in Equation (A.2), we can	page=45 xpos=0 ypos=5 single-column full-justified hanged-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Body	express xBLEU = exp(P) · B, leading to the following equation	page=45 xpos=0 ypos=6 single-column right-indent font-larger aligned-line shorter-tail headchar-lower above-double-space above-line-space
B-Equation	P = 1 <sub>4</sub> X <sub>n</sub> = <sup>4</sup> <sub>1</sub> log X i = <sup>N</sup> <sub>1</sub> <sub>k</sub> X = K <sub>1</sub> m n,i,k − log X i = N 1 k X = K 1 c n,i,k !	page=45 xpos=2 ypos=6 single-column centered left-indent right-indent box indented-line longer-tail line-double-space line-space headchar-capital
I-Equation	(A.3)	page=45 xpos=9 ypos=6 single-column left-indent indented-line longer-tail above-blank-line above-double-space above-line-space
I-Equation	B = ϕ (1 − R) (A.4)	page=45 xpos=2 ypos=7 single-column left-indent font-larger hanged-line line-blank-line line-double-space line-space headchar-capital above-double-space above-line-space
I-Equation	P N P K r <sub>i,k</sub>	page=45 xpos=2 ypos=7 single-column left-indent right-indent font-largest indented-line shorter-tail line-double-space line-space headchar-capital
I-Equation	R = <sub>P</sub> N <sub>i</sub> = <sup>i</sup> = <sub>1</sub> 1 P K k = k = 1 1 c 1,i,k	page=45 xpos=2 ypos=7 single-column left-indent right-indent font-largest hanged-line headchar-capital
I-Equation	(A.5)	page=45 xpos=9 ypos=7 single-column left-indent indented-line longer-tail above-blank-line above-double-space above-line-space
B-Body	Taking the derivative of xBLEU with respect to w, we get	page=45 xpos=0 ypos=8 single-column right-indent hanged-line shorter-tail line-blank-line line-double-space line-space headchar-capital above-blank-line above-double-space above-line-space
I-Equation	N K	page=45 xpos=3 ypos=8 single-column left-indent right-indent font-smallest indented-line shorter-tail line-blank-line line-double-space line-space headchar-capital
I-Equation	∂ xBLEU = X X ∂ <sub>∂</sub> xBLEU <sub>log</sub> p <sub>i,k</sub> <sup>∂</sup> <sup>log</sup> ∂ w <sup>p</sup> i,k	page=45 xpos=2 ypos=8 single-column left-indent right-indent font-largest hanged-line longer-tail
I-Equation	(A.6)	page=45 xpos=9 ypos=8 single-column left-indent indented-line longer-tail
I-Equation	∂ w	page=45 xpos=2 ypos=8 single-column left-indent right-indent hanged-line shorter-tail
I-Equation	i = 1 k = 1	page=45 xpos=3 ypos=8 single-column left-indent right-indent font-smallest indented-line longer-tail itemization headchar-lower above-double-space above-line-space
I-Equation	N K K	page=45 xpos=3 ypos=9 single-column left-indent right-indent font-smallest longer-tail line-double-space line-space headchar-capital
I-Equation	= <sup>X</sup> X X <sup>∂</sup> <sub>∂</sub> <sup>xBLEU</sup> <sub>log</sub> p <sub>i,k</sub> <sup>∂</sup> <sup>log</sup> ∂ s i,k <sup>p</sup> 0 i,k ∂ ∂ <sup>s</sup> w i,k 0	page=45 xpos=3 ypos=9 single-column left-indent right-indent font-largest hanged-line longer-tail
I-Equation	(A.7)	page=45 xpos=9 ypos=9 single-column left-indent indented-line longer-tail above-line-space
I-Equation	i = 1 k = 1 k <sup>0</sup> = 1	page=45 xpos=3 ypos=9 single-column left-indent right-indent font-smallest hanged-line shorter-tail line-space itemization headchar-lower above-blank-line above-double-space above-line-space
Page	46	page=45 xpos=0 ypos=9 single-column right-indent hanged-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Neubig and Watanabe Optimization for Statistical Machine Translation	page=46 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital column-bottom above-blank-line above-double-space above-line-space
I-Body	Thus,	page=46 xpos=0 ypos=0 left-column right-indent column-top line-blank-line line-double-space line-space headchar-capital tailchar-comma column-bottom above-double-space above-line-space
B-Equation	∂ s <sub>i,k</sub> 0 = γ · <sub>h(</sub> f (i) <sub>,</sub> e <sub>k</sub> 0 , d k 0 ) (A.8)	page=46 xpos=3 ypos=0 single-column left-indent font-largest column-top line-double-space line-space
I-Equation	∂ w	page=46 xpos=3 ypos=1 single-column left-indent right-indent shorter-tail above-line-space
I-Equation	∂ log p <sub>i,k</sub> = δ <sub>(k,</sub> k 0 ) − p <sub>i,k</sub> 0 (A.9)	page=46 xpos=3 ypos=1 single-column left-indent font-largest hanged-line longer-tail line-space
I-Equation	∂ s <sub>i,k</sub> 0	page=46 xpos=3 ypos=1 single-column left-indent right-indent font-largest indented-line shorter-tail column-bottom above-double-space above-line-space
I-Body	and	page=46 xpos=0 ypos=1 left-column right-indent column-top line-double-space line-space headchar-lower column-bottom above-double-space above-line-space
B-Equation	∂ xBLEU = <sub>exp(P)</sub> · <sub>∂</sub> <sub>log</sub> ∂ B p <sub>i,k</sub> + exp(P) · ∂ log ∂ P p i,k · B	page=46 xpos=2 ypos=2 single-column centered left-indent right-indent font-largest column-top line-double-space line-space
I-Equation	(A.10)	page=46 xpos=9 ypos=2 single-column left-indent indented-line longer-tail
I-Equation	∂ log p <sub>i,k</sub>	page=46 xpos=2 ypos=2 single-column left-indent right-indent font-largest hanged-line shorter-tail column-bottom above-double-space above-line-space
B-Body	Additionally, the following equation holds:	page=46 xpos=0 ypos=2 left-column right-indent column-top line-double-space line-space headchar-capital tailchar-colon above-double-space above-line-space
B-Equation	∂ B = ∂ B <sup>∂</sup> <sup>1</sup> <sup>−</sup> R	page=46 xpos=1 ypos=2 left-column left-indent right-indent font-largest indented-line shorter-tail line-double-space line-space
I-Equation	∂ log p <sub>i,k</sub> ∂ 1 − R ∂ log p i,k	page=46 xpos=0 ypos=3 left-column left-indent right-indent font-largest hanged-line column-bottom
I-Equation	(A.11)	page=46 xpos=9 ypos=3 right-column left-indent column-top column-bottom above-double-space above-line-space
I-Equation	= ϕ 0 (1 − R) · ( − R) · <sub>P</sub> N <sub>i</sub> 0 = <sub>1</sub> P <sup>r</sup> i,k K k 0 = 1 r i 0 ,k 0 − P N i 0 = 1 P c 1,i,k K k 0 = 1 c 1,i 0 ,k 0 <sup>!</sup> (A.12)	page=46 xpos=1 ypos=3 single-column left-indent box column-top line-double-space line-space above-double-space above-line-space
I-Equation	4	page=46 xpos=2 ypos=3 single-column left-indent right-indent font-smallest indented-line shorter-tail line-double-space line-space numeric-only
I-Equation	∂ log p <sub>i,k</sub> 4 <sub>n</sub> = <sub>1</sub> P <sup>N</sup> i 0 = 1 P K k 0 = 1 m n,i 0 ,k 0 <sup>−</sup> P N i 0 = 1 P <sup>c</sup> n,i,k K k 0 = 1 c n,i 0 ,k 0	page=46 xpos=0 ypos=4 single-column left-indent right-indent font-largest hanged-line longer-tail
I-Equation	m <sub>n,i,k</sub>	page=46 xpos=3 ypos=4 single-column left-indent right-indent font-largest indented-line shorter-tail itemization headchar-lower
I-Equation	∂ P = 1 X (A.13)	page=46 xpos=1 ypos=4 single-column left-indent font-largest hanged-line longer-tail above-blank-line above-double-space above-line-space
B-Body	After calculating this gradient, it is possible to optimize this according to standard	page=46 xpos=0 ypos=4 single-column left-indent hanged-line line-blank-line line-double-space line-space headchar-capital
I-Body	gradient-based methods. However, like MR using sentence-level evaluation mentioned	page=46 xpos=0 ypos=4 single-column full-justified hanged-line headchar-lower
I-Body	in Section 5.5, the evaluation measure is not convex, and the same precautions need to	page=46 xpos=0 ypos=4 single-column full-justified aligned-line headchar-lower
E-Body	be taken to avoid falling into local optima.	page=46 xpos=0 ypos=5 single-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-Body	References Blunsom, Phil, Trevor Cohn, and Miles	page=46 xpos=0 ypos=5 single-column right-indent aligned-line longer-tail line-blank-line line-double-space line-space string-reference headchar-capital column-bottom above-line-space
B-Reference	Andrew, Galen and Jianfeng Gao. 2007.	page=46 xpos=0 ypos=5 left-column right-indent font-smallest column-top line-space year headchar-capital tailchar-period
I-Reference	Scalable training of L1-regularized	page=46 xpos=0 ypos=5 left-column left-indent right-indent font-smallest indented-line shorter-tail headchar-capital
I-Reference	log-linear models. In Proceedings of ICML,	page=46 xpos=0 ypos=6 left-column left-indent right-indent font-smallest aligned-line longer-tail headchar-lower tailchar-comma
I-Reference	pages 33–40, Corvalis, OR.	page=46 xpos=0 ypos=6 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period column-bottom
B-Reference	Osborne. 2008. A discriminative latent	page=46 xpos=5 ypos=5 right-column right-indent font-smallest column-top year headchar-capital
I-Reference	variable model for statistical machine	page=46 xpos=5 ypos=5 right-column right-indent font-smallest aligned-line headchar-lower
I-Reference	translation. In Proceedings of ACL/HLT,	page=46 xpos=5 ypos=6 right-column right-indent font-smallest aligned-line headchar-lower tailchar-comma
I-Reference	pages 200–208, Columbus, OH.	page=46 xpos=5 ypos=6 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period column-bottom
B-Reference	Banerjee, Satanjeev and Alon Lavie. 2005. Blunsom, Phil and Miles Osborne. 2008.	page=46 xpos=0 ypos=6 single-column right-indent font-larger column-top year headchar-capital tailchar-period column-bottom
I-Reference	METEOR: An automatic metric for MT	page=46 xpos=0 ypos=6 left-column left-indent right-indent font-smallest column-top headchar-capital
I-Reference	evaluation with improved correlation with	page=46 xpos=0 ypos=6 left-column left-indent right-indent font-smallest aligned-line longer-tail headchar-lower
I-Reference	human judgments. In Proceedings of the	page=46 xpos=0 ypos=6 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower column-bottom
B-Reference	Probabilistic inference for machine	page=46 xpos=5 ypos=6 right-column right-indent font-smallest column-top headchar-capital
I-Reference	translation. In Proceedings of EMNLP,	page=46 xpos=5 ypos=6 right-column right-indent font-smallest aligned-line longer-tail headchar-lower tailchar-comma
I-Reference	pages 215–223, Honolulu, HI.	page=46 xpos=5 ypos=6 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period column-bottom
I-Reference	ACL Workshop on Intrinsic and Extrinsic Bojar, Ondrej, Christian Buck, Christian	page=46 xpos=0 ypos=6 single-column left-indent right-indent font-larger column-top headchar-capital column-bottom
I-Reference	Evaluation Measures for Machine Translation	page=46 xpos=0 ypos=7 left-column left-indent right-indent font-smallest column-top headchar-capital
I-Reference	and/or Summarization, pages 65–72,	page=46 xpos=0 ypos=7 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-comma
I-Reference	Ann Arbor, MI.	page=46 xpos=0 ypos=7 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Bazrafshan, Marzieh, Tagyoung Chung, and	page=46 xpos=0 ypos=7 left-column right-indent font-smallest hanged-line longer-tail headchar-capital
I-Reference	Daniel Gildea. 2012. Tuning as linear	page=46 xpos=0 ypos=7 left-column left-indent right-indent font-smallest indented-line shorter-tail year headchar-capital
I-Reference	regression. In Proceedings of HLT/NAACL,	page=46 xpos=0 ypos=7 left-column left-indent right-indent font-smallest aligned-line longer-tail headchar-lower tailchar-comma
I-Reference	pages 543–547, Montréal.	page=46 xpos=0 ypos=7 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Reference	Bentley, J. L. and T. A. Ottmann. 1979.	page=46 xpos=0 ypos=8 left-column right-indent font-smallest hanged-line longer-tail year headchar-capital tailchar-period column-bottom
I-Reference	Federmann, Barry Haddow, Philipp	page=46 xpos=5 ypos=7 right-column right-indent font-smallest column-top headchar-capital
B-Reference	Koehn, Johannes Leveling, Christof Monz,	page=46 xpos=5 ypos=7 right-column right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-comma
I-Reference	Pavel Pecina, Matt Post, Herve	page=46 xpos=5 ypos=7 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital
B-Reference	Saint-Amand, Radu Soricut, Lucia Specia,	page=46 xpos=5 ypos=7 right-column right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-comma
I-Reference	and Aleš Tamchyna. 2014. Findings of the	page=46 xpos=5 ypos=7 right-column right-indent font-smallest aligned-line year headchar-lower
I-Reference	2014 workshop on statistical machine	page=46 xpos=5 ypos=7 right-column right-indent font-smallest aligned-line shorter-tail year
I-Reference	translation. In Proceedings of WMT,	page=46 xpos=5 ypos=7 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-comma
I-Reference	pages 12–58, Baltimore, MD.	page=46 xpos=5 ypos=8 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period column-bottom
I-Reference	Algorithms for reporting and counting Bottou, Léon. 1998. Online algorithms and	page=46 xpos=0 ypos=8 single-column left-indent right-indent column-top year headchar-capital column-bottom
I-Reference	geometric intersections. IEEE Transactions	page=46 xpos=0 ypos=8 left-column left-indent right-indent font-smallest column-top headchar-lower
I-Reference	on Computers, 28(9):643–647.	page=46 xpos=0 ypos=8 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Reference	Berger, Adam L., Vincent J. Della Pietra, and	page=46 xpos=0 ypos=8 left-column right-indent font-smallest hanged-line longer-tail headchar-capital
I-Reference	Stephen A. Della Pietra. 1996. A maximum	page=46 xpos=0 ypos=8 left-column left-indent right-indent font-smallest indented-line year headchar-capital column-bottom
I-Reference	stochastic approximations. In David Saad,	page=46 xpos=5 ypos=8 right-column right-indent font-smallest column-top headchar-lower tailchar-comma
B-Reference	editor, Online Learning and Neural Networks.	page=46 xpos=5 ypos=8 right-column full-justified font-smallest aligned-line longer-tail headchar-lower tailchar-period
I-Reference	Cambridge University Press, pages 9–42,	page=46 xpos=5 ypos=8 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-comma
I-Reference	Cambridge, UK.	page=46 xpos=5 ypos=8 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period column-bottom
I-Reference	entropy approach to natural language Brown, Peter F., Vincent J. Della Pietra,	page=46 xpos=0 ypos=9 single-column left-indent right-indent column-top headchar-lower tailchar-comma column-bottom
I-Reference	processing. Computational Linguistics,	page=46 xpos=0 ypos=9 left-column left-indent right-indent font-smallest column-top headchar-lower tailchar-comma
I-Reference	22:39–71.	page=46 xpos=0 ypos=9 left-column left-indent right-indent font-smallest aligned-line shorter-tail tailchar-period column-bottom
B-Reference	Stephen A. Della Pietra, and Robert L.	page=46 xpos=5 ypos=9 right-column right-indent font-smallest column-top headchar-capital tailchar-period
I-Reference	Mercer. 1993. The mathematics of	page=46 xpos=5 ypos=9 right-column right-indent font-smallest aligned-line shorter-tail year headchar-capital above-blank-line above-double-space above-line-space
Page	47	page=46 xpos=9 ypos=9 right-column left-indent indented-line longer-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Computational Linguistics Volume 42, Number 1	page=47 xpos=0 ypos=0 single-column right-over font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Reference	statistical machine translation: Parameter machine translation. In Proceedings of	page=47 xpos=0 ypos=0 single-column left-indent right-over font-smallest indented-line shorter-tail line-blank-line line-double-space line-space headchar-lower
I-Reference	estimation. Computational Linguistics, HLT/NAACL, pages 218–226, Boulder, CO.	page=47 xpos=0 ypos=0 single-column left-indent right-over font-smallest aligned-line longer-tail headchar-lower tailchar-period
I-Reference	19(2):263–311. Chiang, David, Yuval Marton, and Philip	page=47 xpos=0 ypos=0 single-column left-indent right-over font-smallest aligned-line shorter-tail
B-Reference	Burges, Chris, Tal Shaked, Erin Renshaw, Ari Resnik. 2008. Online large-margin training	page=47 xpos=0 ypos=1 single-column right-over font-smallest hanged-line longer-tail year headchar-capital
I-Reference	Lazier, Matt Deeds, Nicole Hamilton, and of syntactic and structural translation	page=47 xpos=0 ypos=1 single-column left-indent right-over font-smallest indented-line shorter-tail headchar-capital
I-Reference	Greg Hullender. 2005. Learning to rank features. In Proceedings of EMNLP,	page=47 xpos=0 ypos=1 single-column left-indent right-over font-smallest aligned-line shorter-tail year headchar-capital tailchar-comma
I-Reference	using gradient descent. In Proceedings of pages 224–233, Honolulu, HI.	page=47 xpos=0 ypos=1 single-column left-indent right-over font-smallest aligned-line shorter-tail headchar-lower tailchar-period
I-Reference	ICML, pages 89–96, Bonn. Chung, Tagyoung and Michel Galley. 2012.	page=47 xpos=0 ypos=1 single-column left-indent right-over font-smallest aligned-line longer-tail year headchar-capital tailchar-period
B-Reference	Callison-Burch, Chris, Philipp Koehn, Direct error rate minimization for	page=47 xpos=0 ypos=1 single-column right-over font-smallest hanged-line shorter-tail headchar-capital
I-Reference	Christof Monz, and Omar Zaidan. 2011. statistical machine translation. In	page=47 xpos=0 ypos=1 single-column left-indent right-over font-smallest indented-line year headchar-capital
I-Reference	Findings of the 2011 workshop on Proceedings of WMT, pages 468–479,	page=47 xpos=0 ypos=2 single-column left-indent right-over font-smallest aligned-line longer-tail year headchar-capital tailchar-comma
I-Reference	statistical machine translation. In Montréal.	page=47 xpos=0 ypos=2 single-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
I-Reference	Proceedings of WMT, pages 22–64, Clark, Jonathan H., Chris Dyer, and Alon	page=47 xpos=0 ypos=2 single-column left-indent right-over font-smallest aligned-line longer-tail headchar-capital
I-Reference	Edinburgh. Lavie. 2014. Locally non-linear learning for	page=47 xpos=0 ypos=2 single-column left-indent right-over font-smallest aligned-line longer-tail year headchar-capital
B-Reference	Cao, Zhe, Tao Qin, Tie-Yan Liu, Ming-Feng statistical machine translation via	page=47 xpos=0 ypos=2 single-column right-over font-smallest hanged-line shorter-tail headchar-capital
I-Reference	Tsai, and Hang Li. 2007. Learning to rank: discretization and structured	page=47 xpos=0 ypos=2 single-column left-indent font-smallest indented-line shorter-tail year headchar-capital
I-Reference	From pairwise approach to listwise regularization. Transactions of the	page=47 xpos=0 ypos=2 single-column left-indent right-over font-smallest aligned-line longer-tail headchar-capital
I-Reference	approach. In Proceedings of ICML, Association for Computational Linguistics,	page=47 xpos=0 ypos=3 single-column left-indent right-over font-smallest aligned-line longer-tail headchar-lower tailchar-comma
I-Reference	pages 129–136, Corvalis, OR. 2(1):393–404.	page=47 xpos=0 ypos=3 single-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Reference	Cer, Daniel, Dan Jurafsky, and Clark, Jonathan H., Chris Dyer, Alon Lavie,	page=47 xpos=0 ypos=3 single-column right-over font-smallest hanged-line longer-tail headchar-capital tailchar-comma
I-Reference	Christopher D. Manning. 2008. and Noah A. Smith. 2011. Better	page=47 xpos=0 ypos=3 single-column left-indent right-over font-smallest indented-line shorter-tail year headchar-capital
I-Reference	Regularization and search for minimum hypothesis testing for statistical machine	page=47 xpos=0 ypos=3 single-column left-indent right-over font-smallest aligned-line longer-tail headchar-capital
I-Reference	error rate training. In Proceedings of WMT, translation: Controlling for optimizer	page=47 xpos=0 ypos=3 single-column left-indent right-over font-smallest aligned-line shorter-tail headchar-lower
I-Reference	pages 26–34, Columbus, OH. instability. In Proceedings of ACL/HLT,	page=47 xpos=0 ypos=3 single-column left-indent right-over font-smallest aligned-line headchar-lower tailchar-comma
I-Reference	Cer, Daniel, Christopher D. Manning, and pages 176–181, Portland, OR.	page=47 xpos=0 ypos=4 single-column centered right-over font-smallest hanged-line shorter-tail headchar-capital tailchar-period
B-Reference	Daniel Jurafsky. 2010. The best lexical Clark, Jonathan H., Alon Lavie, and Chris	page=47 xpos=0 ypos=4 single-column left-indent right-over font-smallest indented-line longer-tail year headchar-capital
I-Reference	metric for phrase-based statistical MT Dyer. 2012. One system, many domains:	page=47 xpos=0 ypos=4 single-column left-indent right-over font-smallest aligned-line year headchar-lower tailchar-colon
I-Reference	system optimization. In Proceedings of Open-domain statistical machine	page=47 xpos=0 ypos=4 single-column left-indent right-over font-smallest aligned-line shorter-tail headchar-lower
I-Reference	HLT/NAACL, pages 555–563, Los Angeles, translation via feature augmentation. In	page=47 xpos=0 ypos=4 single-column left-indent right-over font-smallest aligned-line longer-tail headchar-capital
I-Reference	CA. Proceedings of AMTA, San Diego, CA.	page=47 xpos=0 ypos=4 single-column left-indent right-over font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Chappelier, Jean-Cédric and Martin Rajman. Collins, Michael. 2002. Discriminative	page=47 xpos=0 ypos=5 single-column right-over font-smallest hanged-line shorter-tail year headchar-capital
I-Reference	1998. A generalized CYK algorithm for training methods for hidden Markov	page=47 xpos=0 ypos=5 single-column left-indent right-over font-smallest indented-line longer-tail year
I-Reference	parsing stochastic CFG. In Proceedings of models: Theory and experiments with	page=47 xpos=0 ypos=5 single-column left-indent right-over font-smallest aligned-line longer-tail headchar-lower
I-Reference	the 1st Workshop on Tabulation in Parsing and perceptron algorithms. In Proceedings of	page=47 xpos=0 ypos=5 single-column left-indent right-over font-smallest aligned-line longer-tail headchar-lower
I-Reference	Deduction, pages 133–137, Paris. EMNLP, pages 1–8, Philadelphia, PA.	page=47 xpos=0 ypos=5 single-column left-indent right-over font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Chatterjee, Samidh and Nicola Cancedda. Collins, Michael, Amir Globerson, Terry Koo,	page=47 xpos=0 ypos=5 single-column right-over font-smallest hanged-line longer-tail headchar-capital tailchar-comma
I-Reference	2010. Minimum error rate training by Xavier Carreras, and Peter L. Bartlett.	page=47 xpos=0 ypos=5 single-column left-indent right-over font-smallest indented-line shorter-tail year tailchar-period
I-Reference	sampling the translation lattice. In 2008. Exponentiated gradient algorithms	page=47 xpos=0 ypos=6 single-column left-indent right-over font-smallest aligned-line longer-tail year headchar-lower
I-Reference	Proceedings of EMNLP, pages 606–615, for conditional random fields and	page=47 xpos=0 ypos=6 single-column left-indent right-over font-smallest aligned-line shorter-tail headchar-capital
I-Reference	Cambridge, MA. max-margin Markov networks. Journal of	page=47 xpos=0 ypos=6 single-column left-indent right-over font-smallest aligned-line longer-tail headchar-capital
B-Reference	Chen, Stanley F. and Ronald Rosenfeld. 1999. Machine Learning Research, 9:1775–1822.	page=47 xpos=0 ypos=6 single-column right-over font-smallest hanged-line shorter-tail year headchar-capital tailchar-period
I-Reference	A Gaussian prior for smoothing maximum Collins, Michael and Brian Roark. 2004.	page=47 xpos=0 ypos=6 single-column left-indent right-over font-smallest indented-line shorter-tail year headchar-capital tailchar-period
I-Reference	entropy models. Technical report Incremental parsing with the perceptron	page=47 xpos=0 ypos=6 single-column left-indent right-over font-smallest aligned-line longer-tail headchar-lower
I-Reference	CMU-CS-99-108, DTIC Document. algorithm. In Proceedings of ACL,	page=47 xpos=0 ypos=6 single-column left-indent right-over font-smallest aligned-line shorter-tail headchar-capital tailchar-comma
I-Reference	Carnegie Mellon School of Computer pages 111–118, Barcelona.	page=47 xpos=0 ypos=7 single-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
I-Reference	Science. Cowan, Brooke, Ivona Kuc̆erová, and	page=47 xpos=0 ypos=7 single-column left-indent right-over font-smallest aligned-line longer-tail headchar-capital
B-Reference	Cherry, Colin and George Foster. 2012. Batch Michael Collins. 2006. A discriminative	page=47 xpos=0 ypos=7 single-column right-over font-smallest hanged-line longer-tail year headchar-capital
I-Reference	tuning strategies for statistical machine model for tree-to-tree translation. In	page=47 xpos=0 ypos=7 single-column left-indent right-over font-smallest indented-line shorter-tail headchar-lower
I-Reference	translation. In Proceedings of HLT/NAACL, Proceedings of EMNLP, pages 232–241,	page=47 xpos=0 ypos=7 single-column left-indent right-over font-smallest aligned-line longer-tail headchar-lower tailchar-comma
I-Reference	pages 427–436, Montréal, Canada. Sydney.	page=47 xpos=0 ypos=7 single-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Reference	Chiang, David. 2007. Hierarchical Crammer, Koby, Ofer Dekel, Joseph Keshet,	page=47 xpos=0 ypos=8 single-column right-over font-smallest hanged-line longer-tail year headchar-capital tailchar-comma
I-Reference	phrase-based translation. Computational Shai Shalev-Shwartz, and Yoram Singer.	page=47 xpos=0 ypos=8 single-column left-indent right-over font-smallest indented-line shorter-tail headchar-lower tailchar-period
I-Reference	Linguistics, 33(2):201–228. 2006. Online passive-aggressive	page=47 xpos=0 ypos=8 single-column left-indent right-over font-smallest aligned-line shorter-tail year headchar-capital
B-Reference	Chiang, David. 2012. Hope and fear for algorithms. Journal of Machine Learning	page=47 xpos=0 ypos=8 single-column right-over font-smallest hanged-line longer-tail year headchar-capital
I-Reference	discriminative training of statistical Research, 7:551–585.	page=47 xpos=0 ypos=8 single-column left-indent right-indent font-smallest indented-line shorter-tail headchar-lower tailchar-period
I-Reference	translation models. Journal of Machine Crammer, Koby, Alex Kulesza, and Mark	page=47 xpos=0 ypos=8 single-column left-indent right-over font-smallest aligned-line longer-tail headchar-lower
I-Reference	Learning Research, 13:1159–1187. Dredze. 2009. Adaptive regularization of	page=47 xpos=0 ypos=8 single-column left-indent right-over font-smallest aligned-line longer-tail year headchar-capital
B-Reference	Chiang, David, Kevin Knight, and Wei Wang. weight vectors. In Proceedings of NIPS,	page=47 xpos=0 ypos=9 single-column right-over font-smallest hanged-line shorter-tail headchar-capital tailchar-comma
I-Reference	2009. 11,001 new features for statistical pages 414–422, Vancouver.	page=47 xpos=0 ypos=9 single-column centered left-indent right-indent font-smallest indented-line shorter-tail year tailchar-period above-blank-line above-double-space above-line-space
Page	48	page=47 xpos=0 ypos=9 single-column right-indent hanged-line shorter-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Neubig and Watanabe	page=48 xpos=0 ypos=0 left-column right-indent font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
B-Reference	Crammer, Koby and Yoram Singer. 2003.	page=48 xpos=0 ypos=0 left-column right-over font-smallest aligned-line longer-tail line-blank-line line-double-space line-space year headchar-capital tailchar-period
I-Reference	Ultraconservative online algorithms for	page=48 xpos=0 ypos=0 left-column left-indent right-over font-smallest indented-line longer-tail headchar-capital
I-Reference	multiclass problems. Journal of Machine	page=48 xpos=0 ypos=0 left-column left-indent right-over font-smallest aligned-line headchar-lower
I-Reference	Learning Research, 3:951–991.	page=48 xpos=0 ypos=1 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Cui, Lei, Xilun Chen, Dongdong Zhang,	page=48 xpos=0 ypos=1 left-column full-justified font-smallest hanged-line longer-tail headchar-capital tailchar-comma
I-Reference	Shujie Liu, Mu Li, and Ming Zhou. 2013.	page=48 xpos=0 ypos=1 left-column left-indent right-over font-smallest indented-line longer-tail year headchar-capital tailchar-period
I-Reference	Multi-domain adaptation for SMT using	page=48 xpos=0 ypos=1 left-column left-indent right-over font-smallest aligned-line headchar-capital
I-Reference	multi-task learning. In Proceedings of	page=48 xpos=0 ypos=1 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	EMNLP, pages 1055–1065, Seattle, WA.	page=48 xpos=0 ypos=1 left-column left-indent right-over font-smallest aligned-line longer-tail headchar-capital tailchar-period
B-Reference	Dean, Jeffrey and Sanjay Ghemawat. 2008.	page=48 xpos=0 ypos=1 left-column right-over font-smallest hanged-line longer-tail year headchar-capital tailchar-period
I-Reference	Mapreduce: Simplified data processing on	page=48 xpos=0 ypos=2 left-column left-indent right-over font-smallest indented-line longer-tail headchar-capital
I-Reference	large clusters. Communications of the ACM,	page=48 xpos=0 ypos=2 left-column left-indent right-over font-smallest aligned-line headchar-lower tailchar-comma
I-Reference	51(1):107–113.	page=48 xpos=0 ypos=2 left-column left-indent right-indent font-smallest aligned-line shorter-tail tailchar-period
B-Reference	DeNero, John, David Chiang, and Kevin	page=48 xpos=0 ypos=2 left-column right-over font-smallest hanged-line longer-tail headchar-capital
I-Reference	Knight. 2009. Fast consensus decoding	page=48 xpos=0 ypos=2 left-column left-indent right-over font-smallest indented-line year headchar-capital
I-Reference	over translation forests. In Proceedings of	page=48 xpos=0 ypos=2 left-column left-indent right-over font-smallest aligned-line longer-tail headchar-lower
I-Reference	ACL/IJCNLP, pages 567–575, Singapore.	page=48 xpos=0 ypos=2 left-column left-indent right-over font-smallest aligned-line headchar-capital tailchar-period
B-Reference	Denkowski, Michael, Chris Dyer, and	page=48 xpos=0 ypos=3 left-column right-indent font-smallest hanged-line shorter-tail headchar-capital
I-Reference	Alon Lavie. 2014. Learning from	page=48 xpos=0 ypos=3 left-column left-indent right-indent font-smallest indented-line shorter-tail year headchar-capital
I-Reference	post-editing: Online model adaptation for	page=48 xpos=0 ypos=3 left-column left-indent right-over font-smallest aligned-line longer-tail headchar-lower
I-Reference	statistical machine translation. In	page=48 xpos=0 ypos=3 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	Proceedings of EACL, pages 395–404,	page=48 xpos=0 ypos=3 left-column centered left-indent right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-comma
I-Reference	Gothenburg.	page=48 xpos=0 ypos=3 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Dreyer, Markus and Yuanzhe Dong. 2015.	page=48 xpos=0 ypos=3 left-column right-over font-smallest hanged-line longer-tail year headchar-capital tailchar-period
I-Reference	APRO: All-pairs ranking optimization for	page=48 xpos=0 ypos=4 left-column left-indent right-over font-smallest indented-line longer-tail headchar-capital
I-Reference	MT tuning. In Proceedings of NAACL,	page=48 xpos=0 ypos=4 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-comma
I-Reference	pages 1018–1023, Denver, CO.	page=48 xpos=0 ypos=4 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Reference	Dreyer, Markus and Daniel Marcu. 2012.	page=48 xpos=0 ypos=4 left-column right-over font-smallest hanged-line longer-tail year headchar-capital tailchar-period
I-Reference	Hyter: Meaning-equivalent semantics for	page=48 xpos=0 ypos=4 left-column left-indent right-over font-smallest indented-line longer-tail headchar-capital
I-Reference	translation evaluation. In Proceedings of	page=48 xpos=0 ypos=4 left-column left-indent right-over font-smallest aligned-line shorter-tail headchar-lower
I-Reference	HLT/NAACL, pages 162–171, Montréal.	page=48 xpos=0 ypos=5 left-column left-indent right-over font-smallest aligned-line headchar-capital tailchar-period
B-Reference	Duan, Nan, Mu Li, Tong Xiao, and Ming	page=48 xpos=0 ypos=5 left-column right-over font-smallest hanged-line headchar-capital
I-Reference	Zhou. 2009. The Feature Subspace method	page=48 xpos=0 ypos=5 left-column left-indent right-over font-smallest indented-line longer-tail year headchar-capital
I-Reference	for SMT system combination. In	page=48 xpos=0 ypos=5 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	Proceedings of EMNLP, pages 1096–1104,	page=48 xpos=0 ypos=5 left-column left-indent right-over font-smallest aligned-line longer-tail headchar-capital tailchar-comma
I-Reference	Singapore.	page=48 xpos=0 ypos=5 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Duchi, John, Elad Hazan, and Yoram Singer.	page=48 xpos=0 ypos=5 left-column right-over font-smallest hanged-line longer-tail headchar-capital tailchar-period
I-Reference	2011. Adaptive subgradient methods	page=48 xpos=0 ypos=6 left-column left-indent right-indent font-smallest indented-line shorter-tail year
I-Reference	for online learning and stochastic	page=48 xpos=0 ypos=6 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	optimization. Journal of Machine Learning	page=48 xpos=0 ypos=6 left-column left-indent right-over font-smallest aligned-line longer-tail headchar-lower
I-Reference	Research, 12:2121–2159.	page=48 xpos=0 ypos=6 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Duchi, John and Yoram Singer. 2009. Efficient	page=48 xpos=0 ypos=6 left-column right-over font-smallest hanged-line longer-tail year headchar-capital
I-Reference	online and batch learning using forward	page=48 xpos=0 ypos=6 left-column left-indent right-over font-smallest indented-line shorter-tail headchar-lower
I-Reference	backward splitting. Journal of Machine	page=48 xpos=0 ypos=6 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	Learning Research, 10:2899–2934.	page=48 xpos=0 ypos=7 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Duh, Kevin and Katrin Kirchhoff. 2008.	page=48 xpos=0 ypos=7 left-column right-indent font-smallest hanged-line longer-tail year headchar-capital tailchar-period
I-Reference	Beyond log-linear models: Boosted	page=48 xpos=0 ypos=7 left-column left-indent right-indent font-smallest indented-line shorter-tail headchar-capital
I-Reference	minimum error rate training for n-best	page=48 xpos=0 ypos=7 left-column left-indent right-over font-smallest aligned-line longer-tail headchar-lower
I-Reference	re-ranking. In Proceedings of ACL/HLT:	page=48 xpos=0 ypos=7 left-column left-indent font-smallest aligned-line headchar-lower tailchar-colon
I-Reference	Short Papers, pages 37–40, Columbus, OH.	page=48 xpos=0 ypos=7 left-column left-indent right-over font-smallest aligned-line longer-tail headchar-capital tailchar-period
B-Reference	Duh, Kevin, Katsuhito Sudoh, Xianchao Wu,	page=48 xpos=0 ypos=8 left-column right-over font-smallest hanged-line headchar-capital tailchar-comma
I-Reference	Hajime Tsukada, and Masaaki Nagata.	page=48 xpos=0 ypos=8 left-column left-indent right-over font-smallest indented-line shorter-tail headchar-capital tailchar-period
I-Reference	2012. Learning to translate with multiple	page=48 xpos=0 ypos=8 left-column left-indent right-over font-smallest aligned-line longer-tail year
I-Reference	objectives. In Proceedings of ACL,	page=48 xpos=0 ypos=8 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-comma
I-Reference	pages 1–10, Jeju Island.	page=48 xpos=0 ypos=8 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Reference	Dyer, Chris. 2010a. A Formal Model of	page=48 xpos=0 ypos=8 left-column right-indent font-smallest hanged-line longer-tail year headchar-capital
I-Reference	Ambiguity and its Applications in Machine	page=48 xpos=0 ypos=8 left-column left-indent right-over font-smallest indented-line longer-tail headchar-capital
I-Reference	Translation. Ph.D. thesis, University of	page=48 xpos=0 ypos=9 left-column left-indent font-smallest aligned-line shorter-tail headchar-capital
I-Reference	Maryland.	page=48 xpos=0 ypos=9 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period column-bottom
I-Reference	Optimization for Statistical Machine Translation	page=48 xpos=5 ypos=0 right-column left-over font-smallest column-top headchar-capital above-blank-line above-double-space above-line-space
B-Reference	Dyer, Chris. 2010b. Two monolingual parses	page=48 xpos=5 ypos=0 right-column left-over right-indent font-smallest indented-line shorter-tail line-blank-line line-double-space line-space year headchar-capital
I-Reference	are better than one (synchronous parse). In	page=48 xpos=5 ypos=0 right-column centered right-indent font-smallest indented-line longer-tail headchar-lower
I-Reference	Proceedings of HLT/NAACL, pages 263–266,	page=48 xpos=5 ypos=0 right-column right-indent font-smallest aligned-line headchar-capital tailchar-comma
I-Reference	Los Angeles, CA.	page=48 xpos=5 ypos=1 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Dyer, Chris, Hendra Setiawan, Yuval Marton,	page=48 xpos=5 ypos=1 right-column left-over font-smallest hanged-line longer-tail headchar-capital tailchar-comma
I-Reference	and Philip Resnik. 2009. The University of	page=48 xpos=5 ypos=1 right-column right-indent font-smallest indented-line shorter-tail year headchar-lower
I-Reference	Maryland statistical machine translation	page=48 xpos=5 ypos=1 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital
I-Reference	system for the Fourth Workshop on	page=48 xpos=5 ypos=1 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	Machine Translation. In Proceedings of	page=48 xpos=5 ypos=1 right-column right-indent font-smallest aligned-line longer-tail headchar-capital
I-Reference	WMT, pages 145–149, Athens.	page=48 xpos=5 ypos=1 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Eidelman, Vladimir. 2012. Optimization	page=48 xpos=5 ypos=2 right-column left-over right-indent font-smallest hanged-line longer-tail year headchar-capital
I-Reference	strategies for online large-margin learning	page=48 xpos=5 ypos=2 right-column right-indent font-smallest indented-line longer-tail headchar-lower
I-Reference	in machine translation. In Proceedings of the	page=48 xpos=5 ypos=2 right-column centered right-indent font-smallest aligned-line headchar-lower
I-Reference	Seventh Workshop on Statistical Machine	page=48 xpos=5 ypos=2 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital
I-Reference	Translation, pages 477–486, Montréal.	page=48 xpos=5 ypos=2 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Eidelman, Vladimir, Yuval Marton, and	page=48 xpos=5 ypos=2 right-column left-over right-indent font-smallest hanged-line headchar-capital
I-Reference	Philip Resnik. 2013. Online relative margin	page=48 xpos=5 ypos=2 right-column full-justified font-smallest indented-line longer-tail year headchar-capital
I-Reference	maximization for statistical machine	page=48 xpos=5 ypos=3 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	translation. In Proceedings of ACL,	page=48 xpos=5 ypos=3 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-comma
I-Reference	pages 1116–1126, Sofia.	page=48 xpos=5 ypos=3 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Reference	Eisner, Jason. 2002. Parameter estimation	page=48 xpos=5 ypos=3 right-column left-over right-indent font-smallest hanged-line longer-tail year headchar-capital
I-Reference	for probabilistic finite-state transducers.	page=48 xpos=5 ypos=3 right-column right-indent font-smallest indented-line longer-tail headchar-lower tailchar-period
I-Reference	In Proceedings of ACL, pages 1–8,	page=48 xpos=5 ypos=3 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-comma
I-Reference	Philadelphia, PA.	page=48 xpos=5 ypos=3 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Flanigan, Jeffrey, Chris Dyer, and Jaime	page=48 xpos=5 ypos=4 right-column left-over right-indent font-smallest hanged-line longer-tail headchar-capital
I-Reference	Carbonell. 2013. Large-scale discriminative	page=48 xpos=5 ypos=4 right-column full-justified font-smallest indented-line longer-tail year headchar-capital
I-Reference	training for statistical machine translation	page=48 xpos=5 ypos=4 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	using held-out line search. In Proceedings of	page=48 xpos=5 ypos=4 right-column centered right-indent font-smallest aligned-line longer-tail headchar-lower
I-Reference	HLT/NAACL, pages 248–258, Atlanta, GA.	page=48 xpos=5 ypos=4 right-column right-indent font-smallest aligned-line headchar-capital tailchar-period
B-Reference	Foster, George and Roland Kuhn. 2009.	page=48 xpos=5 ypos=4 right-column left-over right-indent font-smallest hanged-line shorter-tail year headchar-capital tailchar-period
B-Reference	Stabilizing minimum error rate training. In	page=48 xpos=5 ypos=5 right-column full-justified font-smallest indented-line longer-tail headchar-capital
I-Reference	Proceedings of WMT, pages 242–249,	page=48 xpos=5 ypos=5 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-comma
I-Reference	Athens.	page=48 xpos=5 ypos=5 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Freund, Yoav, Raj Iyer, Robert E. Schapire,	page=48 xpos=5 ypos=5 right-column left-over right-indent font-smallest hanged-line longer-tail headchar-capital tailchar-comma
I-Reference	and Yoram Singer. 2003. An efficient	page=48 xpos=5 ypos=5 right-column right-indent font-smallest indented-line shorter-tail year headchar-lower
I-Reference	boosting algorithm for combining	page=48 xpos=5 ypos=5 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	preferences. Journal of Machine Learning	page=48 xpos=5 ypos=5 right-column right-indent font-smallest aligned-line longer-tail headchar-lower
I-Reference	Research, 4:933–969.	page=48 xpos=5 ypos=6 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Galley, Michel and Christopher D. Manning.	page=48 xpos=5 ypos=6 right-column left-over right-indent font-smallest hanged-line longer-tail headchar-capital tailchar-period
I-Reference	2008. A simple and effective hierarchical	page=48 xpos=5 ypos=6 right-column right-indent font-smallest indented-line shorter-tail year
I-Reference	phrase reordering model. In Proceedings of	page=48 xpos=5 ypos=6 right-column right-indent font-smallest aligned-line longer-tail headchar-lower
I-Reference	EMNLP, pages 848–856, Honolulu, HI.	page=48 xpos=5 ypos=6 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Galley, Michel and Chris Quirk. 2011.	page=48 xpos=5 ypos=6 right-column left-over right-indent font-smallest hanged-line shorter-tail year headchar-capital tailchar-period
I-Reference	Optimal search for minimum error rate	page=48 xpos=5 ypos=6 right-column right-indent font-smallest indented-line longer-tail headchar-capital
I-Reference	training. In Proceedings of EMNLP,	page=48 xpos=5 ypos=7 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-comma
I-Reference	pages 38–49, Edinburgh.	page=48 xpos=5 ypos=7 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Reference	Galley, Michel, Chris Quirk, Colin Cherry,	page=48 xpos=5 ypos=7 right-column left-over right-indent font-smallest hanged-line longer-tail headchar-capital tailchar-comma
I-Reference	and Kristina Toutanova. 2013. Regularized	page=48 xpos=5 ypos=7 right-column right-indent font-smallest indented-line longer-tail year headchar-lower
I-Reference	minimum error rate training. In Proceedings	page=48 xpos=5 ypos=7 right-column full-justified font-smallest aligned-line headchar-lower
I-Reference	of EMNLP, pages 1948–1959, Seattle, WA.	page=48 xpos=5 ypos=7 right-column right-indent font-smallest aligned-line shorter-tail year headchar-lower tailchar-period
B-Reference	Gao, Jianfeng and Xiaodong He. 2013.	page=48 xpos=5 ypos=8 right-column left-over right-indent font-smallest hanged-line shorter-tail year headchar-capital tailchar-period
I-Reference	Training MRF-based phrase translation	page=48 xpos=5 ypos=8 right-column right-indent font-smallest indented-line longer-tail headchar-capital
I-Reference	models using gradient ascent. In	page=48 xpos=5 ypos=8 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	Proceedings of HLT/NAACL, pages 450–459,	page=48 xpos=5 ypos=8 right-column right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-comma
I-Reference	Atlanta, GA.	page=48 xpos=5 ypos=8 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Gesmundo, Andrea and James Henderson.	page=48 xpos=5 ypos=8 right-column left-over right-indent font-smallest hanged-line longer-tail headchar-capital tailchar-period
I-Reference	2011. Heuristic search for non-bottom-up	page=48 xpos=5 ypos=8 right-column right-indent font-smallest indented-line year
I-Reference	tree structure prediction. In Proceedings of	page=48 xpos=5 ypos=9 right-column right-indent font-smallest aligned-line headchar-lower
I-Reference	EMNLP, pages 899–908, Edinburgh.	page=48 xpos=5 ypos=9 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period above-blank-line above-double-space above-line-space
Page	49	page=48 xpos=9 ypos=9 right-column left-indent indented-line longer-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Computational Linguistics	page=49 xpos=0 ypos=0 left-column right-indent font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
B-Reference	Gimpel, Kevin, Dhruv Batra, Chris Dyer,	page=49 xpos=0 ypos=0 left-column right-over font-smallest aligned-line longer-tail line-blank-line line-double-space line-space headchar-capital tailchar-comma
I-Reference	and Gregory Shakhnarovich. 2013.	page=49 xpos=0 ypos=0 left-column left-indent right-indent font-smallest indented-line shorter-tail year headchar-lower tailchar-period
I-Reference	A systematic exploration of diversity	page=49 xpos=0 ypos=0 left-column left-indent right-indent font-smallest aligned-line longer-tail headchar-capital
I-Reference	in machine translation. In	page=49 xpos=0 ypos=1 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	Proceedings of EMNLP,	page=49 xpos=0 ypos=1 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-comma
I-Reference	pages 1100–1111, Seattle, WA.	page=49 xpos=0 ypos=1 left-column left-indent right-indent font-smallest aligned-line longer-tail headchar-lower tailchar-period
B-Reference	Gimpel, Kevin and Noah A. Smith. 2009.	page=49 xpos=0 ypos=1 left-column right-over font-smallest hanged-line longer-tail year headchar-capital tailchar-period
I-Reference	Feature-rich translation by	page=49 xpos=0 ypos=1 left-column left-indent right-indent font-smallest indented-line shorter-tail headchar-capital
I-Reference	quasi-synchronous lattice parsing. In	page=49 xpos=0 ypos=1 left-column left-indent right-indent font-smallest aligned-line longer-tail headchar-lower
I-Reference	Proceedings of EMNLP, pages 219–228,	page=49 xpos=0 ypos=1 left-column left-indent right-indent font-smallest aligned-line headchar-capital tailchar-comma
I-Reference	Singapore.	page=49 xpos=0 ypos=2 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Gimpel, Kevin and Noah A. Smith.	page=49 xpos=0 ypos=2 left-column right-indent font-smallest hanged-line longer-tail headchar-capital tailchar-period
I-Reference	2012. Structured ramp loss minimization	page=49 xpos=0 ypos=2 left-column left-indent right-over font-smallest indented-line longer-tail year
I-Reference	for machine translation. In Proceedings of	page=49 xpos=0 ypos=2 left-column left-indent right-over font-smallest aligned-line headchar-lower
I-Reference	HLT/NAACL, pages 221–231, Montréal.	page=49 xpos=0 ypos=2 left-column left-indent right-over font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Graham, Yvette, Timothy Baldwin, Alistair	page=49 xpos=0 ypos=2 left-column right-over font-smallest hanged-line longer-tail headchar-capital
I-Reference	Moffat, and Justin Zobel. 2014. Is machine	page=49 xpos=0 ypos=2 left-column left-indent right-over font-smallest indented-line longer-tail year headchar-capital
I-Reference	translation getting better over time? In	page=49 xpos=0 ypos=3 left-column left-indent right-over font-smallest aligned-line shorter-tail headchar-lower
I-Reference	Proceedings of EACL, pages 443–451,	page=49 xpos=0 ypos=3 left-column centered left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-comma
I-Reference	Gothenburg.	page=49 xpos=0 ypos=3 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Green, Spence, Daniel Cer, and Christopher	page=49 xpos=0 ypos=3 left-column right-over font-smallest hanged-line longer-tail headchar-capital
I-Reference	Manning. 2014. An empirical comparison	page=49 xpos=0 ypos=3 left-column left-indent right-over font-smallest indented-line year headchar-capital
I-Reference	of features and tuning for phrase-based	page=49 xpos=0 ypos=3 left-column left-indent right-over font-smallest aligned-line shorter-tail headchar-lower
I-Reference	machine translation. In Proceedings of	page=49 xpos=0 ypos=3 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	WMT, pages 466–476, Baltimore, MD.	page=49 xpos=0 ypos=4 left-column left-indent right-indent font-smallest aligned-line headchar-capital tailchar-period
B-Reference	Green, Spence, Sida Wang, Daniel Cer, and	page=49 xpos=0 ypos=4 left-column right-over font-smallest hanged-line longer-tail headchar-capital
I-Reference	Christopher D. Manning. 2013. Fast and	page=49 xpos=0 ypos=4 left-column left-indent right-over font-smallest indented-line year headchar-capital
I-Reference	adaptive online training of feature-rich	page=49 xpos=0 ypos=4 left-column left-indent right-over font-smallest aligned-line shorter-tail headchar-lower
I-Reference	translation models. In Proceedings of ACL,	page=49 xpos=0 ypos=4 left-column left-indent right-over font-smallest aligned-line longer-tail headchar-lower tailchar-comma
I-Reference	pages 311–321, Sofia.	page=49 xpos=0 ypos=4 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Reference	Green, Spence, Sida I. Wang, Jason Chuang,	page=49 xpos=0 ypos=5 left-column right-over font-smallest hanged-line longer-tail headchar-capital tailchar-comma
I-Reference	Jeffrey Heer, Sebastian Schuster, and	page=49 xpos=0 ypos=5 left-column left-indent right-indent font-smallest indented-line shorter-tail headchar-capital
I-Reference	Christopher D. Manning. 2014. Human	page=49 xpos=0 ypos=5 left-column left-indent right-over font-smallest aligned-line longer-tail year headchar-capital
I-Reference	effort and machine learnability in	page=49 xpos=0 ypos=5 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	computer aided translation. In	page=49 xpos=0 ypos=5 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	Proceedings of EMNLP, pages 1225–1236,	page=49 xpos=0 ypos=5 left-column left-indent right-over font-smallest aligned-line longer-tail headchar-capital tailchar-comma
I-Reference	Doha.	page=49 xpos=0 ypos=5 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Haddow, Barry. 2013. Applying pairwise	page=49 xpos=0 ypos=6 left-column right-over font-smallest hanged-line longer-tail year headchar-capital
I-Reference	ranked optimization to improve the	page=49 xpos=0 ypos=6 left-column centered left-indent right-indent font-smallest indented-line shorter-tail headchar-lower
I-Reference	interpolation of translation models. In	page=49 xpos=0 ypos=6 left-column left-indent font-smallest aligned-line longer-tail headchar-lower
I-Reference	Proceedings of HLT/NAACL, pages 342–347,	page=49 xpos=0 ypos=6 left-column left-indent right-over font-smallest aligned-line longer-tail headchar-capital tailchar-comma
I-Reference	Atlanta, GA.	page=49 xpos=0 ypos=6 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Haddow, Barry, Abhishek Arun, and Philipp	page=49 xpos=0 ypos=6 left-column right-over font-smallest hanged-line longer-tail headchar-capital
I-Reference	Koehn. 2011. SampleRank training for	page=49 xpos=0 ypos=6 left-column left-indent font-smallest indented-line shorter-tail year headchar-capital
I-Reference	phrase-based machine translation. In	page=49 xpos=0 ypos=7 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	Proceedings of WMT, pages 261–271,	page=49 xpos=0 ypos=7 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-comma
I-Reference	Edinburgh.	page=49 xpos=0 ypos=7 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Hasan, Saša, Richard Zens, and Hermann	page=49 xpos=0 ypos=7 left-column right-over font-smallest hanged-line longer-tail headchar-capital
I-Reference	Ney. 2007. Are very large N-best lists	page=49 xpos=0 ypos=7 left-column left-indent right-indent font-smallest indented-line shorter-tail year headchar-capital
I-Reference	useful for SMT? In Proceedings of	page=49 xpos=0 ypos=7 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	HLT/NAACL, pages 57–60, Rochester, NY.	page=49 xpos=0 ypos=8 left-column left-indent right-over font-smallest aligned-line longer-tail headchar-capital tailchar-period
B-Reference	Hayashi, Katsuhiko, Taro Watanabe, Hajime	page=49 xpos=0 ypos=8 left-column right-over font-smallest hanged-line headchar-capital
I-Reference	Tsukada, and Hideki Isozaki. 2009.	page=49 xpos=0 ypos=8 left-column left-indent right-indent font-smallest indented-line shorter-tail year headchar-capital tailchar-period
I-Reference	Structural support vector machines for	page=49 xpos=0 ypos=8 left-column left-indent right-over font-smallest aligned-line longer-tail headchar-capital
I-Reference	log-linear approach in statistical machine	page=49 xpos=0 ypos=8 left-column left-indent right-over font-smallest aligned-line longer-tail headchar-lower
I-Reference	translation. In Proceedings of IWSLT,	page=49 xpos=0 ypos=8 left-column centered left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-comma
I-Reference	pages 144–151, Tokyo.	page=49 xpos=0 ypos=8 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Reference	He, Xiaodong and Li Deng. 2012. Maximum	page=49 xpos=0 ypos=9 left-column right-over font-smallest hanged-line longer-tail year headchar-capital
I-Reference	expected BLEU training of phrase and	page=49 xpos=0 ypos=9 left-column left-indent font-smallest indented-line shorter-tail headchar-lower above-blank-line above-double-space above-line-space
Page	50	page=49 xpos=0 ypos=9 left-column right-indent hanged-line shorter-tail line-blank-line line-double-space line-space numeric-only column-bottom
B-Header	Volume 42, Number 1	page=49 xpos=7 ypos=0 right-column left-indent right-over font-smallest column-top headchar-capital above-blank-line above-double-space above-line-space
I-Reference	lexicon translation models. In Proceedings	page=49 xpos=5 ypos=0 right-column right-over font-smallest hanged-line shorter-tail line-blank-line line-double-space line-space headchar-lower
I-Reference	of ACL, pages 292–301, Jeju Island.	page=49 xpos=5 ypos=0 right-column right-over font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Reference	He, Yifan and Andy Way. 2009. Improving	page=49 xpos=5 ypos=0 right-column left-over right-over font-smallest hanged-line longer-tail year headchar-capital
I-Reference	the objective function in minimum error	page=49 xpos=5 ypos=1 right-column right-over font-smallest indented-line headchar-lower
I-Reference	rate training. In Proceedings of MT Summit,	page=49 xpos=5 ypos=1 right-column right-over font-smallest aligned-line longer-tail headchar-lower tailchar-comma
I-Reference	pages 238–245, Ottawa.	page=49 xpos=5 ypos=1 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Reference	Herbrich, Ralf, Thore Graepel, and Klaus	page=49 xpos=5 ypos=1 right-column left-over right-over font-smallest hanged-line longer-tail headchar-capital
I-Reference	Obermayer. 1999. Support vector learning	page=49 xpos=5 ypos=1 right-column right-over font-smallest indented-line longer-tail year headchar-capital
I-Reference	for ordinal regression. In Proceedings of	page=49 xpos=5 ypos=1 right-column right-over font-smallest aligned-line shorter-tail headchar-lower
I-Reference	ICANN, pages 97–102, Edinburgh.	page=49 xpos=5 ypos=1 right-column right-over font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Hopkins, Mark and Jonathan May. 2011.	page=49 xpos=5 ypos=2 right-column left-over right-over font-smallest hanged-line longer-tail year headchar-capital tailchar-period
I-Reference	Tuning as ranking. In Proceedings of	page=49 xpos=5 ypos=2 right-column right-over font-smallest indented-line shorter-tail headchar-capital
I-Reference	EMNLP, pages 1352–1362, Edinburgh.	page=49 xpos=5 ypos=2 right-column right-over font-smallest aligned-line longer-tail headchar-capital tailchar-period
B-Reference	Hsieh, Cho-Jui, Kai-Wei Chang, Chih-Jen Lin,	page=49 xpos=5 ypos=2 right-column left-over right-over font-smallest hanged-line longer-tail headchar-capital tailchar-comma
I-Reference	S. Sathiya Keerthi, and S. Sundararajan.	page=49 xpos=5 ypos=2 right-column right-over font-smallest indented-line shorter-tail headchar-capital tailchar-period
I-Reference	2008. A dual coordinate descent method	page=49 xpos=5 ypos=2 right-column right-over font-smallest aligned-line year
I-Reference	for large-scale linear SVM. In Proceedings of	page=49 xpos=5 ypos=2 right-column right-over font-smallest aligned-line longer-tail headchar-lower
I-Reference	ICML, pages 408–415, Helsinki.	page=49 xpos=5 ypos=3 right-column full-justified font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Huang, Liang and David Chiang. 2007.	page=49 xpos=5 ypos=3 right-column left-over right-over font-smallest hanged-line longer-tail year headchar-capital tailchar-period
I-Reference	Forest rescoring: Faster decoding with	page=49 xpos=5 ypos=3 right-column right-over font-smallest indented-line longer-tail headchar-capital
I-Reference	integrated language models. In Proceedings	page=49 xpos=5 ypos=3 right-column right-over font-smallest aligned-line longer-tail headchar-lower
I-Reference	of ACL, pages 144–151, Prague.	page=49 xpos=5 ypos=3 right-column centered right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Reference	Huang, Liang, Suphan Fayong, and Yang	page=49 xpos=5 ypos=3 right-column left-over right-over font-smallest hanged-line longer-tail headchar-capital
I-Reference	Guo. 2012. Structured perceptron with	page=49 xpos=5 ypos=3 right-column right-over font-smallest indented-line year headchar-capital
I-Reference	inexact search. In Proceedings of	page=49 xpos=5 ypos=4 right-column centered right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	HLT/NAACL, pages 142–151, Montréal.	page=49 xpos=5 ypos=4 right-column right-over font-smallest aligned-line longer-tail headchar-capital tailchar-period
B-Reference	Huang, Liang, Kevin Knight, and Aravind	page=49 xpos=5 ypos=4 right-column left-over right-over font-smallest hanged-line longer-tail headchar-capital
I-Reference	Joshi. 2006. Statistical syntax-directed	page=49 xpos=5 ypos=4 right-column right-over font-smallest indented-line shorter-tail year headchar-capital
I-Reference	translation with extended domain of	page=49 xpos=5 ypos=4 right-column right-over font-smallest aligned-line headchar-lower
I-Reference	locality. In Proceedings of AMTA,	page=49 xpos=5 ypos=4 right-column right-over font-smallest aligned-line shorter-tail headchar-lower tailchar-comma
I-Reference	pages 66–73, Cambridge, MA.	page=49 xpos=5 ypos=5 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Reference	Joachims, Thorsten. 1998. Text Categorization	page=49 xpos=5 ypos=5 right-column left-over right-over font-smallest hanged-line longer-tail year headchar-capital
I-Reference	with Support Vector Machines: Learning with	page=49 xpos=5 ypos=5 right-column right-over font-smallest indented-line headchar-lower
I-Reference	Many Relevant Features. Springer,	page=49 xpos=5 ypos=5 right-column right-over font-smallest aligned-line shorter-tail headchar-capital tailchar-comma
I-Reference	New York.	page=49 xpos=5 ypos=5 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Klein, Dan and Christopher D. Manning.	page=49 xpos=5 ypos=5 right-column left-over right-over font-smallest hanged-line longer-tail headchar-capital tailchar-period
I-Reference	2004. Parsing and hypergraphs. In Harry	page=49 xpos=5 ypos=5 right-column right-over font-smallest indented-line longer-tail year
I-Reference	Bunt, John Carroll, and Giorgio Satta,	page=49 xpos=5 ypos=6 right-column right-over font-smallest aligned-line shorter-tail headchar-capital tailchar-comma
I-Reference	editors, New Developments in Parsing	page=49 xpos=5 ypos=6 right-column right-over font-smallest aligned-line shorter-tail headchar-lower
I-Reference	Technology. Kluwer Academic Publishers,	page=49 xpos=5 ypos=6 right-column right-over font-smallest aligned-line longer-tail headchar-capital tailchar-comma
I-Reference	Norwell, MA, pages 351–372.	page=49 xpos=5 ypos=6 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Koehn, Philipp. 2010. Statistical Machine	page=49 xpos=5 ypos=6 right-column left-over right-over font-smallest hanged-line longer-tail year headchar-capital
I-Reference	Translation. Cambridge University Press,	page=49 xpos=5 ypos=6 right-column right-over font-smallest indented-line longer-tail headchar-capital tailchar-comma
I-Reference	New York, NY.	page=49 xpos=5 ypos=6 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Koehn, Philipp, Hieu Hoang, Alexandra	page=49 xpos=5 ypos=7 right-column left-over right-over font-smallest hanged-line longer-tail headchar-capital
I-Reference	Birch, Chris Callison-Burch, Marcello	page=49 xpos=5 ypos=7 right-column right-over font-smallest indented-line headchar-capital
I-Reference	Federico, Nicola Bertoldi, Brooke Cowan,	page=49 xpos=5 ypos=7 right-column right-over font-smallest aligned-line longer-tail headchar-capital tailchar-comma
I-Reference	Wade Shen, Christine Moran, Richard	page=49 xpos=5 ypos=7 right-column right-over font-smallest aligned-line shorter-tail headchar-capital
I-Reference	Zens, Chris Dyer, Ondrej Bojar, Alexandra	page=49 xpos=5 ypos=7 right-column right-over font-smallest aligned-line longer-tail headchar-capital
I-Reference	Constantin, and Evan Herbst. 2007. Moses:	page=49 xpos=5 ypos=7 right-column right-over font-smallest aligned-line year headchar-capital tailchar-colon
I-Reference	Open source toolkit for statistical machine	page=49 xpos=5 ypos=8 right-column right-over font-smallest aligned-line headchar-capital
I-Reference	translation. In Proceedings of ACL: Demo	page=49 xpos=5 ypos=8 right-column right-over font-smallest aligned-line shorter-tail headchar-lower
I-Reference	and Poster Sessions, pages 177–180, Prague.	page=49 xpos=5 ypos=8 right-column right-over font-smallest aligned-line longer-tail headchar-lower tailchar-period
B-Reference	Koehn, Philipp, Franz Josef Och, and Daniel	page=49 xpos=5 ypos=8 right-column left-over right-over font-smallest hanged-line headchar-capital
I-Reference	Marcu. 2003. Statistical phrase-based	page=49 xpos=5 ypos=8 right-column right-over font-smallest indented-line shorter-tail year headchar-capital
I-Reference	translation. In Proceedings of HLT/NAACL,	page=49 xpos=5 ypos=8 right-column right-over font-smallest aligned-line longer-tail headchar-lower tailchar-comma
I-Reference	pages 48–54, Edmonton.	page=49 xpos=5 ypos=8 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Reference	Koehn, Philipp and Josh Schroeder. 2007.	page=49 xpos=5 ypos=9 right-column left-over right-over font-smallest hanged-line longer-tail year headchar-capital tailchar-period
I-Reference	Experiments in domain adaptation for	page=49 xpos=5 ypos=9 right-column right-over font-smallest indented-line headchar-capital page-bottom
B-Header	Neubig and Watanabe Optimization for Statistical Machine Translation	page=50 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Reference	statistical machine translation. In Liu, Chang, Daniel Dahlmeier, and	page=50 xpos=0 ypos=0 single-column left-indent right-indent font-smallest indented-line shorter-tail line-blank-line line-double-space line-space headchar-lower column-bottom
I-Reference	Proceedings of WMT, pages 224–227,	page=50 xpos=0 ypos=0 left-column left-indent right-indent font-smallest column-top headchar-capital tailchar-comma
I-Reference	Prague.	page=50 xpos=0 ypos=0 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Kumar, Shankar, Wolfgang Macherey, Chris	page=50 xpos=0 ypos=1 left-column right-indent font-smallest hanged-line longer-tail headchar-capital
I-Reference	Dyer, and Franz Och. 2009. Efficient	page=50 xpos=0 ypos=1 left-column left-indent right-indent font-smallest indented-line shorter-tail year headchar-capital column-bottom
I-Reference	minimum error rate training and	page=50 xpos=0 ypos=1 single-column left-indent right-indent font-smallest column-top headchar-lower column-bottom
B-Reference	Hwee Tou Ng. 2011. Better evaluation	page=50 xpos=5 ypos=0 right-column right-indent font-smallest column-top year headchar-capital
I-Reference	metrics lead to better machine translation.	page=50 xpos=5 ypos=0 right-column right-indent font-smallest aligned-line longer-tail headchar-lower tailchar-period
I-Reference	In Proceedings of EMNLP, pages 375–384,	page=50 xpos=5 ypos=1 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-comma
I-Reference	Edinburgh.	page=50 xpos=5 ypos=1 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period column-bottom
I-Reference	Liu, Dong C. and Jorge Nocedal. 1989. On	page=50 xpos=5 ypos=1 single-column left-indent right-indent font-smallest column-top year headchar-capital column-bottom
I-Reference	minimum Bayes-risk decoding for	page=50 xpos=0 ypos=1 left-column left-indent right-indent font-smallest column-top headchar-lower
I-Reference	translation hypergraphs and lattices. In	page=50 xpos=0 ypos=1 left-column left-indent right-indent font-smallest aligned-line longer-tail headchar-lower
I-Reference	Proceedings of ACL/IJCNLP, pages 163–171,	page=50 xpos=0 ypos=1 left-column left-indent right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-comma column-bottom
I-Reference	Singapore.	page=50 xpos=0 ypos=1 single-column left-indent right-indent font-smallest column-top headchar-capital tailchar-period column-bottom
I-Reference	the limited memory BFGS method for	page=50 xpos=5 ypos=1 right-column right-indent font-smallest column-top headchar-lower
I-Reference	large scale optimization. Mathematical	page=50 xpos=5 ypos=1 right-column right-indent font-smallest aligned-line headchar-lower
I-Reference	Programming, 45(3):503–528.	page=50 xpos=5 ypos=1 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period column-bottom
I-Reference	Liu, Lemao, Hailong Cao, Taro Watanabe,	page=50 xpos=5 ypos=1 single-column left-indent right-indent font-smallest column-top headchar-capital tailchar-comma column-bottom
I-Reference	Lagarda, Antonio and Francisco	page=50 xpos=0 ypos=2 left-column right-indent font-smallest column-top headchar-capital
B-Reference	Casacuberta. 2008. Applying boosting to	page=50 xpos=0 ypos=2 left-column left-indent right-indent font-smallest indented-line longer-tail year headchar-capital
I-Reference	statistical machine translation. In	page=50 xpos=0 ypos=2 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	Proceedings of EAMT, pages 88–96,	page=50 xpos=0 ypos=2 left-column left-indent right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-comma column-bottom
I-Reference	Tiejun Zhao, Mo Yu, and Conghui Zhu.	page=50 xpos=5 ypos=2 right-column right-indent font-smallest column-top headchar-capital tailchar-period
B-Reference	2012. Locally training the log-linear model	page=50 xpos=5 ypos=2 right-column right-indent font-smallest aligned-line longer-tail year
I-Reference	for SMT. In Proceedings of EMNLP/CoNLL,	page=50 xpos=5 ypos=2 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-comma
I-Reference	pages 402–411, Jeju Island.	page=50 xpos=5 ypos=2 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period column-bottom
I-Reference	Hamburg. Liu, Lemao and Liang Huang. 2014.	page=50 xpos=0 ypos=2 single-column left-indent right-indent font-smallest column-top year headchar-capital tailchar-period column-bottom
I-Reference	Leusch, Gregor, Evgeny Matusov, and	page=50 xpos=0 ypos=2 left-column right-indent font-smallest column-top headchar-capital
I-Reference	Hermann Ney. 2008. Complexity of	page=50 xpos=0 ypos=2 left-column left-indent right-indent font-smallest indented-line year headchar-capital
I-Reference	finding the BLEU-optimal hypothesis in a	page=50 xpos=0 ypos=3 left-column left-indent right-indent font-smallest aligned-line longer-tail headchar-lower column-bottom
I-Reference	confusion network. In Proceedings of	page=50 xpos=0 ypos=3 single-column left-indent right-indent font-smallest column-top headchar-lower column-bottom
B-Reference	Search-aware tuning for machine	page=50 xpos=5 ypos=2 right-column right-indent font-smallest column-top headchar-capital
I-Reference	translation. In Proceedings of EMNLP,	page=50 xpos=5 ypos=2 right-column right-indent font-smallest aligned-line longer-tail headchar-lower tailchar-comma
I-Reference	pages 1942–1952, Doha.	page=50 xpos=5 ypos=3 right-column right-indent font-smallest aligned-line shorter-tail year headchar-lower tailchar-period column-bottom
I-Reference	Liu, Lemao, Taro Watanabe, Eiichiro Sumita,	page=50 xpos=5 ypos=3 single-column left-indent right-indent font-smallest column-top headchar-capital tailchar-comma column-bottom
I-Reference	EMNLP, pages 839–847, Honolulu, HI.	page=50 xpos=0 ypos=3 left-column left-indent right-indent font-smallest column-top headchar-capital tailchar-period
B-Reference	Li, Mu, Yinggong Zhao, Dongdong Zhang,	page=50 xpos=0 ypos=3 left-column right-indent font-smallest hanged-line longer-tail headchar-capital tailchar-comma
I-Reference	and Ming Zhou. 2010. Adaptive	page=50 xpos=0 ypos=3 left-column left-indent right-indent font-smallest indented-line shorter-tail year headchar-lower
I-Reference	development data selection for log-linear	page=50 xpos=0 ypos=3 left-column left-indent right-indent font-smallest aligned-line longer-tail headchar-lower column-bottom
I-Reference	model in statistical machine translation. In	page=50 xpos=0 ypos=3 single-column left-indent right-indent font-smallest column-top headchar-lower column-bottom
I-Reference	and Tiejun Zhao. 2013. Additive neural	page=50 xpos=5 ypos=3 right-column right-indent font-smallest column-top year headchar-lower
I-Reference	networks for statistical machine	page=50 xpos=5 ypos=3 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	translation. In Proceedings of ACL,	page=50 xpos=5 ypos=3 right-column right-indent font-smallest aligned-line longer-tail headchar-lower tailchar-comma
I-Reference	pages 791–801, Sofia.	page=50 xpos=5 ypos=3 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period column-bottom
I-Reference	Liu, Lemao, Tiejun Zhao, Taro Watanabe,	page=50 xpos=5 ypos=3 single-column left-indent right-indent font-smallest column-top headchar-capital tailchar-comma column-bottom
I-Reference	Proceedings of COLING, pages 662–670,	page=50 xpos=0 ypos=4 left-column left-indent right-indent font-smallest column-top headchar-capital tailchar-comma
I-Reference	Beijing.	page=50 xpos=0 ypos=4 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Li, Zhifei and Jason Eisner. 2009. First- and	page=50 xpos=0 ypos=4 left-column right-indent font-smallest hanged-line longer-tail year headchar-capital
I-Reference	second-order expectation semirings with	page=50 xpos=0 ypos=4 left-column left-indent right-indent font-smallest indented-line headchar-lower
I-Reference	applications to minimum-risk training on	page=50 xpos=0 ypos=4 left-column left-indent right-indent font-smallest aligned-line headchar-lower column-bottom
I-Reference	translation forests. In Proceedings of	page=50 xpos=0 ypos=4 single-column left-indent right-indent font-smallest column-top headchar-lower column-bottom
B-Reference	Hailong Cao, and Conghui Zhu. 2012.	page=50 xpos=5 ypos=4 right-column right-indent font-smallest column-top year headchar-capital tailchar-period
I-Reference	Expected error minimization with	page=50 xpos=5 ypos=4 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital
I-Reference	ultraconservative update for SMT. In	page=50 xpos=5 ypos=4 right-column right-indent font-smallest aligned-line longer-tail headchar-lower
I-Reference	Proceedings of COLING: Posters,	page=50 xpos=5 ypos=4 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-comma
I-Reference	pages 723–732, Mumbai.	page=50 xpos=5 ypos=4 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period column-bottom
I-Reference	Liu, Lemao, Tiejun Zhao, Taro Watanabe, and	page=50 xpos=5 ypos=4 single-column left-indent font-smallest column-top headchar-capital column-bottom
I-Reference	EMNLP, pages 40–51, Singapore.	page=50 xpos=0 ypos=5 left-column left-indent right-indent font-smallest column-top headchar-capital tailchar-period
B-Reference	Li, Zhifei and Sanjeev Khudanpur. 2009.	page=50 xpos=0 ypos=5 left-column right-indent font-smallest hanged-line longer-tail year headchar-capital tailchar-period
I-Reference	Forest reranking for machine translation	page=50 xpos=0 ypos=5 left-column left-indent right-indent font-smallest indented-line longer-tail headchar-capital
I-Reference	with the perceptron algorithm. In J. Olive,	page=50 xpos=0 ypos=5 left-column left-indent right-indent font-smallest aligned-line longer-tail headchar-lower tailchar-comma column-bottom
I-Reference	C. Christianson, & J. McCary, editors,	page=50 xpos=0 ypos=5 single-column left-indent right-indent font-smallest column-top headchar-capital tailchar-comma column-bottom
B-Reference	Eiichiro Sumita. 2013. Tuning SMT with a	page=50 xpos=5 ypos=5 right-column right-indent font-smallest column-top year headchar-capital
I-Reference	large number of features via online feature	page=50 xpos=5 ypos=5 right-column right-indent font-smallest aligned-line longer-tail headchar-lower
I-Reference	grouping. In Proceedings of IJCNLP,	page=50 xpos=5 ypos=5 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-comma
I-Reference	pages 279–285, Nagoya.	page=50 xpos=5 ypos=5 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period column-bottom
I-Reference	Lo, Chi-kiu, Karteek Addanki, Markus Saers,	page=50 xpos=5 ypos=5 single-column left-indent right-indent font-smallest column-top headchar-capital tailchar-comma column-bottom
I-Reference	Handbook of Natural Language Processing	page=50 xpos=0 ypos=5 left-column left-indent right-indent font-smallest column-top headchar-capital
I-Reference	and Machine Translation. Springer,	page=50 xpos=0 ypos=5 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-comma
I-Reference	pages 226–236.	page=50 xpos=0 ypos=6 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Reference	Li, Zhifei, Ziyuan Wang, Jason Eisner,	page=50 xpos=0 ypos=6 left-column right-indent font-smallest hanged-line longer-tail headchar-capital tailchar-comma
I-Reference	Sanjeev Khudanpur, and Brian Roark.	page=50 xpos=0 ypos=6 left-column left-indent right-indent font-smallest indented-line longer-tail headchar-capital tailchar-period column-bottom
I-Reference	2011. Minimum imputed-risk:	page=50 xpos=0 ypos=6 single-column left-indent right-indent font-smallest column-top year tailchar-colon column-bottom
I-Reference	and Dekai Wu. 2013. Improving machine	page=50 xpos=5 ypos=5 right-column right-indent font-smallest column-top year headchar-lower
I-Reference	translation by training against an	page=50 xpos=5 ypos=5 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	automatic semantic frame based	page=50 xpos=5 ypos=6 right-column right-indent font-smallest aligned-line headchar-lower
I-Reference	evaluation metric. In Proceedings of ACL:	page=50 xpos=5 ypos=6 right-column right-indent font-smallest aligned-line longer-tail headchar-lower tailchar-colon
I-Reference	Short Papers, pages 375–381, Sofia.	page=50 xpos=5 ypos=6 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period column-bottom
I-Reference	Lopez, Adam. 2008. Statistical machine	page=50 xpos=5 ypos=6 single-column left-indent right-indent font-smallest column-top year headchar-capital column-bottom
I-Reference	Unsupervised discriminative training for	page=50 xpos=0 ypos=6 left-column left-indent right-indent font-smallest column-top headchar-capital
I-Reference	machine translation. In Proceedings of	page=50 xpos=0 ypos=6 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower column-bottom
I-Reference	translation. ACM Computing Surveys,	page=50 xpos=5 ypos=6 right-column right-indent font-smallest column-top headchar-lower tailchar-comma
I-Reference	40(3):1–49.	page=50 xpos=5 ypos=6 right-column right-indent font-smallest aligned-line shorter-tail tailchar-period column-bottom
I-Reference	EMNLP, pages 920–929, Edinburgh. Macherey, Wolfgang, Franz Och, Ignacio	page=50 xpos=0 ypos=6 single-column left-indent right-indent font-smallest column-top headchar-capital column-bottom
I-Reference	Liang, Huashen, Min Zhang, and Tiejun	page=50 xpos=0 ypos=7 left-column right-indent font-smallest column-top headchar-capital
B-Reference	Zhao. 2012. Forced decoding for minimum	page=50 xpos=0 ypos=7 left-column left-indent right-indent font-smallest indented-line longer-tail year headchar-capital
I-Reference	error rate training in statistical machine	page=50 xpos=0 ypos=7 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	translation. Journal of Computational	page=50 xpos=0 ypos=7 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	Information Systems, 8(2):861–868.	page=50 xpos=0 ypos=7 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period column-bottom
I-Reference	Liang, Percy, Alexandre Bouchard-Côté, Dan	page=50 xpos=0 ypos=7 single-column right-indent font-smallest column-top headchar-capital column-bottom
B-Reference	Thayer, and Jakob Uszkoreit. 2008.	page=50 xpos=5 ypos=7 right-column right-indent font-smallest column-top year headchar-capital tailchar-period
I-Reference	Lattice-based minimum error rate training	page=50 xpos=5 ypos=7 right-column right-indent font-smallest aligned-line longer-tail headchar-capital
I-Reference	for statistical machine translation. In	page=50 xpos=5 ypos=7 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower
B-Reference	Proceedings of EMNLP, pages 725–734,	page=50 xpos=5 ypos=7 right-column right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-comma
I-Reference	Honolulu, HI.	page=50 xpos=5 ypos=7 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period column-bottom
I-Reference	Marton, Yuval and Philip Resnik. 2008. Soft	page=50 xpos=5 ypos=7 single-column left-indent right-indent font-smallest column-top year headchar-capital column-bottom
I-Reference	Klein, and Ben Taskar. 2006. An end-to-end	page=50 xpos=0 ypos=8 left-column left-indent right-indent font-smallest column-top year headchar-capital
I-Reference	discriminative approach to machine	page=50 xpos=0 ypos=8 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	translation. In Proceedings of COLING/ACL,	page=50 xpos=0 ypos=8 left-column left-indent right-indent font-smallest aligned-line longer-tail headchar-lower tailchar-comma
I-Reference	pages 761–768, Sydney.	page=50 xpos=0 ypos=8 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period column-bottom
B-Reference	Lin, Chin-Yew and Franz Josef Och. 2004.	page=50 xpos=0 ypos=8 single-column right-indent font-smallest column-top year headchar-capital tailchar-period column-bottom
I-Reference	syntactic constraints for hierarchical	page=50 xpos=5 ypos=8 right-column right-indent font-smallest column-top headchar-lower
B-Reference	phrased-based translation. In Proceedings of	page=50 xpos=5 ypos=8 right-column full-justified font-smallest aligned-line longer-tail headchar-lower
I-Reference	ACL/HLT, pages 1003–1011, Columbus,	page=50 xpos=5 ypos=8 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-comma
I-Reference	OH.	page=50 xpos=5 ypos=8 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period column-bottom
I-Reference	Mathur, Prashant, Cettolo Mauro, and	page=50 xpos=5 ypos=8 single-column left-indent right-indent font-smallest column-top headchar-capital column-bottom
I-Reference	Orange: a method for evaluating	page=50 xpos=0 ypos=8 left-column left-indent right-indent font-smallest column-top headchar-capital
I-Reference	automatic evaluation metrics for machine	page=50 xpos=0 ypos=8 left-column left-indent right-indent font-smallest aligned-line longer-tail headchar-lower
I-Reference	translation. In Proceedings of COLING,	page=50 xpos=0 ypos=9 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-comma
I-Reference	pages 501–507, Geneva.	page=50 xpos=0 ypos=9 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period column-bottom
B-Reference	Marcello Federico. 2013. Online learning	page=50 xpos=5 ypos=8 right-column right-indent font-smallest column-top year headchar-capital
I-Footnote	approaches in computer assisted	page=50 xpos=5 ypos=8 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Footnote	translation. In Proceedings of WMT,	page=50 xpos=5 ypos=9 right-column right-indent font-smallest aligned-line longer-tail headchar-lower tailchar-comma
I-Footnote	pages 301–308, Sofia.	page=50 xpos=5 ypos=9 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
Page	51	page=50 xpos=9 ypos=9 right-column left-indent indented-line longer-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Computational Linguistics	page=51 xpos=0 ypos=0 left-column right-indent font-smallest page-top headchar-capital column-bottom above-blank-line above-double-space above-line-space
B-Reference	McDonald, Ryan, Koby Crammer, and	page=51 xpos=0 ypos=0 single-column right-indent font-smallest column-top line-blank-line line-double-space line-space headchar-capital column-bottom
I-Reference	Volume 42, Number 1	page=51 xpos=7 ypos=0 right-column left-indent right-over font-smallest column-top headchar-capital column-bottom above-blank-line above-double-space above-line-space
B-Reference	Pauls, Adam, John Denero, and Dan Klein.	page=51 xpos=5 ypos=0 single-column left-indent right-over font-smallest column-top line-blank-line line-double-space line-space headchar-capital tailchar-period column-bottom
I-Reference	Fernando Pereira. 2005. Online	page=51 xpos=0 ypos=0 left-column left-indent right-indent font-smallest column-top year headchar-capital
I-Reference	large-margin training of dependency	page=51 xpos=0 ypos=0 left-column left-indent right-indent font-smallest aligned-line longer-tail headchar-lower
I-Reference	parsers. In Proceedings of ACL, pages 91–98,	page=51 xpos=0 ypos=1 left-column left-indent right-indent font-smallest aligned-line longer-tail headchar-lower tailchar-comma
I-Reference	Ann Arbor, MI.	page=51 xpos=0 ypos=1 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period column-bottom
I-Reference	McDonald, Ryan, Keith Hall, and Gideon	page=51 xpos=0 ypos=1 single-column right-indent font-smallest column-top headchar-capital column-bottom
B-Reference	2009. Consensus training for consensus	page=51 xpos=5 ypos=0 right-column right-over font-smallest column-top year
I-Reference	decoding in machine translation. In	page=51 xpos=5 ypos=0 right-column right-over font-smallest aligned-line shorter-tail headchar-lower
B-Reference	Proceedings of EMNLP, pages 1418–1427,	page=51 xpos=5 ypos=1 right-column right-over font-smallest aligned-line longer-tail headchar-capital tailchar-comma
I-Reference	Singapore.	page=51 xpos=5 ypos=1 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period column-bottom
I-Reference	Pecina, Pavel, Antonio Toral, and Josef van	page=51 xpos=5 ypos=1 single-column left-indent right-over font-smallest column-top headchar-capital column-bottom
I-Reference	Mann. 2010. Distributed training strategies	page=51 xpos=0 ypos=1 left-column left-indent right-indent font-smallest column-top year headchar-capital
I-Reference	for the structured perceptron. In	page=51 xpos=0 ypos=1 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	Proceedings of HLT/NAACL, pages 456–464,	page=51 xpos=0 ypos=1 left-column left-indent right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-comma
I-Reference	Los Angeles, CA.	page=51 xpos=0 ypos=1 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Moore, Robert C. and Chris Quirk. 2008.	page=51 xpos=0 ypos=2 left-column right-indent font-smallest hanged-line longer-tail year headchar-capital tailchar-period column-bottom
I-Reference	Random restarts in minimum error rate	page=51 xpos=0 ypos=2 single-column left-indent right-indent font-smallest column-top headchar-capital column-bottom
B-Reference	Genabith. 2012. Simple and effective	page=51 xpos=5 ypos=1 right-column right-over font-smallest column-top year headchar-capital
I-Reference	parameter tuning for domain adaptation	page=51 xpos=5 ypos=1 right-column right-over font-smallest aligned-line longer-tail headchar-lower
I-Reference	of statistical machine translation. In	page=51 xpos=5 ypos=1 right-column right-over font-smallest aligned-line shorter-tail headchar-lower
B-Reference	Proceedings of COLING, pages 2209–2224,	page=51 xpos=5 ypos=1 right-column right-over font-smallest aligned-line longer-tail headchar-capital tailchar-comma
I-Reference	Mumbai.	page=51 xpos=5 ypos=2 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period column-bottom
I-Reference	Peitz, Stephan, Arne Mauser, Joern Wuebker,	page=51 xpos=5 ypos=2 single-column left-indent right-over font-smallest column-top headchar-capital tailchar-comma column-bottom
I-Reference	training for statistical machine translation.	page=51 xpos=0 ypos=2 left-column left-indent right-indent font-smallest column-top headchar-lower tailchar-period
I-Reference	In Proceedings of COLING, pages 585–592,	page=51 xpos=0 ypos=2 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-comma
I-Reference	Manchester.	page=51 xpos=0 ypos=2 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Nakov, Preslav, Francisco Guzman, and	page=51 xpos=0 ypos=2 left-column right-indent font-smallest hanged-line longer-tail headchar-capital column-bottom
I-Reference	Stephan Vogel. 2012. Optimizing for	page=51 xpos=0 ypos=2 single-column left-indent right-indent font-smallest column-top year headchar-capital column-bottom
I-Reference	and Hermann Ney. 2012. Forced	page=51 xpos=5 ypos=2 right-column right-indent font-smallest column-top year headchar-lower
I-Reference	derivations for hierarchical machine	page=51 xpos=5 ypos=2 right-column right-over font-smallest aligned-line longer-tail headchar-lower
I-Reference	translation. In Proceedings of COLING:	page=51 xpos=5 ypos=2 right-column right-over font-smallest aligned-line longer-tail headchar-lower tailchar-colon
I-Reference	Posters, pages 933–942, Mumbai.	page=51 xpos=5 ypos=2 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period column-bottom
I-Reference	Platt, John C. 1999. Fast training of support	page=51 xpos=5 ypos=2 single-column left-indent right-over font-smallest column-top year headchar-capital column-bottom
I-Reference	sentence-level BLEU+1 yields short	page=51 xpos=0 ypos=3 left-column left-indent right-indent font-smallest column-top headchar-lower
I-Reference	translations. In Proceedings of COLING,	page=51 xpos=0 ypos=3 left-column left-indent right-indent font-smallest aligned-line longer-tail headchar-lower tailchar-comma
I-Reference	pages 1979–1994, Mumbai.	page=51 xpos=0 ypos=3 left-column left-indent right-indent font-smallest aligned-line shorter-tail year headchar-lower tailchar-period
B-Reference	Nakov, Preslav, Francisco Guzmán, and	page=51 xpos=0 ypos=3 left-column right-indent font-smallest hanged-line longer-tail headchar-capital
I-Reference	Stephan Vogel. 2013. A tale about pro and	page=51 xpos=0 ypos=3 left-column left-indent right-indent font-smallest indented-line longer-tail year headchar-capital
I-Reference	monsters. In Proceedings of ACL: Short	page=51 xpos=0 ypos=3 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower column-bottom
I-Reference	Papers, pages 12–17, Sofia.	page=51 xpos=0 ypos=3 single-column left-indent right-indent font-smallest column-top headchar-capital tailchar-period column-bottom
I-Reference	vector machines using sequential minimal	page=51 xpos=5 ypos=3 right-column right-over font-smallest column-top headchar-lower
I-Reference	optimization. In Bernhard Schölkopf,	page=51 xpos=5 ypos=3 right-column right-over font-smallest aligned-line shorter-tail headchar-lower tailchar-comma
B-Reference	Christopher J. C. Burges, and Alexander	page=51 xpos=5 ypos=3 right-column right-over font-smallest aligned-line longer-tail headchar-capital
I-Reference	J. Smola, editors, Advances in Kernel	page=51 xpos=5 ypos=3 right-column right-over font-smallest aligned-line shorter-tail headchar-capital
B-Reference	Methods. MIT Press, Cambridge, MA,	page=51 xpos=5 ypos=3 right-column right-over font-smallest aligned-line longer-tail headchar-capital tailchar-comma
I-Reference	pages 185–208.	page=51 xpos=5 ypos=3 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period column-bottom
I-Reference	Press, William H., Saul A. Teukolsky,	page=51 xpos=5 ypos=3 single-column left-indent right-over font-smallest column-top headchar-capital tailchar-comma column-bottom
B-Reference	Neidert, Julia, Sebastian Schuster, Spence	page=51 xpos=0 ypos=4 left-column right-indent font-smallest column-top headchar-capital
I-Reference	Green, Kenneth Heafield, and Christopher	page=51 xpos=0 ypos=4 left-column left-indent right-indent font-smallest indented-line longer-tail headchar-capital
I-Reference	Manning. 2014. Stanford University’s	page=51 xpos=0 ypos=4 left-column left-indent right-indent font-smallest aligned-line shorter-tail year headchar-capital
I-Reference	submissions to the WMT 2014 translation	page=51 xpos=0 ypos=4 left-column left-indent right-indent font-smallest aligned-line longer-tail year headchar-lower
I-Reference	task. In Proceedings of WMT,	page=51 xpos=0 ypos=4 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-comma column-bottom
I-Reference	pages 150–156, Baltimore, MD.	page=51 xpos=0 ypos=4 single-column left-indent right-indent font-smallest column-top headchar-lower tailchar-period column-bottom
I-Reference	William T. Vetterling, and Brian P.	page=51 xpos=5 ypos=4 right-column right-indent font-smallest column-top headchar-capital tailchar-period
B-Reference	Flannery. 2007. Numerical Recipes 3rd	page=51 xpos=5 ypos=4 right-column right-over font-smallest aligned-line longer-tail year headchar-capital
I-Reference	Edition: The Art of Scientific Computing.	page=51 xpos=5 ypos=4 right-column right-over font-smallest aligned-line longer-tail headchar-capital tailchar-period
B-Reference	Cambridge University Press, New York,	page=51 xpos=5 ypos=4 right-column right-over font-smallest aligned-line longer-tail headchar-capital tailchar-comma
I-Reference	NY.	page=51 xpos=5 ypos=4 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period column-bottom
I-Reference	Razmara, Majid and Anoop Sarkar. 2013.	page=51 xpos=5 ypos=4 single-column left-indent right-over font-smallest column-top year headchar-capital tailchar-period column-bottom
B-Reference	Nguyen, Patrick, Milind Mahajan, and	page=51 xpos=0 ypos=5 left-column right-indent font-smallest column-top headchar-capital
I-Reference	Xiaodong He. 2007. Training	page=51 xpos=0 ypos=5 left-column left-indent right-indent font-smallest indented-line shorter-tail year headchar-capital
I-Reference	non-parametric features for statistical	page=51 xpos=0 ypos=5 left-column left-indent right-indent font-smallest aligned-line longer-tail headchar-lower column-bottom
B-Reference	Stacking for statistical machine translation.	page=51 xpos=5 ypos=5 right-column right-over font-smallest column-top headchar-capital tailchar-period
I-Reference	In Proceedings of ACL: Short Papers,	page=51 xpos=5 ypos=5 right-column full-justified font-smallest aligned-line shorter-tail headchar-capital tailchar-comma
I-Reference	pages 334–339, Sofia.	page=51 xpos=5 ypos=5 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period column-bottom
I-Reference	machine translation. In Proceedings of Recht, Benjamin, Christopher Re, Stephen	page=51 xpos=0 ypos=5 single-column left-indent right-over font-smallest column-top headchar-lower column-bottom
I-Reference	WMT, pages 72–79, Prague.	page=51 xpos=0 ypos=5 left-column left-indent right-indent font-smallest column-top headchar-capital tailchar-period
B-Reference	Nocedal, Jorge and Stephen J. Wright. 2006.	page=51 xpos=0 ypos=5 left-column right-indent font-smallest hanged-line longer-tail year headchar-capital tailchar-period
I-Reference	Conjugate Gradient Methods. Springer,	page=51 xpos=0 ypos=5 left-column left-indent right-indent font-smallest indented-line shorter-tail headchar-capital tailchar-comma
I-Reference	New York.	page=51 xpos=0 ypos=6 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period column-bottom
I-Reference	Och, Franz Josef. 2003. Minimum error rate	page=51 xpos=0 ypos=6 single-column right-indent font-smallest column-top year headchar-capital column-bottom
B-Reference	Wright, and Feng Niu. 2011. Hogwild:	page=51 xpos=5 ypos=5 right-column right-over font-smallest column-top year headchar-capital tailchar-colon
I-Reference	A lock-free approach to parallelizing	page=51 xpos=5 ypos=5 right-column right-over font-smallest aligned-line shorter-tail headchar-capital
I-Reference	stochastic gradient descent. In Proceedings	page=51 xpos=5 ypos=5 right-column right-over font-smallest aligned-line longer-tail headchar-lower
I-Reference	of NIPS, pages 693–701, Vancouver.	page=51 xpos=5 ypos=6 right-column right-over font-smallest aligned-line shorter-tail headchar-lower tailchar-period column-bottom
I-Reference	Rosti, Antti-Veikko, Bing Zhang, Spyros	page=51 xpos=5 ypos=6 single-column left-indent right-over font-smallest column-top headchar-capital column-bottom
I-Reference	training in statistical machine translation.	page=51 xpos=0 ypos=6 left-column left-indent right-indent font-smallest column-top headchar-lower tailchar-period
I-Reference	In Proceedings of ACL, pages 160–167,	page=51 xpos=0 ypos=6 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-comma
I-Reference	Sapporo.	page=51 xpos=0 ypos=6 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Och, Franz Josef and Hermann Ney. 2002.	page=51 xpos=0 ypos=6 left-column right-indent font-smallest hanged-line longer-tail year headchar-capital tailchar-period column-bottom
I-Reference	Discriminative training and maximum	page=51 xpos=0 ypos=6 single-column left-indent right-indent font-smallest column-top headchar-capital column-bottom
B-Reference	Matsoukas, and Richard Schwartz. 2010.	page=51 xpos=5 ypos=6 right-column right-over font-smallest column-top year headchar-capital tailchar-period
I-Reference	BBN system description for WMT10	page=51 xpos=5 ypos=6 right-column right-over font-smallest aligned-line shorter-tail headchar-capital
I-Reference	system combination task. In Proceedings of	page=51 xpos=5 ypos=6 right-column right-over font-smallest aligned-line longer-tail headchar-lower
I-Reference	WMT, pages 321–326, Uppsala.	page=51 xpos=5 ypos=6 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period column-bottom
I-Reference	Rosti, Antti-Veikko, Bing Zhang, Spyros	page=51 xpos=5 ypos=6 single-column left-indent right-over font-smallest column-top headchar-capital column-bottom
I-Reference	entropy models for statistical machine	page=51 xpos=0 ypos=7 left-column left-indent right-indent font-smallest column-top headchar-lower
I-Reference	translation. In Proceedings of ACL,	page=51 xpos=0 ypos=7 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-comma
I-Reference	pages 295–302, Philadelphia, PA.	page=51 xpos=0 ypos=7 left-column left-indent right-indent font-smallest aligned-line headchar-lower tailchar-period
B-Reference	Och, Franz Josef and Hermann Ney. 2003.	page=51 xpos=0 ypos=7 left-column right-indent font-smallest hanged-line longer-tail year headchar-capital tailchar-period
I-Reference	A systematic comparison of various	page=51 xpos=0 ypos=7 left-column left-indent right-indent font-smallest indented-line shorter-tail headchar-capital column-bottom
I-Reference	statistical alignment models. Computational	page=51 xpos=0 ypos=7 single-column left-indent right-indent font-smallest column-top headchar-lower column-bottom
B-Reference	Matsoukas, and Richard Schwartz. 2011.	page=51 xpos=5 ypos=7 right-column right-over font-smallest column-top year headchar-capital tailchar-period
I-Reference	Expected BLEU training for graphs: BBN	page=51 xpos=5 ypos=7 right-column right-over font-smallest aligned-line headchar-capital
I-Reference	system description for WMT11 system	page=51 xpos=5 ypos=7 right-column right-over font-smallest aligned-line shorter-tail headchar-lower
I-Reference	combination task. In Proceedings of WMT,	page=51 xpos=5 ypos=7 right-column right-over font-smallest aligned-line longer-tail headchar-lower tailchar-comma
I-Reference	pages 159–165, Edinburgh.	page=51 xpos=5 ypos=7 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period column-bottom
I-Reference	Roth, Benjamin, Andrew McCallum, Marc	page=51 xpos=5 ypos=7 single-column left-indent right-over font-smallest column-top headchar-capital column-bottom
I-Reference	Linguistics, 29(1):19–51.	page=51 xpos=0 ypos=8 left-column left-indent right-indent font-smallest column-top headchar-capital tailchar-period
B-Reference	Papineni, K. A. 1999. Discriminative training	page=51 xpos=0 ypos=8 left-column right-indent font-smallest hanged-line longer-tail year headchar-capital
I-Reference	via linear programming. In Proceedings of	page=51 xpos=0 ypos=8 left-column left-indent right-indent font-smallest indented-line shorter-tail headchar-lower
I-Reference	ICASSP, pages 561–564, Phoenix, AZ.	page=51 xpos=0 ypos=8 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period column-bottom
I-Reference	Papineni, Kishore, Salim Roukos, Todd	page=51 xpos=0 ypos=8 single-column right-indent font-smallest column-top headchar-capital column-bottom
B-Reference	Dymetman, and Nicola Cancedda. 2010.	page=51 xpos=5 ypos=8 right-column right-over font-smallest column-top year headchar-capital tailchar-period
I-Reference	Machine translation using overlapping	page=51 xpos=5 ypos=8 right-column right-over font-smallest aligned-line shorter-tail headchar-capital
I-Reference	alignments and SampleRank. In	page=51 xpos=5 ypos=8 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	Proceedings of AMTA, Denver, CO.	page=51 xpos=5 ypos=8 right-column right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-period column-bottom
I-Reference	Saluja, Avneesh, Ian Lane, and Ying Zhang.	page=51 xpos=5 ypos=8 single-column left-indent right-over font-smallest column-top headchar-capital tailchar-period column-bottom
I-Reference	Ward, and Wei-Jing Zhu. 2002. BLEU: a	page=51 xpos=0 ypos=8 left-column left-indent right-indent font-smallest column-top year headchar-capital
I-Reference	method for automatic evaluation of	page=51 xpos=0 ypos=8 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	machine translation. In Proceedings of ACL,	page=51 xpos=0 ypos=9 left-column left-indent right-indent font-smallest aligned-line longer-tail headchar-lower tailchar-comma
I-Reference	pages 311–318, Philadelphia, PA.	page=51 xpos=0 ypos=9 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
Page	52	page=51 xpos=0 ypos=9 left-column right-indent hanged-line shorter-tail line-blank-line line-double-space line-space numeric-only column-bottom
B-Reference	2012. Machine translation with binary	page=51 xpos=5 ypos=8 right-column right-over font-smallest column-top year
I-Reference	feedback: A large-margin approach.	page=51 xpos=5 ypos=8 right-column right-over font-smallest aligned-line shorter-tail headchar-lower tailchar-period
I-Reference	In Proceedings of AMTA, San Diego,	page=51 xpos=5 ypos=9 right-column right-over font-smallest aligned-line shorter-tail headchar-capital tailchar-comma
I-Reference	CA.	page=51 xpos=5 ypos=9 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period page-bottom
B-Header	Neubig and Watanabe Optimization for Statistical Machine Translation	page=52 xpos=0 ypos=0 single-column full-justified font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Reference	Sanchis-Trilles, Germán and Francisco Suzuki, Jun, Kevin Duh, and Masaaki	page=52 xpos=0 ypos=0 single-column right-indent font-smallest aligned-line shorter-tail line-blank-line line-double-space line-space headchar-capital column-bottom
I-Reference	Casacuberta. 2010. Log-linear weight	page=52 xpos=0 ypos=0 left-column left-indent right-indent font-smallest column-top year headchar-capital
I-Reference	optimisation via Bayesian adaptation	page=52 xpos=0 ypos=0 left-column left-indent right-indent font-smallest aligned-line headchar-lower
I-Reference	in statistical machine translation. In	page=52 xpos=0 ypos=1 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	Proceedings of COLING: Posters,	page=52 xpos=0 ypos=1 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-comma column-bottom
I-Reference	pages 1077–1085, Beijing.	page=52 xpos=0 ypos=1 single-column left-indent right-indent font-smallest column-top headchar-lower tailchar-period column-bottom
B-Reference	Nagata. 2011. Distributed minimum error	page=52 xpos=5 ypos=0 right-column right-indent font-smallest column-top year headchar-capital
I-Reference	rate training of SMT using particle swarm	page=52 xpos=5 ypos=0 right-column right-indent font-smallest aligned-line headchar-lower
I-Reference	optimization. In Proceedings of IJCNLP,	page=52 xpos=5 ypos=1 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-comma
I-Reference	pages 649–657, Chiang Mai.	page=52 xpos=5 ypos=1 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period column-bottom
I-Reference	Tan, Ming, Tian Xia, Shaojun Wang, and	page=52 xpos=5 ypos=1 single-column left-indent right-indent font-smallest column-top headchar-capital column-bottom
B-Reference	Sankaran, Baskaran, Anoop Sarkar, and	page=52 xpos=0 ypos=1 left-column right-indent font-smallest column-top headchar-capital
I-Reference	Kevin Duh. 2013. Multi-metric	page=52 xpos=0 ypos=1 left-column left-indent right-indent font-smallest indented-line shorter-tail year headchar-capital
I-Reference	optimization using ensemble tuning. In	page=52 xpos=0 ypos=1 left-column left-indent right-indent font-smallest aligned-line longer-tail headchar-lower
I-Reference	Proceedings of HLT/NAACL, pages 947–957,	page=52 xpos=0 ypos=1 left-column left-indent right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-comma column-bottom
I-Reference	Atlanta, GA.	page=52 xpos=0 ypos=2 single-column left-indent right-indent font-smallest column-top headchar-capital tailchar-period column-bottom
B-Reference	Bowen Zhou. 2013. A corpus level MIRA	page=52 xpos=5 ypos=1 right-column right-indent font-smallest column-top year headchar-capital
I-Reference	tuning strategy for machine translation. In	page=52 xpos=5 ypos=1 right-column right-indent font-smallest aligned-line longer-tail headchar-lower
I-Reference	Proceedings of EMNLP, pages 851–856,	page=52 xpos=5 ypos=1 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-comma
I-Reference	Seattle, WA.	page=52 xpos=5 ypos=1 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period column-bottom
I-Reference	Taskar, Ben, Vassil Chatalbashev, Daphne	page=52 xpos=5 ypos=2 single-column left-indent right-indent font-smallest column-top headchar-capital column-bottom
I-Reference	Servan, Christophe and Holger Schwenk.	page=52 xpos=0 ypos=2 left-column right-indent font-smallest column-top headchar-capital tailchar-period
I-Reference	2011. Optimising multiple metrics with	page=52 xpos=0 ypos=2 left-column left-indent right-indent font-smallest indented-line year
I-Reference	MERT. Prague Bulletin of Mathematical	page=52 xpos=0 ypos=2 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital
I-Reference	Linguistics, 96(1):109–117.	page=52 xpos=0 ypos=2 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period column-bottom
B-Reference	Setiawan, Hendra and Bowen Zhou. 2013.	page=52 xpos=0 ypos=2 single-column right-indent font-smallest column-top year headchar-capital tailchar-period column-bottom
I-Reference	Koller, and Carlos Guestrin. 2005.	page=52 xpos=5 ypos=2 right-column right-indent font-smallest column-top year headchar-capital tailchar-period
B-Reference	Learning structured prediction models: A	page=52 xpos=5 ypos=2 right-column right-indent font-smallest aligned-line longer-tail headchar-capital
I-Reference	large margin approach. In Proceedings of	page=52 xpos=5 ypos=2 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	ICML, pages 896–903, Bonn.	page=52 xpos=5 ypos=2 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period column-bottom
I-Reference	Tibshirani, Robert. 1996. Regression	page=52 xpos=5 ypos=2 single-column left-indent right-indent font-smallest column-top year headchar-capital column-bottom
I-Reference	Discriminative training of 150 million	page=52 xpos=0 ypos=2 left-column left-indent right-indent font-smallest column-top headchar-capital
I-Reference	translation parameters and its application	page=52 xpos=0 ypos=3 left-column left-indent right-indent font-smallest aligned-line longer-tail headchar-lower
I-Reference	to pruning. In Proceedings of HLT/NAACL,	page=52 xpos=0 ypos=3 left-column left-indent right-indent font-smallest aligned-line headchar-lower tailchar-comma column-bottom
I-Reference	pages 335–341, Atlanta, GA.	page=52 xpos=0 ypos=3 single-column left-indent right-indent font-smallest column-top headchar-lower tailchar-period column-bottom
I-Reference	shrinkage and selection via the lasso.	page=52 xpos=5 ypos=2 right-column right-indent font-smallest column-top headchar-lower tailchar-period
I-Reference	Journal of the Royal Statistical Society.	page=52 xpos=5 ypos=3 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Series B (Methodological), 58(1):267–288.	page=52 xpos=5 ypos=3 right-column right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-period column-bottom
I-Reference	Tillman, Christoph. 2004. A unigram	page=52 xpos=5 ypos=3 single-column left-indent right-indent font-smallest column-top year headchar-capital column-bottom
B-Reference	Shen, Libin, Anoop Sarkar, and Franz Josef	page=52 xpos=0 ypos=3 left-column right-indent font-smallest column-top headchar-capital
I-Reference	Och. 2004. Discriminative reranking for	page=52 xpos=0 ypos=3 left-column left-indent right-indent font-smallest indented-line shorter-tail year headchar-capital
I-Reference	machine translation. In Proceedings of	page=52 xpos=0 ypos=3 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower column-bottom
I-Reference	HLT/NAACL, pages 177–184, Boston, MA.	page=52 xpos=0 ypos=3 single-column left-indent right-indent font-smallest column-top headchar-capital tailchar-period column-bottom
I-Reference	orientation model for statistical machine	page=52 xpos=5 ypos=3 right-column right-indent font-smallest column-top headchar-lower
I-Reference	translation. In Proceedings of HLT/NAACL,	page=52 xpos=5 ypos=3 right-column right-indent font-smallest aligned-line longer-tail headchar-lower tailchar-comma
I-Reference	pages 101–104, Boston, MA.	page=52 xpos=5 ypos=3 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period column-bottom
I-Reference	Tillmann, Christoph and Tong Zhang. 2006.	page=52 xpos=5 ypos=3 single-column left-indent right-indent font-smallest column-top year headchar-capital tailchar-period column-bottom
B-Reference	Simianer, Patrick, Stefan Riezler, and Chris	page=52 xpos=0 ypos=4 left-column right-indent font-smallest column-top headchar-capital
I-Reference	Dyer. 2012. Joint feature selection in	page=52 xpos=0 ypos=4 left-column left-indent right-indent font-smallest indented-line shorter-tail year headchar-capital
I-Reference	distributed stochastic learning for	page=52 xpos=0 ypos=4 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower column-bottom
I-Reference	large-scale discriminative training in SMT.	page=52 xpos=0 ypos=4 single-column left-indent right-indent font-smallest column-top headchar-lower tailchar-period column-bottom
B-Reference	A discriminative global training algorithm	page=52 xpos=5 ypos=4 right-column right-indent font-smallest column-top headchar-capital
I-Reference	for statistical MT. In Proceedings of	page=52 xpos=5 ypos=4 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	COLING/ACL, pages 721–728, Sydney.	page=52 xpos=5 ypos=4 right-column right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-period column-bottom
I-Reference	Toutanova, Kristina and Byung-Gyu Ahn.	page=52 xpos=5 ypos=4 single-column left-indent right-indent font-smallest column-top headchar-capital tailchar-period column-bottom
I-Reference	In Proceedings of ACL, pages 11–21, Jeju	page=52 xpos=0 ypos=4 left-column left-indent right-indent font-smallest column-top headchar-capital
I-Reference	Island.	page=52 xpos=0 ypos=4 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Sindhwani, Vikas, S. Sathiya Keerthi, and	page=52 xpos=0 ypos=5 left-column right-indent font-smallest hanged-line longer-tail headchar-capital
I-Reference	Olivier Chapelle. 2006. Deterministic	page=52 xpos=0 ypos=5 left-column left-indent right-indent font-smallest indented-line shorter-tail year headchar-capital column-bottom
I-Reference	annealing for semi-supervised kernel	page=52 xpos=0 ypos=5 single-column left-indent right-indent font-smallest column-top headchar-lower column-bottom
B-Reference	2013. Learning non-linear features for	page=52 xpos=5 ypos=4 right-column right-indent font-smallest column-top year
I-Reference	machine translation using gradient	page=52 xpos=5 ypos=4 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	boosting machines. In Proceedings of ACL:	page=52 xpos=5 ypos=5 right-column right-indent font-smallest aligned-line longer-tail headchar-lower tailchar-colon
I-Reference	Short Papers, pages 406–411, Sofia.	page=52 xpos=5 ypos=5 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period column-bottom
I-Reference	Tromble, Roy, Shankar Kumar, Franz Och,	page=52 xpos=5 ypos=5 single-column left-indent right-indent font-smallest column-top headchar-capital tailchar-comma column-bottom
I-Reference	machines. In Proceedings of ICML,	page=52 xpos=0 ypos=5 left-column left-indent right-indent font-smallest column-top headchar-lower tailchar-comma
I-Reference	pages 841–848, Pittsburgh, PA.	page=52 xpos=0 ypos=5 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Reference	Smith, David A. and Jason Eisner. 2006.	page=52 xpos=0 ypos=5 left-column right-indent font-smallest hanged-line longer-tail year headchar-capital tailchar-period
I-Reference	Minimum risk annealing for training	page=52 xpos=0 ypos=5 left-column left-indent right-indent font-smallest indented-line headchar-capital
I-Reference	log-linear models. In Proceedings of	page=52 xpos=0 ypos=6 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower column-bottom
I-Reference	COLING/ACL: Poster Sessions,	page=52 xpos=0 ypos=6 single-column left-indent right-indent font-smallest column-top headchar-capital tailchar-comma column-bottom
I-Reference	and Wolfgang Macherey. 2008. Lattice	page=52 xpos=5 ypos=5 right-column right-indent font-smallest column-top year headchar-lower
I-Reference	minimum Bayes-risk decoding for	page=52 xpos=5 ypos=5 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	statistical machine translation. In	page=52 xpos=5 ypos=5 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower
B-Reference	Proceedings of EMNLP, pages 620–629,	page=52 xpos=5 ypos=5 right-column right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-comma
I-Reference	Honolulu, HI.	page=52 xpos=5 ypos=6 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period column-bottom
I-Reference	Turian, Joseph, Benjamin Wellington, and	page=52 xpos=5 ypos=6 single-column left-indent right-indent font-smallest column-top headchar-capital column-bottom
I-Reference	pages 787–794, Sydney.	page=52 xpos=0 ypos=6 left-column left-indent right-indent font-smallest column-top headchar-lower tailchar-period
B-Reference	Snover, Matthew, Bonnie Dorr, Richard	page=52 xpos=0 ypos=6 left-column right-indent font-smallest hanged-line longer-tail headchar-capital
I-Reference	Schwartz, Linnea Micciulla, and John	page=52 xpos=0 ypos=6 left-column left-indent right-indent font-smallest indented-line headchar-capital
I-Reference	Makhoul. 2006. A study of translation edit	page=52 xpos=0 ypos=6 left-column left-indent right-indent font-smallest aligned-line longer-tail year headchar-capital
I-Reference	rate with targeted human annotation. In	page=52 xpos=0 ypos=6 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower column-bottom
I-Reference	Proceedings of AMTA, pages 223–231,	page=52 xpos=0 ypos=7 single-column left-indent right-indent font-smallest column-top headchar-capital tailchar-comma column-bottom
B-Reference	I. Dan Melamed. 2006. Scalable	page=52 xpos=5 ypos=6 right-column right-indent font-smallest column-top year headchar-capital
I-Reference	discriminative learning for natural	page=52 xpos=5 ypos=6 right-column right-indent font-smallest aligned-line longer-tail headchar-lower
I-Reference	language parsing and translation. In	page=52 xpos=5 ypos=6 right-column right-indent font-smallest aligned-line longer-tail headchar-lower
I-Reference	Proceedings of NIPS, pages 1409–1416,	page=52 xpos=5 ypos=6 right-column right-indent font-smallest aligned-line headchar-capital tailchar-comma
I-Reference	Vancouver.	page=52 xpos=5 ypos=6 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period column-bottom
I-Reference	Ueffing, Nicola, Gholamreza Haffari, and	page=52 xpos=5 ypos=7 single-column left-indent right-indent font-smallest column-top headchar-capital column-bottom
I-Reference	Cambridge, MA.	page=52 xpos=0 ypos=7 left-column left-indent right-indent font-smallest column-top headchar-capital tailchar-period
B-Reference	Sokolov, Artem, Guillaume Wisniewski, and	page=52 xpos=0 ypos=7 left-column right-indent font-smallest hanged-line longer-tail headchar-capital
I-Reference	Francois Yvon. 2012a. Computing lattice	page=52 xpos=0 ypos=7 left-column left-indent right-indent font-smallest indented-line shorter-tail year headchar-capital column-bottom
I-Reference	BLEU oracle scores for machine	page=52 xpos=0 ypos=7 single-column left-indent right-indent font-smallest column-top headchar-capital column-bottom
B-Reference	Anoop Sarkar. 2007. Transductive learning	page=52 xpos=5 ypos=7 right-column right-indent font-smallest column-top year headchar-capital
I-Reference	for statistical machine translation. In	page=52 xpos=5 ypos=7 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Reference	Proceedings of ACL, pages 25–32, Prague.	page=52 xpos=5 ypos=7 right-column right-indent font-smallest aligned-line longer-tail headchar-capital tailchar-period column-bottom
I-Reference	Ueffing, Nicola, Franz Josef Och, and	page=52 xpos=5 ypos=7 single-column left-indent right-indent font-smallest column-top headchar-capital column-bottom
I-Reference	translation. In Proceedings of EACL,	page=52 xpos=0 ypos=7 left-column left-indent right-indent font-smallest column-top headchar-lower tailchar-comma
I-Reference	pages 120–129, Avignon.	page=52 xpos=0 ypos=8 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Reference	Sokolov, Artem, Guillaume Wisniewski, and	page=52 xpos=0 ypos=8 left-column right-indent font-smallest hanged-line longer-tail headchar-capital
I-Reference	François Yvon. 2012b. Non-linear n-best	page=52 xpos=0 ypos=8 left-column left-indent right-indent font-smallest indented-line shorter-tail year headchar-capital column-bottom
B-Reference	Hermann Ney. 2002. Generation of word	page=52 xpos=5 ypos=7 right-column right-indent font-smallest column-top year headchar-capital
I-Reference	graphs in statistical machine translation. In	page=52 xpos=5 ypos=8 right-column full-justified font-smallest aligned-line longer-tail headchar-lower
I-Reference	Proceedings of EMNLP, pages 156–163,	page=52 xpos=5 ypos=8 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-comma
I-Reference	Philadelphia, PA.	page=52 xpos=5 ypos=8 right-column right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period column-bottom
I-Reference	list reranking with few features. In Venugopal, Ashish and Stephan Vogel. 2005.	page=52 xpos=0 ypos=8 single-column left-indent right-indent font-smallest column-top year headchar-lower tailchar-period column-bottom
I-Reference	Proceedings of AMTA, San Diego, CA.	page=52 xpos=0 ypos=8 left-column left-indent right-indent font-smallest column-top headchar-capital tailchar-period
B-Reference	Sokolov, Artem and François Yvon. 2011.	page=52 xpos=0 ypos=8 left-column right-indent font-smallest hanged-line longer-tail year headchar-capital tailchar-period
I-Reference	Minimum error rate training semiring. In	page=52 xpos=0 ypos=8 left-column left-indent right-indent font-smallest indented-line longer-tail headchar-capital
I-Reference	Proceedings of EAMT, pages 241–248,	page=52 xpos=0 ypos=9 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-comma
I-Reference	Leuven.	page=52 xpos=0 ypos=9 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period column-bottom
B-Reference	Considerations in maximum mutual	page=52 xpos=5 ypos=8 right-column right-indent font-smallest column-top headchar-capital
I-Footnote	information and minimum classification	page=52 xpos=5 ypos=8 right-column right-indent font-smallest aligned-line longer-tail headchar-lower
I-Footnote	error training for statistical machine	page=52 xpos=5 ypos=8 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Footnote	translation. In Proceedings of EAMT,	page=52 xpos=5 ypos=9 right-column right-indent font-smallest aligned-line headchar-lower tailchar-comma
I-Footnote	pages 271–279, Budapest.	page=52 xpos=5 ypos=9 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
Page	53	page=52 xpos=9 ypos=9 right-column left-indent indented-line longer-tail line-blank-line line-double-space line-space numeric-only page-bottom
B-Header	Computational Linguistics Volume 42, Number 1	page=53 xpos=0 ypos=0 single-column right-over font-smallest page-top headchar-capital above-blank-line above-double-space above-line-space
I-Reference	Wang, Zhuoran, John Shawe-Taylor, and Xiao, Xinyan and Deyi Xiong. 2013.	page=53 xpos=0 ypos=0 single-column right-over font-smallest aligned-line shorter-tail line-blank-line line-double-space line-space year headchar-capital tailchar-period
B-Reference	Sandor Szedmak. 2007. Kernel regression Max-margin synchronous grammar	page=53 xpos=0 ypos=0 single-column left-indent right-over font-smallest indented-line longer-tail year headchar-capital
I-Reference	based machine translation. In induction for machine translation. In	page=53 xpos=0 ypos=0 single-column left-indent right-over font-smallest aligned-line longer-tail headchar-lower
I-Reference	Proceedings of HLT/NAACL: Short Papers, Proceedings of EMNLP, pages 255–264,	page=53 xpos=0 ypos=1 single-column left-indent right-over font-smallest aligned-line headchar-capital tailchar-comma
I-Reference	pages 185–188, Rochester, NY. Seattle, WA.	page=53 xpos=0 ypos=1 single-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Reference	Watanabe, Taro. 2012. Optimized online rank Yamada, Kenji and Kevin Knight. 2001. A	page=53 xpos=0 ypos=1 single-column right-over font-smallest hanged-line longer-tail year headchar-capital
I-Reference	learning for machine translation. In syntax-based statistical translation model.	page=53 xpos=0 ypos=1 single-column left-indent right-over font-smallest indented-line longer-tail headchar-lower tailchar-period
I-Reference	Proceedings of HLT/NAACL, pages 253–262, In Proceedings of ACL, pages 523–530,	page=53 xpos=0 ypos=1 single-column left-indent right-over font-smallest aligned-line shorter-tail headchar-capital tailchar-comma
I-Reference	Montréal. Toulouse.	page=53 xpos=0 ypos=1 single-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Watanabe, Taro, Jun Suzuki, Hajime Yu, Heng, Liang Huang, Haitao Mi, and Kai	page=53 xpos=0 ypos=1 single-column right-over font-smallest hanged-line longer-tail headchar-capital
I-Reference	Tsukada, and Hideki Isozaki. 2007. Online Zhao. 2013. Max-violation perceptron and	page=53 xpos=0 ypos=2 single-column left-indent right-over font-smallest indented-line year headchar-capital
I-Reference	large-margin training for statistical forced decoding for scalable MT training.	page=53 xpos=0 ypos=2 single-column left-indent right-over font-smallest aligned-line headchar-lower tailchar-period
I-Reference	machine translation. In Proceedings of In Proceedings of EMNLP, pages 1112–1123,	page=53 xpos=0 ypos=2 single-column left-indent right-over font-smallest aligned-line longer-tail headchar-lower tailchar-comma
I-Reference	EMNLP/CoNLL, pages 764–773, Seattle, WA.	page=53 xpos=0 ypos=2 single-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
I-Reference	Prague. Zaidan, Omar F. and Chris Callison-Burch.	page=53 xpos=0 ypos=2 single-column left-indent right-over font-smallest aligned-line longer-tail headchar-capital tailchar-period
B-Reference	Wu, Dekai. 1997. Stochastic inversion 2009. Feasibility of human-in-the-loop	page=53 xpos=0 ypos=2 single-column right-over font-smallest hanged-line shorter-tail year headchar-capital
I-Reference	transduction grammars and bilingual minimum error rate training. In Proceedings	page=53 xpos=0 ypos=2 single-column left-indent right-over font-smallest indented-line longer-tail headchar-lower
I-Reference	parsing of parallel corpora. Computational of EMNLP, pages 52–61, Singapore.	page=53 xpos=0 ypos=3 single-column left-indent right-over font-smallest aligned-line shorter-tail headchar-lower tailchar-period
I-Reference	Linguistics, 23(3):377–403. Zens, Richard, Sasa Hasan, and Hermann	page=53 xpos=0 ypos=3 single-column left-indent right-over font-smallest aligned-line longer-tail headchar-capital
B-Reference	Wuebker, Joern, Arne Mauser, and Hermann Ney. 2007. A systematic comparison of	page=53 xpos=0 ypos=3 single-column right-over font-smallest hanged-line year headchar-capital
I-Reference	Ney. 2010. Training phrase translation training criteria for statistical machine	page=53 xpos=0 ypos=3 single-column left-indent right-over font-smallest indented-line year headchar-capital
I-Reference	models with leaving-one-out. In translation. In Proceedings of	page=53 xpos=0 ypos=3 single-column left-indent right-over font-smallest aligned-line shorter-tail headchar-lower
I-Reference	Proceedings of ACL, pages 475–484, EMNLP/CoNLL, pages 524–532, Prague.	page=53 xpos=0 ypos=3 single-column left-indent right-over font-smallest aligned-line longer-tail headchar-capital tailchar-period
I-Reference	Uppsala. Zhao, Bing and Shengyuan Chen. 2009. A	page=53 xpos=0 ypos=3 single-column left-indent right-over font-smallest aligned-line year headchar-capital
B-Reference	Wuebker, Joern, Stephan Peitz, Andreas simplex Armijo downhill algorithm for	page=53 xpos=0 ypos=4 single-column right-over font-smallest hanged-line headchar-capital
I-Reference	Guta, and Hermann Ney. 2014. The RWTH optimizing statistical machine translation	page=53 xpos=0 ypos=4 single-column left-indent right-over font-smallest indented-line longer-tail year headchar-capital
I-Reference	Aachen machine translation systems for decoding parameters. In Proceedings of	page=53 xpos=0 ypos=4 single-column left-indent right-over font-smallest aligned-line shorter-tail headchar-capital
I-Reference	IWSLT 2014. In Proceedings of IWSLT, HLT/NAACL: Short Papers, pages 21–24,	page=53 xpos=0 ypos=4 single-column left-indent right-over font-smallest aligned-line longer-tail year headchar-capital tailchar-comma
I-Reference	pages 150–154, Lake Tahoe, NV. Boulder, CO.	page=53 xpos=0 ypos=4 single-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Reference	Xiang, Bing and Abraham Ittycheriah. 2011. Zhao, Yinggong, Shujie Liu, Yangsheng Ji,	page=53 xpos=0 ypos=4 single-column right-over font-smallest hanged-line longer-tail year headchar-capital tailchar-comma
I-Reference	Discriminative feature-tied mixture Jiajun Chen, and Guodong Zhou. 2011.	page=53 xpos=0 ypos=5 single-column left-indent right-over font-smallest indented-line year headchar-capital tailchar-period
I-Reference	modeling for statistical machine Transductive minimum error rate training	page=53 xpos=0 ypos=5 single-column left-indent right-over font-smallest aligned-line longer-tail headchar-lower
I-Reference	translation. In Proceedings of ACL/HLT, for statistical machine translation. In	page=53 xpos=0 ypos=5 single-column left-indent right-over font-smallest aligned-line shorter-tail headchar-lower
I-Reference	pages 424–428, Portland, OR. Proceedings of IJCNLP, pages 641–648,	page=53 xpos=0 ypos=5 single-column left-indent right-over font-smallest aligned-line headchar-lower tailchar-comma
B-Reference	Xiao, Lin. 2010. Dual averaging methods for Chiang Mai.	page=53 xpos=0 ypos=5 single-column right-indent font-smallest hanged-line shorter-tail year headchar-capital tailchar-period
I-Reference	regularized stochastic learning and online Zhou, Liang, Chin-Yew Lin, and Eduard	page=53 xpos=0 ypos=5 single-column left-indent right-over font-smallest indented-line longer-tail headchar-lower
I-Reference	optimization. Journal of Machine Learning Hovy. 2006. Re-evaluating machine	page=53 xpos=0 ypos=5 single-column left-indent right-over font-smallest aligned-line shorter-tail year headchar-lower
I-Reference	Research, 11:2543–2596. translation results with paraphrase	page=53 xpos=0 ypos=6 single-column left-indent right-over font-smallest aligned-line headchar-capital
B-Reference	Xiao, Xinyan, Yang Liu, Qun Liu, and support. In Proceedings of EMNLP,	page=53 xpos=0 ypos=6 single-column right-over font-smallest hanged-line shorter-tail headchar-capital tailchar-comma
I-Reference	Shouxun Lin. 2011. Fast generation pages 77–84, Sydney.	page=53 xpos=0 ypos=6 single-column left-indent right-over font-smallest indented-line shorter-tail year headchar-capital tailchar-period
I-Reference	of translation forest for large-scale Zinkevich, Martin, John Langford, and	page=53 xpos=0 ypos=6 single-column left-indent right-over font-smallest aligned-line longer-tail headchar-lower
I-Reference	SMT discriminative training. In Alex J. Smola. 2009. Slow learners are fast.	page=53 xpos=0 ypos=6 single-column left-indent right-over font-smallest aligned-line longer-tail year headchar-capital tailchar-period
I-Reference	Proceedings of EMNLP, In Proceedings of NIPS, pages 2331–2339,	page=53 xpos=0 ypos=6 single-column left-indent right-over font-smallest aligned-line shorter-tail headchar-capital tailchar-comma
I-Reference	pages 880–888, Edinburgh. Vancouver.	page=53 xpos=0 ypos=6 single-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
Page	54	page=53 xpos=0 ypos=9 single-column right-indent hanged-line shorter-tail line-blank-line line-double-space line-space numeric-only
