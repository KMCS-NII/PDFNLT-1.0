B-Header	Proceedings of HLT-NAACL 2003	page=0 xpos=6 ypos=0 right-column left-indent right-indent font-smallest page-top year headchar-capital
I-Header	Main Papers , pp. 173-180	page=0 xpos=7 ypos=0 right-column left-indent right-indent font-smallest indented-line headchar-capital
I-Header	Edmonton, May-June 2003	page=0 xpos=7 ypos=0 right-column left-indent right-indent font-smallest indented-line year headchar-capital column-bottom above-double-space above-line-space
Title	Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network	page=0 xpos=0 ypos=0 single-column centered left-indent right-indent font-largest column-top line-double-space line-space headchar-capital column-bottom above-double-space above-line-space
Author	Kristina Toutanova	page=0 xpos=1 ypos=0 left-column left-indent right-indent font-largest column-top line-double-space line-space headchar-capital above-line-space
B-Affiliation	Computer Science Dept.	page=0 xpos=1 ypos=1 left-column left-indent right-indent line-space headchar-capital tailchar-period above-line-space
I-Affiliation	Stanford University	page=0 xpos=2 ypos=1 left-column left-indent right-indent indented-line shorter-tail line-space headchar-capital above-line-space
Address	Stanford, CA 94305-9040	page=0 xpos=1 ypos=1 left-column left-indent right-indent hanged-line longer-tail line-space headchar-capital above-line-space
Email	kristina@cs.stanford.edu	page=0 xpos=1 ypos=1 left-column left-indent right-indent font-smallest indented-line shorter-tail line-space symbol-atmark headchar-lower above-blank-line above-double-space above-line-space
Author	Christopher D. Manning	page=0 xpos=1 ypos=2 left-column left-indent right-indent font-largest hanged-line longer-tail line-blank-line line-double-space line-space headchar-capital above-line-space
B-Affiliation	Computer Science Dept.	page=0 xpos=1 ypos=2 left-column left-indent right-indent indented-line shorter-tail line-space headchar-capital tailchar-period above-line-space
I-Affiliation	Stanford University	page=0 xpos=2 ypos=2 left-column left-indent right-indent indented-line shorter-tail line-space headchar-capital above-line-space
Address	Stanford, CA 94305-9040	page=0 xpos=1 ypos=3 left-column left-indent right-indent hanged-line longer-tail line-space headchar-capital above-line-space
Email	manning@stanford.edu	page=0 xpos=1 ypos=3 left-column left-indent right-indent font-smallest indented-line shorter-tail line-space symbol-atmark headchar-lower above-blank-line above-double-space above-line-space
AbstractHeader	Abstract	page=0 xpos=1 ypos=3 left-column centered left-indent right-indent font-largest shorter-tail line-blank-line line-double-space line-space string-abstract headchar-capital above-blank-line above-double-space above-line-space
B-Abstract	We present a new part-of-speech tagger that	page=0 xpos=0 ypos=4 left-column centered left-indent right-indent hanged-line longer-tail line-blank-line line-double-space line-space headchar-capital
I-Abstract	demonstrates the following ideas: (i) explicit	page=0 xpos=0 ypos=4 left-column centered left-indent right-indent aligned-line headchar-lower
I-Abstract	use of both preceding and following tag con-	page=0 xpos=0 ypos=4 left-column centered left-indent right-indent aligned-line headchar-lower tailchar-hiphen
I-Abstract	texts via a dependency network representa-	page=0 xpos=0 ypos=4 left-column centered left-indent right-indent aligned-line headchar-lower tailchar-hiphen
I-Abstract	tion, (ii) broad use of lexical features, includ-	page=0 xpos=0 ypos=4 left-column centered left-indent right-indent aligned-line headchar-lower tailchar-hiphen
I-Abstract	ing jointly conditioning on multiple consecu-	page=0 xpos=0 ypos=4 left-column centered left-indent right-indent aligned-line headchar-lower tailchar-hiphen
I-Abstract	tive words, (iii) effective use of priors in con-	page=0 xpos=0 ypos=5 left-column centered left-indent right-indent aligned-line headchar-lower tailchar-hiphen
I-Abstract	ditional loglinear models, and (iv) fine-grained	page=0 xpos=0 ypos=5 left-column centered left-indent right-indent aligned-line headchar-lower
I-Abstract	modeling of unknown word features. Using	page=0 xpos=0 ypos=5 left-column centered left-indent right-indent aligned-line headchar-lower
I-Abstract	these ideas together, the resulting tagger gives	page=0 xpos=0 ypos=5 left-column centered left-indent right-indent aligned-line headchar-lower
I-Abstract	a 97.24% accuracy on the Penn Treebank WSJ,	page=0 xpos=0 ypos=5 left-column centered left-indent right-indent aligned-line itemization headchar-lower tailchar-comma
I-Abstract	an error reduction of 4.4% on the best previous	page=0 xpos=0 ypos=5 left-column centered left-indent right-indent aligned-line headchar-lower
I-Abstract	single automatically learned tagging result.	page=0 xpos=0 ypos=6 left-column left-indent right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-SectionHeader	1 Introduction	page=0 xpos=0 ypos=6 left-column right-indent font-largest hanged-line shorter-tail line-blank-line line-double-space line-space numbered-heading1 above-double-space above-line-space
B-Body	Almost all approaches to sequence problems such as part-	page=0 xpos=0 ypos=6 left-column full-justified aligned-line longer-tail line-double-space line-space headchar-capital tailchar-hiphen
I-Body	of-speech tagging take a unidirectional approach to con-	page=0 xpos=0 ypos=7 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ditioning inference along the sequence. Regardless of	page=0 xpos=0 ypos=7 left-column full-justified aligned-line headchar-lower
I-Body	whether one is using HMMs, maximum entropy condi-	page=0 xpos=0 ypos=7 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tional sequence models, or other techniques like decision	page=0 xpos=0 ypos=7 left-column full-justified aligned-line headchar-lower
I-Body	trees, most systems work in one direction through the	page=0 xpos=0 ypos=7 left-column full-justified aligned-line headchar-lower
I-Body	sequence (normally left to right, but occasionally right	page=0 xpos=0 ypos=7 left-column full-justified aligned-line headchar-lower
I-Body	to left, e.g., Church (1988)). There are a few excep-	page=0 xpos=0 ypos=8 left-column full-justified aligned-line year headchar-lower tailchar-hiphen
I-Body	tions, such as Brill’s transformation-based learning (Brill,	page=0 xpos=0 ypos=8 left-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	1995), but most of the best known and most successful	page=0 xpos=0 ypos=8 left-column full-justified aligned-line year
E-Body	approaches of recent years have been unidirectional.	page=0 xpos=0 ypos=8 left-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	Most sequence models can be seen as chaining to-	page=0 xpos=0 ypos=8 left-column left-indent indented-line longer-tail headchar-capital tailchar-hiphen
I-Body	gether the scores or decisions from successive local mod-	page=0 xpos=0 ypos=8 left-column full-justified hanged-line headchar-lower tailchar-hiphen
I-Body	els to form a global model for an entire sequence. Clearly	page=0 xpos=0 ypos=9 left-column full-justified aligned-line headchar-lower
I-Body	the identity of a tag is correlated with both past and future	page=0 xpos=0 ypos=9 left-column full-justified aligned-line headchar-lower
I-Body	tags’ identities. However, in the unidirectional (causal)	page=0 xpos=0 ypos=9 left-column full-justified aligned-line headchar-lower
I-Body	case, only one direction of influence is explicitly consid-	page=0 xpos=0 ypos=9 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ered at each local point. For example, in a left-to-right	page=0 xpos=0 ypos=9 left-column full-justified aligned-line headchar-lower column-bottom
Author	Dan Klein	page=0 xpos=6 ypos=0 right-column left-indent right-indent font-largest column-top headchar-capital above-line-space
B-Affiliation	Computer Science Dept.	page=0 xpos=6 ypos=1 right-column left-indent right-indent hanged-line longer-tail line-space headchar-capital tailchar-period above-line-space
I-Affiliation	Stanford University	page=0 xpos=6 ypos=1 right-column left-indent right-indent indented-line shorter-tail line-space headchar-capital above-line-space
Address	Stanford, CA 94305-9040	page=0 xpos=6 ypos=1 right-column left-indent right-indent hanged-line longer-tail line-space headchar-capital above-line-space
Email	klein@cs.stanford.edu	page=0 xpos=6 ypos=1 right-column left-indent right-indent font-smallest indented-line shorter-tail line-space symbol-atmark headchar-lower above-blank-line above-double-space above-line-space
Author	Yoram Singer	page=0 xpos=6 ypos=2 right-column left-indent right-indent font-largest indented-line shorter-tail line-blank-line line-double-space line-space headchar-capital above-line-space
B-Affiliation	School of Computer Science	page=0 xpos=6 ypos=2 right-column left-indent right-indent hanged-line longer-tail line-space headchar-capital above-line-space
I-Affiliation	The Hebrew University	page=0 xpos=6 ypos=2 right-column left-indent right-indent indented-line shorter-tail line-space headchar-capital above-line-space
Address	Jerusalem 91904, Israel	page=0 xpos=6 ypos=3 right-column left-indent right-indent aligned-line line-space year headchar-capital above-line-space
Email	singer@cs.huji.ac.il	page=0 xpos=6 ypos=3 right-column left-indent right-indent font-smallest indented-line shorter-tail line-space symbol-atmark headchar-lower above-blank-line above-double-space above-line-space
I-Body	first-order HMM, the current tag t <sub>0</sub> is predicted based on	page=0 xpos=5 ypos=3 right-column full-justified font-largest hanged-line longer-tail line-blank-line line-double-space line-space headchar-lower
I-Body	the previous tag t <sub>−1</sub> (and the current word). <sup>1</sup> The back-	page=0 xpos=5 ypos=3 right-column full-justified font-largest aligned-line headchar-lower tailchar-hiphen
I-Body	ward interaction between t <sub>0</sub> and the next tag t +1 shows	page=0 xpos=5 ypos=4 right-column full-justified font-largest aligned-line headchar-lower
I-Body	up implicitly later, when t <sub>+1</sub> is generated in turn. While	page=0 xpos=5 ypos=4 right-column full-justified font-larger aligned-line headchar-lower
I-Body	unidirectional models are therefore able to capture both	page=0 xpos=5 ypos=4 right-column full-justified aligned-line headchar-lower
I-Body	directions of influence, there are good reasons for sus-	page=0 xpos=5 ypos=4 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	pecting that it would be advantageous to make informa-	page=0 xpos=5 ypos=4 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tion from both directions explicitly available for condi-	page=0 xpos=5 ypos=4 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tioning at each local point in the model: (i) because of	page=0 xpos=5 ypos=5 right-column full-justified aligned-line headchar-lower
I-Body	smoothing and interactions with other modeled features,	page=0 xpos=5 ypos=5 right-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	terms like P(t <sub>0</sub> |t +1 , . . .) might give a sharp estimate of t 0	page=0 xpos=5 ypos=5 right-column full-justified font-largest aligned-line headchar-lower
I-Body	even when terms like P(t <sub>+1</sub> |t <sub>0</sub> , . . .) do not, and (ii) jointly	page=0 xpos=5 ypos=5 right-column full-justified font-largest aligned-line headchar-lower
I-Body	considering the left and right context together might be	page=0 xpos=5 ypos=5 right-column full-justified aligned-line headchar-lower
I-Body	especially revealing. In this paper we exploit this idea,	page=0 xpos=5 ypos=5 right-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	using dependency networks, with a series of local con-	page=0 xpos=5 ypos=6 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ditional loglinear (aka maximum entropy or multiclass	page=0 xpos=5 ypos=6 right-column full-justified aligned-line headchar-lower
I-Body	logistic regression) models as one way of providing ef-	page=0 xpos=5 ypos=6 right-column full-justified aligned-line headchar-lower tailchar-hiphen
E-Body	ficient bidirectional inference.	page=0 xpos=5 ypos=6 right-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-line-space
B-Body	Secondly, while all taggers use lexical information,	page=0 xpos=5 ypos=6 right-column left-indent indented-line longer-tail line-space headchar-capital tailchar-comma
I-Body	and, indeed, it is well-known that lexical probabilities	page=0 xpos=5 ypos=6 right-column full-justified hanged-line headchar-lower
I-Body	are much more revealing than tag sequence probabilities	page=0 xpos=5 ypos=7 right-column full-justified aligned-line headchar-lower
I-Body	(Charniak et al., 1993), most taggers make quite limited	page=0 xpos=5 ypos=7 right-column full-justified aligned-line year
I-Body	use of lexical probabilities (compared with, for example,	page=0 xpos=5 ypos=7 right-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	the bilexical probabilities commonly used in current sta-	page=0 xpos=5 ypos=7 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tistical parsers). While modern taggers may be more prin-	page=0 xpos=5 ypos=7 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	cipled than the classic CLAWS tagger (Marshall, 1987),	page=0 xpos=5 ypos=8 right-column full-justified aligned-line year headchar-lower tailchar-comma
I-Body	they are in some respects inferior in their use of lexical	page=0 xpos=5 ypos=8 right-column full-justified aligned-line headchar-lower
I-Body	information: CLAWS, through its IDIOMTAG module,	page=0 xpos=5 ypos=8 right-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	categorically captured many important, correct taggings	page=0 xpos=5 ypos=8 right-column full-justified aligned-line headchar-lower
I-Body	of frequent idiomatic word sequences. In this work, we	page=0 xpos=5 ypos=8 right-column full-justified aligned-line headchar-lower
I-Body	incorporate appropriate multiword feature templates so	page=0 xpos=5 ypos=8 right-column full-justified aligned-line headchar-lower
I-Body	that such facts can be learned and used automatically by	page=0 xpos=5 ypos=9 right-column full-justified aligned-line headchar-lower above-double-space above-line-space
B-Footnote	1 Rather than subscripting all variables with a position index,	page=0 xpos=5 ypos=9 right-column left-indent font-largest indented-line line-double-space line-space numbered-heading1 tailchar-comma
I-Footnote	we use a hopefully clearer relative notation, where t 0 denotes	page=0 xpos=5 ypos=9 right-column full-justified font-larger hanged-line headchar-lower
I-Footnote	the current position and t −n and t +n are left and right context	page=0 xpos=5 ypos=9 right-column full-justified aligned-line headchar-lower
I-Footnote	tags, and similarly for words.	page=0 xpos=5 ypos=9 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period page-bottom
Figure	__Figure 1__	page=1 xpos=0 ypos=-1 left-column centered left-indent right-indent box page-top figure-area above-blank-line above-double-space above-line-space
B-Caption	Figure 1: Dependency networks: (a) the (standard) left-to-right	page=1 xpos=0 ypos=3 left-column full-justified font-smallest hanged-line longer-tail line-blank-line line-double-space line-space string-figure headchar-capital
I-Caption	first-order CMM, (b) the (reversed) right-to-left CMM, and (c)	page=1 xpos=0 ypos=3 left-column full-justified font-smallest aligned-line headchar-lower
E-Caption	the bidirectional dependency network.	page=1 xpos=0 ypos=3 left-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
E-Body	the model.	page=1 xpos=0 ypos=3 left-column right-indent aligned-line shorter-tail line-double-space line-space headchar-lower tailchar-period above-line-space
B-Body	Having expressive templates leads to a large number	page=1 xpos=0 ypos=3 left-column left-indent indented-line longer-tail line-space headchar-capital
I-Body	of features, but we show that by suitable use of a prior	page=1 xpos=0 ypos=4 left-column full-justified hanged-line headchar-lower
I-Body	(i.e., regularization) in the conditional loglinear model –	page=1 xpos=0 ypos=4 left-column full-justified aligned-line
I-Body	something not used by previous maximum entropy tag-	page=1 xpos=0 ypos=4 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	gers – many such features can be added with an overall	page=1 xpos=0 ypos=4 left-column full-justified aligned-line headchar-lower
I-Body	positive effect on the model. Indeed, as for the voted per-	page=1 xpos=0 ypos=4 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ceptron of Collins (2002), we can get performance gains	page=1 xpos=0 ypos=5 left-column full-justified aligned-line year headchar-lower
I-Body	by reducing the support threshold for features to be in-	page=1 xpos=0 ypos=5 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	cluded in the model. Combining all these ideas, together	page=1 xpos=0 ypos=5 left-column full-justified aligned-line headchar-lower
I-Body	with a few additional handcrafted unknown word fea-	page=1 xpos=0 ypos=5 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tures, gives us a part-of-speech tagger with a per-position	page=1 xpos=0 ypos=5 left-column full-justified aligned-line headchar-lower
I-Body	tag accuracy of 97.24%, and a whole-sentence correct	page=1 xpos=0 ypos=5 left-column full-justified aligned-line headchar-lower
I-Body	rate of 56.34% on Penn Treebank WSJ data. This is the	page=1 xpos=0 ypos=6 left-column full-justified aligned-line headchar-lower
I-Body	best automatically learned part-of-speech tagging result	page=1 xpos=0 ypos=6 left-column full-justified aligned-line headchar-lower
I-Body	known to us, representing an error reduction of 4.4% on	page=1 xpos=0 ypos=6 left-column full-justified aligned-line headchar-lower
I-Body	the model presented in Collins (2002), using the same	page=1 xpos=0 ypos=6 left-column full-justified aligned-line year headchar-lower
I-Body	data splits, and a larger error reduction of 12.1% from the	page=1 xpos=0 ypos=6 left-column full-justified aligned-line headchar-lower
I-Body	more similar best previous loglinear model in Toutanova	page=1 xpos=0 ypos=7 left-column full-justified aligned-line headchar-lower
E-Body	and Manning (2000).	page=1 xpos=0 ypos=7 left-column right-indent aligned-line shorter-tail year headchar-lower tailchar-period above-double-space above-line-space
B-SectionHeader	2 Bidirectional Dependency Networks	page=1 xpos=0 ypos=7 left-column right-indent font-largest aligned-line longer-tail line-double-space line-space numbered-heading1 above-double-space above-line-space
B-Body	When building probabilistic models for tag sequences,	page=1 xpos=0 ypos=7 left-column full-justified aligned-line longer-tail line-double-space line-space headchar-capital tailchar-comma
I-Body	we often decompose the global probability of sequences	page=1 xpos=0 ypos=8 left-column full-justified aligned-line headchar-lower
I-Body	using a directed graphical model (e.g., an HMM (Brants,	page=1 xpos=0 ypos=8 left-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	2000) or a conditional Markov model (CMM) (Ratna-	page=1 xpos=0 ypos=8 left-column full-justified aligned-line year tailchar-hiphen
I-Body	parkhi, 1996)). In such models, the probability assigned	page=1 xpos=0 ypos=8 left-column full-justified aligned-line year headchar-lower
I-Body	to a tagged sequence of words x = ht, wi is the product	page=1 xpos=0 ypos=8 left-column full-justified aligned-line headchar-lower
I-Body	of a sequence of local portions of the graphical model,	page=1 xpos=0 ypos=9 left-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	one from each time slice. For example, in the left-to-right	page=1 xpos=0 ypos=9 left-column full-justified aligned-line headchar-lower
I-Body	CMM shown in figure 1(a),	page=1 xpos=0 ypos=9 left-column right-indent aligned-line shorter-tail headchar-capital tailchar-comma above-line-space
B-Equation	P(t, w) = Y <sub>i</sub> P(t i |t i−1 , w i )	page=1 xpos=1 ypos=9 left-column centered left-indent right-indent font-largest indented-line longer-tail line-space headchar-capital column-bottom
B-Body	That is, the replicated structure is a local model	page=1 xpos=5 ypos=0 right-column full-justified column-top headchar-capital
I-Body	P(t <sub>0</sub> |t −1 , w 0 ). <sup>2</sup> Of course, if there are too many con-	page=1 xpos=5 ypos=0 right-column centered left-indent font-largest headchar-capital tailchar-hiphen
I-Body	ditioned quantities, these local models may have to be	page=1 xpos=5 ypos=0 right-column full-justified headchar-lower
I-Body	estimated in some sophisticated way; it is typical in tag-	page=1 xpos=5 ypos=0 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ging to populate these models with little maximum en-	page=1 xpos=5 ypos=0 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tropy models. For example, we might populate a model	page=1 xpos=5 ypos=0 right-column full-justified aligned-line headchar-lower
I-Body	for P(t <sub>0</sub> |t −1 , w 0 ) with a maxent model of the form:	page=1 xpos=5 ypos=1 right-column right-indent font-largest aligned-line shorter-tail headchar-lower tailchar-colon above-double-space above-line-space
B-Equation	P λ (t <sub>0</sub> |t −1 , w 0 ) = P <sup>exp(λ</sup> t 0 <sub>0</sub> exp(λ ht 0 ,t ht −1 0 0 ,t i −1 <sup>+</sup> i λ + ht λ 0 ,w ht 0 0 0 ,w i ) 0 i )	page=1 xpos=5 ypos=1 right-column centered left-indent right-indent box indented-line line-double-space line-space headchar-capital above-double-space above-line-space
B-Body	In this case, the w <sub>0</sub> and t −1 can have joint effects on t 0 , but	page=1 xpos=5 ypos=1 right-column full-justified font-largest hanged-line longer-tail line-double-space line-space headchar-capital
I-Body	there are not joint features involving all three variables	page=1 xpos=5 ypos=2 right-column full-justified aligned-line headchar-lower
I-Body	(though there could have been such features). We say that	page=1 xpos=5 ypos=2 right-column full-justified aligned-line
I-Body	this model uses the feature templates ht <sub>0</sub> , t −1 i (previous	page=1 xpos=5 ypos=2 right-column full-justified font-largest aligned-line headchar-lower
E-Body	tag features) and ht <sub>0</sub> , w 0 i (current word features).	page=1 xpos=5 ypos=2 right-column right-indent font-largest aligned-line shorter-tail headchar-lower tailchar-period
B-Body	Clearly, both the preceding tag t <sub>−1</sub> and following tag	page=1 xpos=5 ypos=2 right-column left-indent font-larger indented-line longer-tail headchar-capital
I-Body	t <sub>+1</sub> carry useful information about a current tag t <sub>0</sub> . Unidi-	page=1 xpos=5 ypos=3 right-column full-justified font-largest hanged-line itemization headchar-lower tailchar-hiphen
I-Body	rectional models do not ignore this influence; in the case	page=1 xpos=5 ypos=3 right-column full-justified aligned-line headchar-lower
I-Body	of a left-to-right CMM, the influence of t <sub>−1</sub> on t <sub>0</sub> is ex-	page=1 xpos=5 ypos=3 right-column full-justified font-largest aligned-line headchar-lower tailchar-hiphen
I-Body	plicit in the P(t <sub>0</sub> |t −1 , w 0 ) local model, while the influ-	page=1 xpos=5 ypos=3 right-column full-justified font-largest aligned-line headchar-lower tailchar-hiphen
I-Body	ence of t <sub>+1</sub> on t <sub>0</sub> is implicit in the local model at the next	page=1 xpos=5 ypos=3 right-column full-justified font-largest aligned-line headchar-lower
I-Body	position (via P(t <sub>+1</sub> |t <sub>0</sub> , w +1 )). The situation is reversed	page=1 xpos=5 ypos=3 right-column full-justified font-largest aligned-line headchar-lower
E-Body	for the right-to-left CMM in figure 1(b).	page=1 xpos=5 ypos=4 right-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	From a seat-of-the-pants machine learning perspective,	page=1 xpos=5 ypos=4 right-column left-indent indented-line longer-tail headchar-capital tailchar-comma
I-Body	when building a classifier to label the tag at a certain posi-	page=1 xpos=5 ypos=4 right-column full-justified hanged-line headchar-lower tailchar-hiphen
I-Body	tion, the obvious thing to do is to explicitly include in the	page=1 xpos=5 ypos=4 right-column full-justified aligned-line headchar-lower
I-Body	local model all predictive features, no matter on which	page=1 xpos=5 ypos=4 right-column full-justified aligned-line headchar-lower
I-Body	side of the target position they lie. There are two good	page=1 xpos=5 ypos=5 right-column full-justified aligned-line headchar-lower
I-Body	formal reasons to expect that a model explicitly condi-	page=1 xpos=5 ypos=5 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tioning on both sides at each position, like figure 1(c)	page=1 xpos=5 ypos=5 right-column full-justified aligned-line headchar-lower
I-Body	could be advantageous. First, because of smoothing	page=1 xpos=5 ypos=5 right-column full-justified aligned-line headchar-lower
I-Body	effects and interaction with other conditioning features	page=1 xpos=5 ypos=5 right-column full-justified aligned-line headchar-lower
I-Body	(like the words), left-to-right factors like P(t <sub>0</sub> |t −1 , w 0 )	page=1 xpos=5 ypos=6 right-column full-justified font-largest aligned-line
I-Body	do not always suffice when t <sub>0</sub> is implicitly needed to de-	page=1 xpos=5 ypos=6 right-column full-justified font-largest aligned-line headchar-lower tailchar-hiphen
I-Body	termine t <sub>−1</sub> . For example, consider a case of observation	page=1 xpos=5 ypos=6 right-column full-justified font-larger aligned-line headchar-lower
I-Body	bias (Klein and Manning, 2002) for a first-order left-to-	page=1 xpos=5 ypos=6 right-column full-justified aligned-line year headchar-lower tailchar-hiphen
I-Body	right CMM. The word to has only one tag ( TO ) in the PTB	page=1 xpos=5 ypos=6 right-column full-justified aligned-line headchar-lower
I-Body	tag set. The TO tag is often preceded by nouns, but rarely	page=1 xpos=5 ypos=6 right-column full-justified aligned-line headchar-lower
I-Body	by modals ( MD ). In a sequence will to fight, that trend	page=1 xpos=5 ypos=7 right-column full-justified aligned-line headchar-lower
I-Body	indicates that will should be a noun rather than a modal	page=1 xpos=5 ypos=7 right-column full-justified aligned-line headchar-lower
I-Body	verb. However, that effect is completely lost in a CMM	page=1 xpos=5 ypos=7 right-column full-justified aligned-line headchar-lower
I-Body	like (a): P(t <sub>will</sub> |will, hstar ti) prefers the modal tagging,	page=1 xpos=5 ypos=7 right-column full-justified font-largest aligned-line headchar-lower tailchar-comma
I-Body	and P( TO |to, t <sub>will</sub> ) is roughly 1 regardless of t will . While	page=1 xpos=5 ypos=7 right-column full-justified font-largest aligned-line headchar-lower
I-Body	the model has an arrow between the two tag positions,	page=1 xpos=5 ypos=8 right-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	that path of influence is severed. <sup>3</sup> The same problem ex-	page=1 xpos=5 ypos=8 right-column full-justified font-largest aligned-line headchar-lower tailchar-hiphen
I-Body	ists in the other direction. If we use the symmetric right-	page=1 xpos=5 ypos=8 right-column full-justified aligned-line headchar-lower tailchar-hiphen above-double-space above-line-space
B-Footnote	2 Throughout this paper we assume that enough boundary	page=1 xpos=5 ypos=8 right-column left-indent font-largest indented-line line-double-space line-space numbered-heading1
I-Footnote	symbols always exist that we can ignore the differences which	page=1 xpos=5 ypos=8 right-column full-justified font-smallest hanged-line headchar-lower
I-Footnote	would otherwise exist at the initial and final few positions.	page=1 xpos=5 ypos=9 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Footnote	3 Despite use of names like “label bias” (Lafferty et al., 2001)	page=1 xpos=5 ypos=9 right-column left-indent font-largest indented-line longer-tail numbered-heading1 year
I-Footnote	or “observation bias”, these effects are really just unwanted	page=1 xpos=5 ypos=9 right-column full-justified font-smallest hanged-line headchar-lower
I-Footnote	explaining-away effects (Cowell et al., 1999, 19), where two	page=1 xpos=5 ypos=9 right-column full-justified font-smallest aligned-line year headchar-lower
I-Footnote	nodes which are not actually in causal competition have been	page=1 xpos=5 ypos=9 right-column full-justified font-smallest aligned-line headchar-lower
I-Footnote	modeled as if they were.	page=1 xpos=5 ypos=9 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period page-bottom
Figure	__Figure 2__	page=2 xpos=0 ypos=0 left-column left-indent right-indent box page-top figure-area above-double-space above-line-space
B-Caption	Figure 2: Simple dependency nets: (a) the Bayes’ net for	page=2 xpos=0 ypos=0 left-column full-justified font-smallest hanged-line longer-tail line-double-space line-space string-figure headchar-capital
I-Caption	P(A)P(B|A), (b) the Bayes’ net for P(A|B)P(B), (c) a bidi-	page=2 xpos=0 ypos=0 left-column centered left-indent font-smallest headchar-capital tailchar-hiphen
I-Caption	rectional net with models of P(A|B) and P(B|A), which is not	page=2 xpos=0 ypos=1 left-column full-justified font-smallest headchar-lower
E-Caption	a Bayes’ net.	page=2 xpos=0 ypos=1 left-column right-indent font-smallest aligned-line shorter-tail itemization headchar-lower tailchar-period above-blank-line above-double-space above-line-space
I-Body	to-left model, fight will receive its more common noun	page=2 xpos=0 ypos=1 left-column full-justified aligned-line longer-tail line-blank-line line-double-space line-space headchar-lower
I-Body	tagging by symmetric reasoning. However, the bidirec-	page=2 xpos=0 ypos=1 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tional model (c) discussed in the next section makes both	page=2 xpos=0 ypos=1 left-column full-justified aligned-line headchar-lower
I-Body	directions available for conditioning at all locations, us-	page=2 xpos=0 ypos=2 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ing replicated models of P(t <sub>0</sub> |t −1 , t +1 , w 0 ), and will be	page=2 xpos=0 ypos=2 left-column full-justified font-largest aligned-line headchar-lower
E-Body	able to get this example correct. <sup>4</sup>	page=2 xpos=0 ypos=2 left-column right-indent font-largest aligned-line shorter-tail headchar-lower above-blank-line above-double-space above-line-space
B-SubsectionHeader	2.1 Semantics of Dependency Networks	page=2 xpos=0 ypos=2 left-column right-indent aligned-line longer-tail line-blank-line line-double-space line-space numbered-heading2 above-double-space above-line-space
B-Body	While the structures in figure 1(a) and (b) are well-	page=2 xpos=0 ypos=3 left-column full-justified aligned-line longer-tail line-double-space line-space headchar-capital tailchar-hiphen
I-Body	understood graphical models with well-known semantics,	page=2 xpos=0 ypos=3 left-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	figure 1(c) is not a standard Bayes’ net, precisely because	page=2 xpos=0 ypos=3 left-column full-justified aligned-line headchar-lower
I-Body	the graph has cycles. Rather, it is a more general de-	page=2 xpos=0 ypos=3 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	pendency network (Heckerman et al., 2000). Each node	page=2 xpos=0 ypos=3 left-column full-justified aligned-line year headchar-lower
I-Body	represents a random variable along with a local condi-	page=2 xpos=0 ypos=4 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tional probability model of that variable, conditioned on	page=2 xpos=0 ypos=4 left-column full-justified aligned-line headchar-lower
I-Body	the source variables of all incoming arcs. In this sense,	page=2 xpos=0 ypos=4 left-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	the semantics are the same as for standard Bayes’ nets.	page=2 xpos=0 ypos=4 left-column full-justified aligned-line headchar-lower tailchar-period
I-Body	However, because the graph is cyclic, the net does not	page=2 xpos=0 ypos=4 left-column full-justified aligned-line headchar-capital
I-Body	correspond to a proper factorization of a large joint prob-	page=2 xpos=0 ypos=5 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ability estimate into local conditional factors. Consider	page=2 xpos=0 ypos=5 left-column full-justified aligned-line headchar-lower
I-Body	the two-node cases shown in figure 2. Formally, for the	page=2 xpos=0 ypos=5 left-column full-justified aligned-line headchar-lower
I-Body	net in (a), we can write P(a, b) = P(a)P(b|a). For (b)	page=2 xpos=0 ypos=5 left-column full-justified aligned-line headchar-lower
I-Body	we write P(a, b) = P(b)P(a|b). However, in (c), the	page=2 xpos=0 ypos=5 left-column full-justified aligned-line headchar-lower
I-Body	nodes A and B carry the information P(a|b) and P(b|a)	page=2 xpos=0 ypos=5 left-column full-justified aligned-line headchar-lower
I-Body	respectively. The chain rule doesn’t allow us to recon-	page=2 xpos=0 ypos=6 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	struct P(a, b) by multiplying these two quantities. Un-	page=2 xpos=0 ypos=6 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	der appropriate conditions, we could reconstruct P(a, b)	page=2 xpos=0 ypos=6 left-column full-justified aligned-line headchar-lower
I-Body	from these quantities using Gibbs sampling, and, in gen-	page=2 xpos=0 ypos=6 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	eral, that is the best we can do. However, while recon-	page=2 xpos=0 ypos=6 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	structing the joint probabilities from these local condi-	page=2 xpos=0 ypos=7 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tional probabilities may be difficult, estimating the local	page=2 xpos=0 ypos=7 left-column full-justified aligned-line headchar-lower
I-Body	probabilities themselves is no harder than it is for acyclic	page=2 xpos=0 ypos=7 left-column full-justified aligned-line headchar-lower
I-Body	models: we take observations of the local environments	page=2 xpos=0 ypos=7 left-column full-justified aligned-line headchar-lower
I-Body	and use any maximum likelihood estimation method we	page=2 xpos=0 ypos=7 left-column full-justified aligned-line headchar-lower
I-Body	desire. In our experiments, we used local maxent models,	page=2 xpos=0 ypos=8 left-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	but if the event space allowed, (smoothed) relative counts	page=2 xpos=0 ypos=8 left-column full-justified aligned-line headchar-lower
E-Body	would do.	page=2 xpos=0 ypos=8 left-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-blank-line above-double-space above-line-space
B-Footnote	4 The effect of indirect influence being weaker than direct in-	page=2 xpos=0 ypos=8 left-column left-indent font-largest indented-line longer-tail line-blank-line line-double-space line-space numbered-heading1 tailchar-hiphen
I-Footnote	fluence is more pronounced for conditionally structured models,	page=2 xpos=0 ypos=8 left-column full-justified font-smallest hanged-line headchar-lower tailchar-comma
I-Footnote	but is potentially an issue even with a simple HMM. The prob-	page=2 xpos=0 ypos=9 left-column full-justified font-smallest aligned-line headchar-lower tailchar-hiphen
I-Footnote	abilistic models for basic left-to-right and right-to-left HMMs	page=2 xpos=0 ypos=9 left-column full-justified font-smallest aligned-line headchar-lower
I-Footnote	with emissions on their states can be shown to be equivalent us-	page=2 xpos=0 ypos=9 left-column full-justified font-smallest aligned-line headchar-lower tailchar-hiphen
I-Footnote	ing Bayes’ rule on the transitions, provided start and end sym-	page=2 xpos=0 ypos=9 left-column full-justified font-smallest aligned-line headchar-lower tailchar-hiphen
I-Footnote	bols are modeled. However, this equivalence is violated in prac-	page=2 xpos=0 ypos=9 left-column full-justified font-smallest aligned-line headchar-lower tailchar-hiphen
I-Footnote	tice by the addition of smoothing.	page=2 xpos=0 ypos=9 left-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period column-bottom
B-Figure	function bestScore()	page=2 xpos=5 ypos=0 right-column right-indent font-smallest column-top headchar-lower
I-Figure	return bestScoreSub(n + 2, hend, end, endi);	page=2 xpos=5 ypos=0 right-column left-indent right-indent font-smallest indented-line longer-tail headchar-lower tailchar-semicolon above-double-space above-line-space
I-Figure	function bestScoreSub(i + 1, ht i−1 , t i , t i+1 i)	page=2 xpos=5 ypos=0 right-column right-indent font-larger hanged-line shorter-tail line-double-space line-space headchar-lower
I-Figure	% memoization	page=2 xpos=5 ypos=0 right-column left-indent right-indent font-smallest indented-line shorter-tail
I-Figure	if (cached(i + 1, ht i−1 , t i , t i+1 i))	page=2 xpos=5 ypos=0 right-column left-indent right-indent font-larger aligned-line longer-tail headchar-lower
I-Figure	return cache(i + 1, ht i−1 , t i , t i+1 i);	page=2 xpos=5 ypos=0 right-column left-indent right-indent font-larger indented-line longer-tail headchar-lower tailchar-semicolon
I-Figure	% left boundary case	page=2 xpos=5 ypos=1 right-column left-indent right-indent font-smallest hanged-line shorter-tail
I-Figure	if (i = −1)	page=2 xpos=5 ypos=1 right-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower
I-Figure	if (ht i−1 , t i , t i+1 i == hstar t, star t, star ti)	page=2 xpos=5 ypos=1 right-column left-indent right-indent font-larger indented-line longer-tail headchar-lower
I-Figure	return 1;	page=2 xpos=5 ypos=1 right-column left-indent right-indent font-smallest indented-line shorter-tail headchar-lower tailchar-semicolon
I-Figure	else	page=2 xpos=5 ypos=1 right-column left-indent right-indent font-smallest hanged-line shorter-tail headchar-lower
I-Figure	return 0;	page=2 xpos=5 ypos=1 right-column left-indent right-indent font-smallest indented-line longer-tail headchar-lower tailchar-semicolon
I-Figure	% recursive case	page=2 xpos=5 ypos=1 right-column left-indent right-indent font-smallest hanged-line longer-tail
I-Figure	return max t <sub>i−2</sub> bestScoreSub(i, ht i−2 , t i−1 , t i i)×	page=2 xpos=5 ypos=2 right-column left-indent right-indent font-largest aligned-line longer-tail headchar-lower
E-Figure	P(t i |t <sub>i−1</sub> , t i+1 , w i );	page=2 xpos=6 ypos=2 right-column left-indent right-indent font-larger indented-line shorter-tail headchar-capital tailchar-semicolon above-double-space above-line-space
B-Caption	Figure 3: Pseudocode for polynomial inference for the first-	page=2 xpos=5 ypos=2 right-column full-justified font-smallest hanged-line longer-tail line-double-space line-space string-figure headchar-capital tailchar-hiphen
E-Caption	order bidirectional CMM (memoized version).	page=2 xpos=5 ypos=2 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-SubsectionHeader	2.2 Inference for Linear Dependency Networks	page=2 xpos=5 ypos=3 right-column right-indent aligned-line longer-tail line-double-space line-space numbered-heading2 above-line-space
B-Body	Cyclic or not, we can view the product of local probabil-	page=2 xpos=5 ypos=3 right-column full-justified aligned-line longer-tail line-space headchar-capital tailchar-hiphen
I-Body	ities from a dependency network as a score:	page=2 xpos=5 ypos=3 right-column right-indent aligned-line shorter-tail headchar-lower tailchar-colon
B-Equation	scor e(x) = Y <sub>i</sub> P(x i |Pa(x i ))	page=2 xpos=6 ypos=3 right-column centered left-indent right-indent font-largest indented-line headchar-lower above-double-space above-line-space
I-Body	where Pa(x <sub>i</sub> ) are the nodes with arcs to the node x i . In the	page=2 xpos=5 ypos=4 right-column full-justified font-largest hanged-line longer-tail line-double-space line-space headchar-lower
I-Body	case of an acyclic model, this score will be the joint prob-	page=2 xpos=5 ypos=4 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ability of the event x, P(x). In the general case, it will not	page=2 xpos=5 ypos=4 right-column full-justified aligned-line headchar-lower
I-Body	be. However, we can still ask for the event, in this case the	page=2 xpos=5 ypos=4 right-column full-justified aligned-line headchar-lower
I-Body	tag sequence, with the highest score. For dependency net-	page=2 xpos=5 ypos=4 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	works like those in figure 1, an adaptation of the Viterbi	page=2 xpos=5 ypos=5 right-column full-justified aligned-line headchar-lower
I-Body	algorithm can be used to find the maximizing sequence	page=2 xpos=5 ypos=5 right-column full-justified aligned-line headchar-lower
I-Body	in polynomial time. Figure 3 gives pseudocode for the	page=2 xpos=5 ypos=5 right-column full-justified aligned-line headchar-lower
I-Body	concrete case of the network in figure 1(d); the general	page=2 xpos=5 ypos=5 right-column full-justified aligned-line headchar-lower
I-Body	case is similar, and is in fact just a max-plus version of	page=2 xpos=5 ypos=5 right-column full-justified aligned-line headchar-lower
I-Body	standard inference algorithms for Bayes’ nets (Cowell et	page=2 xpos=5 ypos=6 right-column full-justified aligned-line headchar-lower
I-Body	al., 1999, 97). In essence, there is no difference between	page=2 xpos=5 ypos=6 right-column full-justified aligned-line year headchar-lower
I-Body	inference on this network and a second-order left-to-right	page=2 xpos=5 ypos=6 right-column full-justified aligned-line headchar-lower
I-Body	CMM or HMM. The only difference is that, when the	page=2 xpos=5 ypos=6 right-column full-justified aligned-line headchar-capital
I-Body	Markov window is at a position i , rather than receiving	page=2 xpos=5 ypos=6 right-column full-justified aligned-line headchar-capital
I-Body	the score for P(t <sub>i</sub> |t i−1 , t i−2 , w i ), one receives the score	page=2 xpos=5 ypos=6 right-column full-justified font-largest aligned-line headchar-lower
E-Body	for P(t <sub>i−1</sub> |t i , t i−2 , w i−1 ).	page=2 xpos=5 ypos=7 right-column right-indent font-largest aligned-line shorter-tail headchar-lower tailchar-period
B-Body	There are some foundational issues worth mention-	page=2 xpos=5 ypos=7 right-column left-indent indented-line longer-tail headchar-capital tailchar-hiphen
I-Body	ing. As discussed previously, the maximum scoring se-	page=2 xpos=5 ypos=7 right-column full-justified hanged-line headchar-lower tailchar-hiphen
I-Body	quence need not be the sequence with maximum likeli-	page=2 xpos=5 ypos=7 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	hood according to the model. There is therefore a worry	page=2 xpos=5 ypos=7 right-column full-justified aligned-line headchar-lower
I-Body	with these models about a kind of “collusion” where the	page=2 xpos=5 ypos=8 right-column full-justified aligned-line headchar-lower
I-Body	model locks onto conditionally consistent but jointly un-	page=2 xpos=5 ypos=8 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	likely sequences. Consider the two-node network in fig-	page=2 xpos=5 ypos=8 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ure 2(c). If we have the following distribution of ob-	page=2 xpos=5 ypos=8 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	servations (in the form ab) h11, 11, 11, 12, 21, 33i, then	page=2 xpos=5 ypos=8 right-column full-justified aligned-line headchar-lower
I-Body	clearly the most likely state of the network is 11. How-	page=2 xpos=5 ypos=8 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ever, the score of 11 is P(a = 1|b = 1)P(b = 1|a = 1)	page=2 xpos=5 ypos=9 right-column full-justified aligned-line headchar-lower
I-Body	= 3/4 × 3/4 = 9/16, while the score of 33 is 1. An ad-	page=2 xpos=5 ypos=9 right-column full-justified aligned-line tailchar-hiphen
I-Body	ditional related problem is that the training set loss (sum	page=2 xpos=5 ypos=9 right-column full-justified aligned-line headchar-lower
I-Body	of negative logarithms of the sequence scores) does not	page=2 xpos=5 ypos=9 right-column full-justified aligned-line headchar-lower
I-Body	bound the training set error (0/1 loss on sequences) from	page=2 xpos=5 ypos=9 right-column full-justified aligned-line headchar-lower page-bottom
Table	__Table 1__	page=3 xpos=-1 ypos=-1 left-column left-over right-indent box page-top table-area above-double-space above-line-space
B-Caption	Table 1: Data set splits used.	page=3 xpos=1 ypos=0 left-column centered left-indent right-indent font-smallest indented-line shorter-tail line-double-space line-space string-table headchar-capital tailchar-period above-blank-line above-double-space above-line-space
I-Body	above. Consider the following training set, for the same	page=3 xpos=0 ypos=1 left-column full-justified hanged-line longer-tail line-blank-line line-double-space line-space headchar-lower
I-Body	network, with each entire data point considered as a label:	page=3 xpos=0 ypos=1 left-column full-justified aligned-line headchar-lower tailchar-colon
I-Body	h11, 22i. The relative-frequency model assigns loss 0 to	page=3 xpos=0 ypos=1 left-column full-justified aligned-line headchar-lower
I-Body	both training examples, but cannot do better than 50%	page=3 xpos=0 ypos=1 left-column full-justified aligned-line headchar-lower
I-Body	error in regenerating the training data labels. These is-	page=3 xpos=0 ypos=2 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	sues are further discussed in Heckerman et al. (2000).	page=3 xpos=0 ypos=2 left-column full-justified aligned-line year headchar-lower tailchar-period
I-Body	Preliminary work of ours suggests that practical use of	page=3 xpos=0 ypos=2 left-column full-justified aligned-line headchar-capital
I-Body	dependency networks is not in general immune to these	page=3 xpos=0 ypos=2 left-column full-justified aligned-line headchar-lower
I-Body	theoretical concerns: a dependency network can choose a	page=3 xpos=0 ypos=2 left-column full-justified aligned-line headchar-lower
I-Body	sequence model that is bidirectionally very consistent but	page=3 xpos=0 ypos=2 left-column full-justified aligned-line headchar-lower
I-Body	does not match the data very well. However, this problem	page=3 xpos=0 ypos=3 left-column full-justified aligned-line headchar-lower
I-Body	does not appear to have prevented the networks from per-	page=3 xpos=0 ypos=3 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	forming well on the tagging problem, probably because	page=3 xpos=0 ypos=3 left-column full-justified aligned-line headchar-lower
I-Body	features linking tags and observations are generally much	page=3 xpos=0 ypos=3 left-column full-justified aligned-line headchar-lower
E-Body	sharper discriminators than tag sequence features.	page=3 xpos=0 ypos=3 left-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-line-space
B-Body	It is useful to contrast this framework with the con-	page=3 xpos=0 ypos=4 left-column left-indent indented-line longer-tail line-space headchar-capital tailchar-hiphen
I-Body	ditional random fields of Lafferty et al. (2001). The	page=3 xpos=0 ypos=4 left-column full-justified hanged-line year headchar-lower
I-Body	CRF approach uses similar local features, but rather than	page=3 xpos=0 ypos=4 left-column full-justified aligned-line headchar-capital
I-Body	chaining together local models, they construct a sin-	page=3 xpos=0 ypos=4 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	gle, globally normalized model. The principal advan-	page=3 xpos=0 ypos=4 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tage of the dependency network approach is that advan-	page=3 xpos=0 ypos=4 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tageous bidirectional effects can be obtained without the	page=3 xpos=0 ypos=5 left-column full-justified aligned-line headchar-lower
E-Body	extremely expensive global training required for CRFs.	page=3 xpos=0 ypos=5 left-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-line-space
B-Body	To summarize, we draw a dependency network in	page=3 xpos=0 ypos=5 left-column left-indent indented-line longer-tail line-space headchar-capital
I-Body	which each node has as neighbors all the other nodes	page=3 xpos=0 ypos=5 left-column full-justified hanged-line headchar-lower
I-Body	that we would like to have influence it directly. Each	page=3 xpos=0 ypos=5 left-column full-justified aligned-line headchar-lower
I-Body	node’s neighborhood is then considered in isolation and	page=3 xpos=0 ypos=6 left-column full-justified aligned-line headchar-lower
I-Body	a local model is trained to maximize the conditional like-	page=3 xpos=0 ypos=6 left-column full-justified aligned-line itemization headchar-lower tailchar-hiphen
I-Body	lihood over the training data of that node. At test time,	page=3 xpos=0 ypos=6 left-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	the sequence with the highest product of local conditional	page=3 xpos=0 ypos=6 left-column full-justified aligned-line headchar-lower
I-Body	scores is calculated and returned. We can always find the	page=3 xpos=0 ypos=6 left-column full-justified aligned-line headchar-lower
I-Body	exact maximizing sequence, but only in the case of an	page=3 xpos=0 ypos=7 left-column full-justified aligned-line headchar-lower
I-Body	acyclic net is it guaranteed to be the maximum likelihood	page=3 xpos=0 ypos=7 left-column full-justified aligned-line headchar-lower
E-Body	sequence.	page=3 xpos=0 ypos=7 left-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-SectionHeader	3 Experiments	page=3 xpos=0 ypos=7 left-column right-indent font-largest aligned-line longer-tail line-double-space line-space numbered-heading1 above-double-space above-line-space
B-Body	The part of speech tagged data used in our experiments is	page=3 xpos=0 ypos=8 left-column full-justified aligned-line longer-tail line-double-space line-space headchar-capital
I-Body	the Wall Street Journal data from Penn Treebank III (Mar-	page=3 xpos=0 ypos=8 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	cus et al., 1994). We extracted tagged sentences from the	page=3 xpos=0 ypos=8 left-column full-justified aligned-line year headchar-lower
I-Body	parse trees. <sup>5</sup> We split the data into training, development,	page=3 xpos=0 ypos=8 left-column full-justified font-largest aligned-line headchar-lower tailchar-comma
I-Body	and test sets as in (Collins, 2002). Table 1 lists character-	page=3 xpos=0 ypos=8 left-column full-justified aligned-line year headchar-lower tailchar-hiphen above-double-space above-line-space
B-Footnote	5 Note that these tags (and sentences) are not identical to	page=3 xpos=0 ypos=9 left-column left-indent font-largest indented-line line-double-space line-space numbered-heading1
I-Footnote	those obtained from the tagged/pos directories of the same disk:	page=3 xpos=0 ypos=9 left-column full-justified font-smallest hanged-line headchar-lower tailchar-colon
I-Footnote	hundreds of tags in the RB/RP/IN set were changed to be more	page=3 xpos=0 ypos=9 left-column full-justified font-smallest aligned-line headchar-lower
I-Footnote	consistent in the parsed/mrg version. Maybe we were the last to	page=3 xpos=0 ypos=9 left-column full-justified font-smallest aligned-line headchar-lower
I-Footnote	discover this, but we’ve never seen it in print.	page=3 xpos=0 ypos=9 left-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period column-bottom
I-Body	istics of the three splits. <sup>6</sup> Except where indicated for the	page=3 xpos=5 ypos=0 right-column full-justified font-largest column-top headchar-lower
E-Body	model BEST , all results are on the development set.	page=3 xpos=5 ypos=0 right-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	One innovation in our reporting of results is that we	page=3 xpos=5 ypos=0 right-column left-indent indented-line longer-tail headchar-capital
I-Body	present whole-sentence accuracy numbers as well as the	page=3 xpos=5 ypos=0 right-column full-justified hanged-line headchar-lower
I-Body	traditional per-tag accuracy measure (over all tokens,	page=3 xpos=5 ypos=0 right-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	even unambiguous ones). This is the quantity that most	page=3 xpos=5 ypos=0 right-column full-justified aligned-line headchar-lower
I-Body	sequence models attempt to maximize (and has been mo-	page=3 xpos=5 ypos=1 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tivated over doing per-state optimization as being more	page=3 xpos=5 ypos=1 right-column full-justified aligned-line headchar-lower
I-Body	useful for subsequent linguistic processing: one wants to	page=3 xpos=5 ypos=1 right-column full-justified aligned-line headchar-lower
I-Body	find a coherent sentence interpretation). Further, while	page=3 xpos=5 ypos=1 right-column full-justified aligned-line headchar-lower
I-Body	some tag errors matter much more than others, to a first	page=3 xpos=5 ypos=1 right-column full-justified aligned-line headchar-lower
I-Body	cut getting a single tag wrong in many of the more com-	page=3 xpos=5 ypos=2 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	mon ways (e.g., proper noun vs. common noun; noun vs.	page=3 xpos=5 ypos=2 right-column full-justified aligned-line headchar-lower tailchar-period
I-Body	verb) would lead to errors in a subsequent processor such	page=3 xpos=5 ypos=2 right-column full-justified aligned-line headchar-lower
I-Body	as an information extraction system or a parser that would	page=3 xpos=5 ypos=2 right-column full-justified aligned-line headchar-lower
I-Body	greatly degrade results for the entire sentence. Finally,	page=3 xpos=5 ypos=2 right-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	the fact that the measure has much more dynamic range	page=3 xpos=5 ypos=2 right-column full-justified aligned-line headchar-lower
E-Body	has some appeal when reporting tagging results.	page=3 xpos=5 ypos=3 right-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	The per-state models in this paper are log-linear mod-	page=3 xpos=5 ypos=3 right-column left-indent indented-line longer-tail headchar-capital tailchar-hiphen
I-Body	els, building upon the models in (Ratnaparkhi, 1996) and	page=3 xpos=5 ypos=3 right-column full-justified hanged-line year headchar-lower
I-Body	(Toutanova and Manning, 2000), though some models are	page=3 xpos=5 ypos=3 right-column full-justified aligned-line year
I-Body	in fact strictly simpler. The features in the models are	page=3 xpos=5 ypos=3 right-column full-justified aligned-line headchar-lower
I-Body	defined using templates; there are different templates for	page=3 xpos=5 ypos=4 right-column full-justified aligned-line headchar-lower
I-Body	rare words aimed at learning the correct tags for unknown	page=3 xpos=5 ypos=4 right-column full-justified aligned-line headchar-lower
I-Body	words. <sup>7</sup> We present the results of three classes of experi-	page=3 xpos=5 ypos=4 right-column full-justified font-largest aligned-line headchar-lower tailchar-hiphen
I-Body	ments: experiments with directionality, experiments with	page=3 xpos=5 ypos=4 right-column full-justified aligned-line headchar-lower
E-Body	lexicalization, and experiments with smoothing.	page=3 xpos=5 ypos=4 right-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-SubsectionHeader	3.1 Experiments with Directionality	page=3 xpos=5 ypos=5 right-column right-indent aligned-line shorter-tail line-double-space line-space numbered-heading2 above-double-space above-line-space
B-Body	In this section, we report experiments using log-linear	page=3 xpos=5 ypos=5 right-column full-justified aligned-line longer-tail line-double-space line-space headchar-capital
I-Body	CMMs to populate nets with various structures, exploring	page=3 xpos=5 ypos=5 right-column full-justified aligned-line headchar-capital
I-Body	the relative value of neighboring words’ tags. Table 2 lists	page=3 xpos=5 ypos=5 right-column full-justified aligned-line headchar-lower
I-Body	the discussed networks. All networks have the same ver-	page=3 xpos=5 ypos=5 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tical feature templates: ht <sub>0</sub> , w 0 i features for known words	page=3 xpos=5 ypos=6 right-column full-justified font-largest aligned-line headchar-lower
I-Body	and various ht <sub>0</sub> , σ (w 1n )i word signature features for all	page=3 xpos=5 ypos=6 right-column full-justified font-largest aligned-line headchar-lower
I-Body	words, known or not, including spelling and capitaliza-	page=3 xpos=5 ypos=6 right-column full-justified aligned-line headchar-lower tailchar-hiphen
E-Body	tion features (see section 3.3).	page=3 xpos=5 ypos=6 right-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	Just this vertical conditioning gives an accuracy of	page=3 xpos=5 ypos=6 right-column left-indent indented-line longer-tail headchar-capital
I-Body	93.69% (denoted as “Baseline” in table 2). <sup>8</sup> Condition-	page=3 xpos=5 ypos=7 right-column full-justified font-largest hanged-line tailchar-hiphen above-double-space above-line-space
B-Footnote	6 Tagger results are only comparable when tested not only on	page=3 xpos=5 ypos=7 right-column left-indent font-largest indented-line line-double-space line-space numbered-heading1
I-Footnote	the same data and tag set, but with the same amount of training	page=3 xpos=5 ypos=7 right-column full-justified font-smallest hanged-line headchar-lower
I-Footnote	data. Brants (2000) illustrates very clearly how tagging perfor-	page=3 xpos=5 ypos=7 right-column full-justified font-smallest aligned-line year headchar-lower tailchar-hiphen
I-Footnote	mance increases as training set size grows, largely because the	page=3 xpos=5 ypos=7 right-column full-justified font-smallest aligned-line headchar-lower
I-Footnote	percentage of unknown words decreases while system perfor-	page=3 xpos=5 ypos=7 right-column full-justified font-smallest aligned-line headchar-lower tailchar-hiphen
I-Footnote	mance on them increases (they become increasingly restricted	page=3 xpos=5 ypos=8 right-column full-justified font-smallest aligned-line headchar-lower
I-Footnote	as to word class).	page=3 xpos=5 ypos=8 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Footnote	7 Except where otherwise stated, a count cutoff of 2 was used	page=3 xpos=5 ypos=8 right-column left-indent font-largest indented-line longer-tail numbered-heading1
I-Footnote	for common word features and 35 for rare word features (tem-	page=3 xpos=5 ypos=8 right-column full-justified font-smallest hanged-line headchar-lower tailchar-hiphen
I-Footnote	plates need a support set strictly greater in size than the cutoff	page=3 xpos=5 ypos=8 right-column full-justified font-smallest aligned-line headchar-lower
I-Footnote	before they are included in the model).	page=3 xpos=5 ypos=8 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Footnote	8 Charniak et al. (1993) noted that such a simple model got	page=3 xpos=5 ypos=9 right-column left-indent font-largest indented-line longer-tail numbered-heading1 year
I-Footnote	90.25%, but this was with no unknown word model beyond	page=3 xpos=5 ypos=9 right-column full-justified font-smallest hanged-line
I-Footnote	a prior distribution over tags. Abney et al. (1999) raise this	page=3 xpos=5 ypos=9 right-column full-justified font-smallest aligned-line itemization year headchar-lower
I-Footnote	baseline to 92.34%, and with our sophisticated unknown word	page=3 xpos=5 ypos=9 right-column full-justified font-smallest aligned-line headchar-lower
I-Footnote	model, it gets even higher. The large number of unambiguous	page=3 xpos=5 ypos=9 right-column full-justified font-smallest aligned-line headchar-lower
I-Footnote	tokens and ones with very skewed distributions make the base-	page=3 xpos=5 ypos=9 right-column full-justified font-smallest aligned-line headchar-lower tailchar-hiphen page-bottom
Table	__Table 2__	page=4 xpos=0 ypos=-3 single-column centered left-indent right-indent box page-top table-area above-double-space above-line-space
B-Caption	Table 2: Tagging accuracy on the development set with different sequence feature templates. †All models include the same vertical	page=4 xpos=0 ypos=0 single-column full-justified font-smallest hanged-line longer-tail line-double-space line-space symbol-dagger string-table headchar-capital
E-Caption	word-tag features (ht 0 , w 0 i and various ht 0 , σ (w 1n )i), though the baseline uses a lower cutoff for these features.	page=4 xpos=0 ypos=0 single-column right-indent font-larger aligned-line shorter-tail headchar-lower tailchar-period
Table	__Table 3__	page=4 xpos=0 ypos=0 single-column centered left-indent right-indent box longer-tail table-area above-double-space above-line-space
B-Caption	Table 3: Tagging accuracy with different lexical feature templates on the development set.	page=4 xpos=1 ypos=2 single-column centered left-indent right-indent font-smallest indented-line shorter-tail line-double-space line-space string-table headchar-capital tailchar-period above-line-space
Table	__Table 4__	page=4 xpos=1 ypos=2 single-column centered left-indent right-indent box hanged-line longer-tail line-space table-area above-double-space above-line-space
B-Caption	Table 4: Final tagging accuracy for the best model on the test set.	page=4 xpos=2 ypos=3 single-column centered left-indent right-indent font-smallest indented-line shorter-tail line-double-space line-space string-table headchar-capital tailchar-period column-bottom above-double-space above-line-space
I-Body	ing on the previous tag as well (model L, ht <sub>0</sub> , t −1 i fea-	page=4 xpos=0 ypos=3 left-column full-justified font-largest column-top line-double-space line-space headchar-lower tailchar-hiphen
I-Body	tures) gives 95.79%. The reverse, model R, using the	page=4 xpos=0 ypos=3 left-column full-justified aligned-line headchar-lower
I-Body	next tag instead, is slightly inferior at 95.14%. Model	page=4 xpos=0 ypos=4 left-column full-justified aligned-line headchar-lower
I-Body	L+R, using both tags simultaneously (but with only the	page=4 xpos=0 ypos=4 left-column full-justified aligned-line headchar-capital
I-Body	individual-direction features) gives a much better accu-	page=4 xpos=0 ypos=4 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	racy of 96.57%. Since this model has roughly twice as	page=4 xpos=0 ypos=4 left-column full-justified aligned-line headchar-lower
I-Body	many tag-tag features, the fact that it outperforms the uni-	page=4 xpos=0 ypos=4 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	directional models is not by itself compelling evidence	page=4 xpos=0 ypos=5 left-column full-justified aligned-line headchar-lower
I-Body	for using bidirectional networks. However, it also out-	page=4 xpos=0 ypos=5 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	performs model L+L <sub>2</sub> which adds the ht 0 , t −2 i second-	page=4 xpos=0 ypos=5 left-column full-justified font-largest aligned-line headchar-lower tailchar-hiphen
I-Body	previous word features instead of next word features,	page=4 xpos=0 ypos=5 left-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	which gives only 96.05% (and R+R <sub>2</sub> gives 95.25%). We	page=4 xpos=0 ypos=6 left-column full-justified font-largest aligned-line headchar-lower
I-Body	conclude that, if one wishes to condition on two neigh-	page=4 xpos=0 ypos=6 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	boring nodes (using two sets of 2-tag features), the sym-	page=4 xpos=0 ypos=6 left-column full-justified aligned-line headchar-lower tailchar-hiphen
E-Body	metric bidirectional model is superior.	page=4 xpos=0 ypos=6 left-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-line-space
B-Body	High-performance taggers typically also include joint	page=4 xpos=0 ypos=7 left-column left-indent indented-line longer-tail line-space headchar-capital
I-Body	three-tag counts in some way, either as tag trigrams	page=4 xpos=0 ypos=7 left-column full-justified hanged-line headchar-lower
I-Body	(Brants, 2000) or tag-triple features (Ratnaparkhi, 1996,	page=4 xpos=0 ypos=7 left-column full-justified aligned-line year tailchar-comma
I-Body	Toutanova and Manning, 2000). Models LL, RR, and CR	page=4 xpos=0 ypos=7 left-column full-justified aligned-line year headchar-capital
I-Body	use only the vertical features and a single set of tag-triple	page=4 xpos=0 ypos=8 left-column full-justified aligned-line headchar-lower
I-Body	features: the left tags (t <sub>−2</sub> , t −1 and t <sub>0</sub> ), right tags (t 0 , t +1 ,	page=4 xpos=0 ypos=8 left-column full-justified font-largest aligned-line headchar-lower tailchar-comma
I-Body	t <sub>+2</sub> ), or centered tags (t −1 , t <sub>0</sub> , t +1 ) respectively. Again,	page=4 xpos=0 ypos=8 left-column full-justified font-largest aligned-line itemization headchar-lower tailchar-comma
I-Body	with roughly equivalent feature sets, the left context is	page=4 xpos=0 ypos=8 left-column full-justified aligned-line headchar-lower
I-Body	better than the right, and the centered context is better	page=4 xpos=0 ypos=8 left-column full-justified aligned-line headchar-lower
E-Body	than either unidirectional context.	page=4 xpos=0 ypos=9 left-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-Footnote	line for this task high, while substantial annotator noise creates	page=4 xpos=0 ypos=9 left-column full-justified font-smallest aligned-line longer-tail line-double-space line-space headchar-lower
I-Footnote	an unknown upper bound on the task.	page=4 xpos=0 ypos=9 left-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period column-bottom
B-SubsectionHeader	3.2 Lexicalization	page=4 xpos=5 ypos=3 right-column right-indent column-top numbered-heading2 above-double-space above-line-space
B-Body	Lexicalization has been a key factor in the advance of	page=4 xpos=5 ypos=3 right-column full-justified aligned-line longer-tail line-double-space line-space headchar-capital
I-Body	statistical parsing models, but has been less exploited	page=4 xpos=5 ypos=4 right-column full-justified aligned-line headchar-lower
I-Body	for tagging. Words surrounding the current word have	page=4 xpos=5 ypos=4 right-column full-justified aligned-line headchar-lower
I-Body	been occasionally used in taggers, such as (Ratnaparkhi,	page=4 xpos=5 ypos=4 right-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	1996), Brill’s transformation based tagger (Brill, 1995),	page=4 xpos=5 ypos=4 right-column full-justified aligned-line year tailchar-comma
I-Body	and the HMM model of Lee et al. (2000), but neverthe-	page=4 xpos=5 ypos=5 right-column full-justified aligned-line year headchar-lower tailchar-hiphen
I-Body	less, the only lexicalization consistently included in tag-	page=4 xpos=5 ypos=5 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ging models is the dependence of the part of speech tag	page=4 xpos=5 ypos=5 right-column full-justified aligned-line headchar-lower
E-Body	of a word on the word itself.	page=4 xpos=5 ypos=5 right-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-line-space
B-Body	In maximum entropy models, joint features which look	page=4 xpos=5 ypos=6 right-column left-indent indented-line longer-tail line-space headchar-capital
I-Body	at surrounding words and their tags, as well as joint fea-	page=4 xpos=5 ypos=6 right-column full-justified hanged-line headchar-lower tailchar-hiphen
I-Body	tures of the current word and surrounding words are in	page=4 xpos=5 ypos=6 right-column full-justified aligned-line headchar-lower
I-Body	principle straightforward additions, but have not been in-	page=4 xpos=5 ypos=6 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	corporated into previous models. We have found these	page=4 xpos=5 ypos=7 right-column full-justified aligned-line headchar-lower
I-Body	features to be very useful. We explore here lexicaliza-	page=4 xpos=5 ypos=7 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tion both alone and in combination with preceding and	page=4 xpos=5 ypos=7 right-column full-justified aligned-line headchar-lower
E-Body	following tag histories.	page=4 xpos=5 ypos=7 right-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-line-space
B-Body	Table 3 shows the development set accuracy of several	page=4 xpos=5 ypos=7 right-column left-indent indented-line longer-tail line-space string-table headchar-capital
I-Body	models with various lexical features. All models use the	page=4 xpos=5 ypos=8 right-column full-justified hanged-line headchar-lower
I-Body	same rare word features as the models in Table 2. The	page=4 xpos=5 ypos=8 right-column full-justified aligned-line headchar-lower
I-Body	first two rows show a baseline model using the current	page=4 xpos=5 ypos=8 right-column full-justified aligned-line headchar-lower
I-Body	word only. The count cutoff for this feature was 0 in the	page=4 xpos=5 ypos=8 right-column full-justified aligned-line headchar-lower
I-Body	first model and 2 for the model in the second row. As	page=4 xpos=5 ypos=9 right-column full-justified aligned-line headchar-lower
I-Body	there are no tag sequence features in these models, the ac-	page=4 xpos=5 ypos=9 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	curacy drops significantly if a higher cutoff is used (from	page=4 xpos=5 ypos=9 right-column full-justified aligned-line headchar-lower
E-Body	a per tag accuracy of about 93.7% to only 60.2%).	page=4 xpos=5 ypos=9 right-column right-indent aligned-line shorter-tail itemization headchar-lower tailchar-period page-bottom
B-Body	The third row shows a model where a tag is de-	page=5 xpos=0 ypos=0 left-column left-indent page-top headchar-capital tailchar-hiphen
I-Body	cided solely by the three words centered at the tag po-	page=5 xpos=0 ypos=0 left-column full-justified hanged-line headchar-lower tailchar-hiphen
I-Body	sition (3W). As far as we are aware, models of this	page=5 xpos=0 ypos=0 left-column full-justified aligned-line headchar-lower
I-Body	sort have not been explored previously, but its accu-	page=5 xpos=0 ypos=0 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	racy is surprisingly high: despite having no sequence	page=5 xpos=0 ypos=0 left-column full-justified aligned-line headchar-lower
I-Body	model at all, it is more accurate than a model which uses	page=5 xpos=0 ypos=0 left-column full-justified aligned-line headchar-lower
I-Body	standard tag fourgram HMM features (ht <sub>0</sub> , w 0 i, ht 0 , t −1 i,	page=5 xpos=0 ypos=1 left-column full-justified font-largest aligned-line headchar-lower tailchar-comma
I-Body	ht 0 , t −1 , t −2 i, ht 0 , t −1 , t −2 , t −3 i, shown in Table 2, model	page=5 xpos=0 ypos=1 left-column full-justified font-largest aligned-line headchar-lower
E-Body	L+LL+LLL).	page=5 xpos=0 ypos=1 left-column right-indent aligned-line shorter-tail headchar-capital tailchar-period
B-Body	The fourth and fifth rows show models with bi-	page=5 xpos=0 ypos=1 left-column left-indent indented-line longer-tail headchar-capital tailchar-hiphen
I-Body	directional tagging features. The fourth model	page=5 xpos=0 ypos=1 left-column full-justified hanged-line headchar-lower
I-Body	(3W+ TAGS ) uses the same tag sequence features as	page=5 xpos=0 ypos=2 left-column full-justified aligned-line
I-Body	the last model in Table 2 (ht <sub>0</sub> , t −1 i, ht 0 , t −1 , t −2 i,	page=5 xpos=0 ypos=2 left-column full-justified font-largest aligned-line headchar-lower tailchar-comma
I-Body	ht 0 , t −1 , t +1 i, ht 0 , t +1 i, ht 0 , t +1 , t +2 i) and current, previ-	page=5 xpos=0 ypos=2 left-column full-justified font-largest aligned-line headchar-lower tailchar-hiphen
I-Body	ous, and next word. The last model has in ad-	page=5 xpos=0 ypos=2 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	dition the feature templates ht <sub>0</sub> , w 0 , t −1 i, ht 0 , w 0 , t +1 i,	page=5 xpos=0 ypos=2 left-column full-justified font-largest aligned-line headchar-lower tailchar-comma
I-Body	ht 0 , w −1 , w 0 i, and ht 0 , w 0 , w +1 i, and includes the im-	page=5 xpos=0 ypos=2 left-column full-justified font-largest aligned-line headchar-lower tailchar-hiphen
I-Body	provements in unknown word modeling discussed in sec-	page=5 xpos=0 ypos=3 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tion 3.3. <sup>9</sup> We call this model BEST . BEST has a to-	page=5 xpos=0 ypos=3 left-column full-justified font-largest aligned-line headchar-lower tailchar-hiphen
I-Body	ken accuracy on the final test set of 97.24% and a sen-	page=5 xpos=0 ypos=3 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tence accuracy of 56.34% (see Table 4). A 95% confi-	page=5 xpos=0 ypos=3 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	dence interval for the accuracy (using a binomial model)	page=5 xpos=0 ypos=3 left-column full-justified aligned-line headchar-lower
E-Body	is (97.15%, 97.33%).	page=5 xpos=0 ypos=4 left-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	In order to understand the gains from using right con-	page=5 xpos=0 ypos=4 left-column left-indent indented-line longer-tail headchar-capital tailchar-hiphen
I-Body	text tags and more lexicalization, let us look at an exam-	page=5 xpos=0 ypos=4 left-column full-justified hanged-line headchar-lower tailchar-hiphen
I-Body	ple of an error that the enriched models learn not to make.	page=5 xpos=0 ypos=4 left-column full-justified aligned-line headchar-lower tailchar-period
I-Body	An interesting example of a common tagging error of the	page=5 xpos=0 ypos=4 left-column full-justified aligned-line headchar-capital
I-Body	simpler models which could be corrected by a determinis-	page=5 xpos=0 ypos=4 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tic fixup rule of the kind used in the IDIOMTAG module	page=5 xpos=0 ypos=5 left-column full-justified aligned-line headchar-lower
I-Body	of (Marshall, 1987) is the expression as X as (often, as	page=5 xpos=0 ypos=5 left-column full-justified aligned-line year headchar-lower
I-Body	far as). This should be tagged as/RB X/{RB,JJ} as/IN in	page=5 xpos=0 ypos=5 left-column full-justified aligned-line headchar-lower
I-Body	the Penn Treebank. A model using only current word and	page=5 xpos=0 ypos=5 left-column full-justified aligned-line headchar-lower
I-Body	two left tags (model L+L <sub>2</sub> in Table 2), made 87 errors on	page=5 xpos=0 ypos=5 left-column full-justified font-largest aligned-line headchar-lower
I-Body	this expression, tagging it as/IN X as/IN – since the tag	page=5 xpos=0 ypos=6 left-column full-justified aligned-line headchar-lower
I-Body	sequence probabilities do not give strong reasons to dis-	page=5 xpos=0 ypos=6 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	prefer the most common tagging of as (it is tagged as IN	page=5 xpos=0 ypos=6 left-column full-justified aligned-line headchar-lower
I-Body	over 80% of the time). However, the model 3W+ TAGS ,	page=5 xpos=0 ypos=6 left-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	which uses two right tags and the two surrounding words	page=5 xpos=0 ypos=6 left-column full-justified aligned-line headchar-lower
I-Body	in addition, made only 8 errors of this kind, and model	page=5 xpos=0 ypos=7 left-column full-justified aligned-line headchar-lower
E-Body	BEST made only 6 errors.	page=5 xpos=0 ypos=7 left-column right-indent aligned-line shorter-tail headchar-capital tailchar-period above-double-space above-line-space
B-SubsectionHeader	3.3 Unknown word features	page=5 xpos=0 ypos=7 left-column right-indent aligned-line longer-tail line-double-space line-space numbered-heading2 above-double-space above-line-space
B-Body	Most of the models presented here use a set of un-	page=5 xpos=0 ypos=7 left-column full-justified aligned-line longer-tail line-double-space line-space headchar-capital tailchar-hiphen
I-Body	known word features basically inherited from (Ratna-	page=5 xpos=0 ypos=7 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	parkhi, 1996), which include using character n-gram pre-	page=5 xpos=0 ypos=8 left-column full-justified aligned-line year headchar-lower tailchar-hiphen
I-Body	fixes and suffixes (for n up to 4), and detectors for a few	page=5 xpos=0 ypos=8 left-column full-justified aligned-line headchar-lower
I-Body	other prominent features of words, such as capitaliza-	page=5 xpos=0 ypos=8 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tion, hyphens, and numbers. Doing error analysis on un-	page=5 xpos=0 ypos=8 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	known words on a simple tagging model (with ht <sub>0</sub> , t −1 i,	page=5 xpos=0 ypos=8 left-column full-justified font-largest aligned-line headchar-lower tailchar-comma
I-Body	ht 0 , t −1 , t −2 i, and hw 0 , t 0 i features) suggested several ad-	page=5 xpos=0 ypos=9 left-column full-justified font-largest aligned-line headchar-lower tailchar-hiphen
I-Body	ditional specialized features that can usefully improve	page=5 xpos=0 ypos=9 left-column full-justified aligned-line headchar-lower above-double-space above-line-space
B-Footnote	9 Thede and Harper (1999) use ht <sub>−1</sub> , t <sub>0</sub> , w 0 i templates in	page=5 xpos=0 ypos=9 left-column left-indent font-largest indented-line line-double-space line-space numbered-heading1 year
I-Footnote	their “full-second order” HMM, achieving an accuracy of	page=5 xpos=0 ypos=9 left-column full-justified font-smallest hanged-line headchar-lower
I-Footnote	96.86%. Here we can add the opposite tiling and other features.	page=5 xpos=0 ypos=9 left-column full-justified font-smallest aligned-line tailchar-period column-bottom
Table	__Table 5__	page=5 xpos=5 ypos=-1 right-column centered left-over right-over box column-top table-area above-line-space
B-Caption	Table 5: Accuracy with and without quadratic regularization.	page=5 xpos=5 ypos=1 right-column centered left-indent right-indent font-smallest indented-line shorter-tail line-space string-table headchar-capital tailchar-period above-double-space above-line-space
I-Body	performance. By far the most significant is a crude com-	page=5 xpos=5 ypos=1 right-column full-justified hanged-line longer-tail line-double-space line-space headchar-lower tailchar-hiphen
I-Body	pany name detector which marks capitalized words fol-	page=5 xpos=5 ypos=1 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	lowed within 3 words by a company name suffix like Co.	page=5 xpos=5 ypos=1 right-column full-justified aligned-line headchar-lower tailchar-period
I-Body	or Inc. This suggests that further gains could be made by	page=5 xpos=5 ypos=1 right-column full-justified aligned-line headchar-lower
I-Body	incorporating a good named entity recognizer as a prepro-	page=5 xpos=5 ypos=2 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	cessor to the tagger (reversing the most common order of	page=5 xpos=5 ypos=2 right-column full-justified aligned-line headchar-lower
I-Body	processing in pipelined systems!), and is a good example	page=5 xpos=5 ypos=2 right-column full-justified aligned-line headchar-lower
I-Body	of something that can only be done when using a condi-	page=5 xpos=5 ypos=2 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tional model. Minor gains come from a few additional	page=5 xpos=5 ypos=2 right-column full-justified aligned-line headchar-lower
I-Body	features: an allcaps feature, and a conjunction feature of	page=5 xpos=5 ypos=3 right-column full-justified aligned-line headchar-lower
I-Body	words that are capitalized and have a digit and a dash in	page=5 xpos=5 ypos=3 right-column full-justified aligned-line headchar-lower
I-Body	them (such words are normally common nouns, such as	page=5 xpos=5 ypos=3 right-column full-justified aligned-line headchar-lower
I-Body	CFC-12 or F/A-18). We also found it advantageous to	page=5 xpos=5 ypos=3 right-column full-justified aligned-line headchar-capital
I-Body	use prefixes and suffixes of length up to 10. Together	page=5 xpos=5 ypos=3 right-column full-justified aligned-line headchar-lower
I-Body	with the larger templates, these features contribute to our	page=5 xpos=5 ypos=3 right-column full-justified aligned-line headchar-lower
I-Body	unknown word accuracies being higher than those of pre-	page=5 xpos=5 ypos=4 right-column full-justified aligned-line headchar-lower tailchar-hiphen
E-Body	viously reported taggers.	page=5 xpos=5 ypos=4 right-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-SubsectionHeader	3.4 Smoothing	page=5 xpos=5 ypos=4 right-column right-indent aligned-line shorter-tail line-double-space line-space numbered-heading2 above-double-space above-line-space
B-Body	With so many features in the model, overtraining is a dis-	page=5 xpos=5 ypos=4 right-column full-justified aligned-line longer-tail line-double-space line-space headchar-capital tailchar-hiphen
I-Body	tinct possibility when using pure maximum likelihood es-	page=5 xpos=5 ypos=5 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	timation. We avoid this by using a Gaussian prior (aka	page=5 xpos=5 ypos=5 right-column full-justified aligned-line headchar-lower
I-Body	quadratic regularization or quadratic penalization) which	page=5 xpos=5 ypos=5 right-column full-justified aligned-line headchar-lower
I-Body	resists high feature weights unless they produce great	page=5 xpos=5 ypos=5 right-column full-justified aligned-line headchar-lower
I-Body	score gain. The regularized objective F is:	page=5 xpos=5 ypos=5 right-column right-indent aligned-line shorter-tail headchar-lower tailchar-colon above-double-space above-line-space
B-Equation	F(λ) = X <sub>i</sub> log(P λ (t i |w, t)) + X n j =1 <sub>2σ</sub> λ 2 j 2	page=5 xpos=5 ypos=6 right-column centered left-indent right-indent box indented-line longer-tail line-double-space line-space headchar-capital above-double-space above-line-space
B-Body	Since we use a conjugate-gradient procedure to maximize	page=5 xpos=5 ypos=6 right-column full-justified hanged-line longer-tail line-double-space line-space headchar-capital
I-Body	the data likelihood, the addition of a penalty term is eas-	page=5 xpos=5 ypos=6 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ily incorporated. Both the total size of the penalty and	page=5 xpos=5 ypos=7 right-column full-justified aligned-line headchar-lower
I-Body	the partial derivatives with repsect to each λ <sub>j</sub> are triv-	page=5 xpos=5 ypos=7 right-column full-justified font-largest aligned-line headchar-lower tailchar-hiphen
I-Body	ial to compute; these are added to the log-likelihood and	page=5 xpos=5 ypos=7 right-column full-justified aligned-line headchar-lower
I-Body	log-likelihood derivatives, and the penalized optimization	page=5 xpos=5 ypos=7 right-column full-justified aligned-line headchar-lower
E-Body	procedes without further modification.	page=5 xpos=5 ypos=7 right-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	We have not extensively experimented with the value	page=5 xpos=5 ypos=8 right-column left-indent indented-line longer-tail headchar-capital
I-Body	of σ <sup>2</sup> – which can even be set differently for different pa-	page=5 xpos=5 ypos=8 right-column full-justified font-largest hanged-line headchar-lower tailchar-hiphen
I-Body	rameters or parameter classes. All the results in this paper	page=5 xpos=5 ypos=8 right-column full-justified aligned-line headchar-lower
I-Body	use a constant σ <sup>2</sup> = 0.5, so that the denominator disap-	page=5 xpos=5 ypos=8 right-column full-justified font-largest aligned-line headchar-lower tailchar-hiphen
I-Body	pears in the above expression. Experiments on a simple	page=5 xpos=5 ypos=8 right-column full-justified aligned-line headchar-lower
I-Body	model with σ made an order of magnitude higher or lower	page=5 xpos=5 ypos=8 right-column full-justified aligned-line headchar-lower
E-Body	both resulted in worse performance than with σ <sup>2</sup> = 0.5.	page=5 xpos=5 ypos=9 right-column right-indent font-largest aligned-line shorter-tail headchar-lower tailchar-period
B-Body	Our experiments show that quadratic regularization	page=5 xpos=5 ypos=9 right-column left-indent indented-line longer-tail headchar-capital
I-Body	is very effective in improving the generalization perfor-	page=5 xpos=5 ypos=9 right-column full-justified hanged-line headchar-lower tailchar-hiphen
I-Body	mance of tagging models, mostly by increasing the num-	page=5 xpos=5 ypos=9 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ber of features which could usefully be incorporated. The	page=5 xpos=5 ypos=9 right-column full-justified aligned-line headchar-lower page-bottom
Table	__Table 6__	page=6 xpos=0 ypos=-2 left-column centered left-indent right-indent box page-top table-area above-line-space
B-Caption	Table 6: Effect of changing common word feature cutoffs (fea-	page=6 xpos=0 ypos=0 left-column full-justified font-smallest hanged-line longer-tail line-space string-table headchar-capital tailchar-hiphen
E-Caption	tures with support ≤ cutoff are excluded from the model).	page=6 xpos=0 ypos=0 left-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
I-Body	number of features used in our complex models – in the	page=6 xpos=0 ypos=0 left-column full-justified aligned-line longer-tail line-double-space line-space headchar-lower
I-Body	several hundreds of thousands, is extremely high in com-	page=6 xpos=0 ypos=0 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	parison with the data set size and the number of features	page=6 xpos=0 ypos=0 left-column full-justified aligned-line headchar-lower
I-Body	used in other machine learning domains. We describe two	page=6 xpos=0 ypos=1 left-column full-justified aligned-line headchar-lower
I-Body	sets of experiments aimed at comparing models with and	page=6 xpos=0 ypos=1 left-column full-justified aligned-line headchar-lower
I-Body	without regularization. One is for a simple model with a	page=6 xpos=0 ypos=1 left-column full-justified aligned-line headchar-lower
I-Body	relatively small number of features, and the other is for a	page=6 xpos=0 ypos=1 left-column full-justified aligned-line headchar-lower
E-Body	model with a large number of features.	page=6 xpos=0 ypos=1 left-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	The usefulness of priors in maximum entropy models	page=6 xpos=0 ypos=2 left-column left-indent indented-line longer-tail headchar-capital
I-Body	is not new to this work: Gaussian prior smoothing is ad-	page=6 xpos=0 ypos=2 left-column full-justified hanged-line headchar-lower tailchar-hiphen
I-Body	vocated in Chen and Rosenfeld (2000), and used in all	page=6 xpos=0 ypos=2 left-column full-justified aligned-line year headchar-lower
I-Body	the stochastic LFG work (Johnson et al., 1999). How-	page=6 xpos=0 ypos=2 left-column full-justified aligned-line year headchar-lower tailchar-hiphen
I-Body	ever, until recently, its role and importance have not been	page=6 xpos=0 ypos=2 left-column full-justified aligned-line headchar-lower
I-Body	widely understood. For example, Zhang and Oles (2001)	page=6 xpos=0 ypos=3 left-column full-justified aligned-line year headchar-lower
I-Body	attribute the perceived limited success of logistic regres-	page=6 xpos=0 ypos=3 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	sion for text categorization to a lack of use of regular-	page=6 xpos=0 ypos=3 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ization. At any rate, regularized conditional loglinear	page=6 xpos=0 ypos=3 left-column full-justified aligned-line headchar-lower
I-Body	models have not previously been applied to the prob-	page=6 xpos=0 ypos=3 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	lem of producing a high quality part-of-speech tagger:	page=6 xpos=0 ypos=4 left-column full-justified aligned-line headchar-lower tailchar-colon
I-Body	Ratnaparkhi (1996), Toutanova and Manning (2000), and	page=6 xpos=0 ypos=4 left-column full-justified aligned-line year headchar-capital
I-Body	Collins (2002) all present unregularized models. Indeed,	page=6 xpos=0 ypos=4 left-column full-justified aligned-line year headchar-capital tailchar-comma
I-Body	the result of Collins (2002) that including low support	page=6 xpos=0 ypos=4 left-column full-justified aligned-line year headchar-lower
I-Body	features helps a voted perceptron model but harms a max-	page=6 xpos=0 ypos=4 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	imum entropy model is undone once the weights of the	page=6 xpos=0 ypos=5 left-column full-justified aligned-line headchar-lower
E-Body	maximum entropy model are regularized.	page=6 xpos=0 ypos=5 left-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	Table 5 shows results on the development set from two	page=6 xpos=0 ypos=5 left-column left-indent indented-line longer-tail string-table headchar-capital
I-Body	pairs of experiments. The first pair of models use com-	page=6 xpos=0 ypos=5 left-column full-justified hanged-line headchar-lower tailchar-hiphen
I-Body	mon word templates ht <sub>0</sub> , w 0 i, ht 0 , t −1 , t −2 i and the same	page=6 xpos=0 ypos=5 left-column full-justified font-largest aligned-line headchar-lower
I-Body	rare word templates as used in the models in table 2. The	page=6 xpos=0 ypos=6 left-column full-justified aligned-line headchar-lower
I-Body	second pair of models use the same features as model	page=6 xpos=0 ypos=6 left-column full-justified aligned-line headchar-lower
I-Body	BEST with a higher frequency cutoff of 5 for common	page=6 xpos=0 ypos=6 left-column full-justified aligned-line headchar-capital
E-Body	word features.	page=6 xpos=0 ypos=6 left-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	For the first pair of models, the error reduction from	page=6 xpos=0 ypos=7 left-column left-indent indented-line longer-tail headchar-capital
I-Body	smoothing is 5.3% overall and 20.1% on unknown words.	page=6 xpos=0 ypos=7 left-column full-justified hanged-line headchar-lower tailchar-period
I-Body	For the second pair of models, the error reduction is	page=6 xpos=0 ypos=7 left-column full-justified aligned-line headchar-capital
I-Body	even bigger: 16.2% overall after convergence and 5.8% if	page=6 xpos=0 ypos=7 left-column full-justified aligned-line headchar-lower
I-Body	looking at the best accuracy achieved by the unsmoothed	page=6 xpos=0 ypos=7 left-column full-justified aligned-line headchar-lower
I-Body	model (by stopping training after 75 iterations; see be-	page=6 xpos=0 ypos=8 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	low). The especially large reduction in unknown word er-	page=6 xpos=0 ypos=8 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ror reflects the fact that, because penalties are effectively	page=6 xpos=0 ypos=8 left-column full-justified aligned-line headchar-lower
I-Body	stronger for rare features than frequent ones, the presence	page=6 xpos=0 ypos=8 left-column full-justified aligned-line headchar-lower
I-Body	of penalties increases the degree to which more general	page=6 xpos=0 ypos=8 left-column full-justified aligned-line headchar-lower
I-Body	cross-word signature features (which apply to unknown	page=6 xpos=0 ypos=9 left-column full-justified aligned-line headchar-lower
I-Body	words) are used, relative to word-specific sparse features	page=6 xpos=0 ypos=9 left-column full-justified aligned-line headchar-lower
E-Body	(which do not apply to unknown words).	page=6 xpos=0 ypos=9 left-column right-indent aligned-line shorter-tail tailchar-period
B-Body	Secondly, use of regularization allows us to incorporate	page=6 xpos=0 ypos=9 left-column left-indent indented-line longer-tail headchar-capital
I-Body	features with low support into the model while improving	page=6 xpos=0 ypos=9 left-column full-justified hanged-line headchar-lower column-bottom
Figure	__Figure 4__	page=6 xpos=5 ypos=-1 right-column centered left-indent right-indent box column-top figure-area above-blank-line above-double-space above-line-space
B-Caption	Figure 4: Accuracy by training iterations, with and without	page=6 xpos=5 ypos=1 right-column full-justified font-smallest hanged-line longer-tail line-blank-line line-double-space line-space string-figure headchar-capital
E-Caption	quadratic regularization.	page=6 xpos=5 ypos=1 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
I-Body	performance. Whereas Ratnaparkhi (1996) used feature	page=6 xpos=5 ypos=1 right-column full-justified aligned-line longer-tail line-double-space line-space year headchar-lower
I-Body	support cutoffs and early stopping to stop overfitting of	page=6 xpos=5 ypos=2 right-column full-justified aligned-line headchar-lower
I-Body	the model, and Collins (2002) contends that including	page=6 xpos=5 ypos=2 right-column full-justified aligned-line year headchar-lower
I-Body	low support features harms a maximum entropy model,	page=6 xpos=5 ypos=2 right-column full-justified aligned-line headchar-lower tailchar-comma
I-Body	our results show that low support features are useful in a	page=6 xpos=5 ypos=2 right-column full-justified aligned-line headchar-lower
I-Body	regularized maximum entropy model. Table 6 contrasts	page=6 xpos=5 ypos=2 right-column full-justified aligned-line headchar-lower
I-Body	our results with those from Collins (2002). Since the	page=6 xpos=5 ypos=3 right-column full-justified aligned-line year headchar-lower
I-Body	models are not the same, the exact numbers are incompa-	page=6 xpos=5 ypos=3 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	rable, but the difference in direction is important: in the	page=6 xpos=5 ypos=3 right-column full-justified aligned-line headchar-lower
I-Body	regularized model, performance improves with the inclu-	page=6 xpos=5 ypos=3 right-column full-justified aligned-line headchar-lower tailchar-hiphen
E-Body	sion of low support features.	page=6 xpos=5 ypos=3 right-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	Finally, in addition to being significantly more accu-	page=6 xpos=5 ypos=4 right-column left-indent indented-line longer-tail headchar-capital tailchar-hiphen
I-Body	rate, smoothed models train much faster than unsmoothed	page=6 xpos=5 ypos=4 right-column full-justified hanged-line headchar-lower
I-Body	ones, and do not benefit from early stopping. For ex-	page=6 xpos=5 ypos=4 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ample, the first smoothed model in Table 5 required 80	page=6 xpos=5 ypos=4 right-column full-justified aligned-line headchar-lower
I-Body	conjugate gradient iterations to converge (somewhat ar-	page=6 xpos=5 ypos=4 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	bitrarily defined as a maximum difference of 10 <sup>−4</sup> in fea-	page=6 xpos=5 ypos=5 right-column full-justified font-largest aligned-line headchar-lower tailchar-hiphen
I-Body	ture weights between iterations), while its corresponding	page=6 xpos=5 ypos=5 right-column full-justified aligned-line headchar-lower
I-Body	unsmoothed model required 335 iterations, thus training	page=6 xpos=5 ypos=5 right-column full-justified aligned-line headchar-lower
I-Body	was roughly 4 times slower. <sup>10</sup> The second pair of models	page=6 xpos=5 ypos=5 right-column full-justified font-largest aligned-line headchar-lower
I-Body	required 134 and 370 iterations respectively. As might	page=6 xpos=5 ypos=6 right-column full-justified aligned-line headchar-lower
I-Body	be expected, unsmoothed models reach their highest gen-	page=6 xpos=5 ypos=6 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	eralization capacity long before convergence and accu-	page=6 xpos=5 ypos=6 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	racy on an unseen test set drops considerably with fur-	page=6 xpos=5 ypos=6 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ther iterations. This is not the case for smoothed mod-	page=6 xpos=5 ypos=6 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	els, as their test set accuracy increases almost monoton-	page=6 xpos=5 ypos=7 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	ically with training iterations. <sup>11</sup> Figure 4 shows a graph	page=6 xpos=5 ypos=7 right-column full-justified font-largest aligned-line headchar-lower
I-Body	of training iterations versus accuracy for the second pair	page=6 xpos=5 ypos=7 right-column full-justified aligned-line headchar-lower
E-Body	of models on the development set.	page=6 xpos=5 ypos=7 right-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
B-SectionHeader	4 Conclusion	page=6 xpos=5 ypos=8 right-column right-indent font-largest aligned-line shorter-tail line-double-space line-space numbered-heading1 above-double-space above-line-space
B-Body	We have shown how broad feature use, when combined	page=6 xpos=5 ypos=8 right-column full-justified aligned-line longer-tail line-double-space line-space headchar-capital
I-Body	with appropriate model regularization, produces a supe-	page=6 xpos=5 ypos=8 right-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	rior level of tagger performance. While experience sug-	page=6 xpos=5 ypos=8 right-column full-justified aligned-line headchar-lower tailchar-hiphen above-double-space above-line-space
B-Footnote	10 On a 2GHz PC, this is still an important difference: our	page=6 xpos=5 ypos=9 right-column left-indent font-largest indented-line line-double-space line-space numbered-heading1
I-Footnote	largest models require about 25 minutes per iteration to train.	page=6 xpos=5 ypos=9 right-column right-indent font-smallest hanged-line shorter-tail headchar-lower tailchar-period
B-Footnote	11 In practice one notices some wiggling in the curve, but	page=6 xpos=5 ypos=9 right-column left-indent font-largest indented-line longer-tail numbered-heading1
I-Footnote	the trend remains upward even beyond our chosen convergence	page=6 xpos=5 ypos=9 right-column full-justified font-smallest hanged-line headchar-lower
I-Footnote	point.	page=6 xpos=5 ypos=9 right-column right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period page-bottom
I-Body	gests that the final accuracy number presented here could	page=7 xpos=0 ypos=0 left-column full-justified page-top headchar-lower
I-Body	be slightly improved upon by classifier combination, it is	page=7 xpos=0 ypos=0 left-column full-justified aligned-line headchar-lower
I-Body	worth noting that not only is this tagger better than any	page=7 xpos=0 ypos=0 left-column full-justified aligned-line headchar-lower
I-Body	previous single tagger, but it also appears to outperform	page=7 xpos=0 ypos=0 left-column full-justified aligned-line headchar-lower
I-Body	Brill and Wu (1998), the best-known combination tagger	page=7 xpos=0 ypos=0 left-column full-justified aligned-line year headchar-capital
I-Body	(they report an accuracy of 97.16% over the same WSJ	page=7 xpos=0 ypos=0 left-column full-justified aligned-line
I-Body	data, but using a larger training set, which should favor	page=7 xpos=0 ypos=1 left-column full-justified aligned-line headchar-lower
E-Body	them).	page=7 xpos=0 ypos=1 left-column right-indent aligned-line shorter-tail headchar-lower tailchar-period
B-Body	While part-of-speech tagging is now a fairly well-worn	page=7 xpos=0 ypos=1 left-column left-indent indented-line longer-tail headchar-capital
I-Body	road, and our ability to win performance increases in	page=7 xpos=0 ypos=1 left-column full-justified hanged-line headchar-lower
I-Body	this domain is starting to be limited by the rate of er-	page=7 xpos=0 ypos=1 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	rors and inconsistencies in the Penn Treebank training	page=7 xpos=0 ypos=2 left-column full-justified aligned-line headchar-lower
I-Body	data, this work also has broader implications. Across	page=7 xpos=0 ypos=2 left-column full-justified aligned-line headchar-lower
I-Body	the many NLP problems which involve sequence mod-	page=7 xpos=0 ypos=2 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	els over sparse multinomial distributions, it suggests that	page=7 xpos=0 ypos=2 left-column full-justified aligned-line headchar-lower
I-Body	feature-rich models with extensive lexicalization, bidirec-	page=7 xpos=0 ypos=2 left-column full-justified aligned-line headchar-lower tailchar-hiphen
I-Body	tional inference, and effective regularization will be key	page=7 xpos=0 ypos=2 left-column full-justified aligned-line headchar-lower
E-Body	elements in producing state-of-the-art results.	page=7 xpos=0 ypos=3 left-column right-indent aligned-line shorter-tail headchar-lower tailchar-period above-double-space above-line-space
AcknowledgementHeader	Acknowledgements	page=7 xpos=0 ypos=3 left-column right-indent font-largest aligned-line shorter-tail line-double-space line-space string-acknowledgement headchar-capital above-double-space above-line-space
B-Acknowledgement	This work was supported in part by the Advanced Re-	page=7 xpos=0 ypos=3 left-column full-justified aligned-line longer-tail line-double-space line-space headchar-capital tailchar-hiphen
I-Acknowledgement	search and Development Activity (ARDA)’s Advanced	page=7 xpos=0 ypos=3 left-column full-justified aligned-line headchar-lower
I-Acknowledgement	Question Answering for Intelligence (AQUAINT) Pro-	page=7 xpos=0 ypos=4 left-column full-justified aligned-line headchar-capital tailchar-hiphen
I-Acknowledgement	gram, by the National Science Foundation under Grant	page=7 xpos=0 ypos=4 left-column full-justified aligned-line headchar-lower
I-Acknowledgement	No. IIS-0085896, and by an IBM Faculty Partnership	page=7 xpos=0 ypos=4 left-column full-justified aligned-line headchar-capital
E-Acknowledgement	Award.	page=7 xpos=0 ypos=4 left-column right-indent aligned-line shorter-tail headchar-capital tailchar-period above-double-space above-line-space
ReferenceHeader	References	page=7 xpos=0 ypos=5 left-column right-indent font-largest aligned-line longer-tail line-double-space line-space string-reference headchar-capital above-double-space above-line-space
B-Reference	Steven Abney, Robert E. Schapire, and Yoram Singer. 1999.	page=7 xpos=0 ypos=5 left-column full-justified font-smallest aligned-line longer-tail line-double-space line-space year headchar-capital tailchar-period
I-Reference	Boosting applied to tagging and PP attachment. In	page=7 xpos=0 ypos=5 left-column left-indent font-smallest indented-line headchar-capital
I-Reference	EMNLP/VLC 1999, pages 38–45.	page=7 xpos=0 ypos=5 left-column left-indent right-indent font-smallest aligned-line shorter-tail year headchar-capital tailchar-period
B-Reference	Thorsten Brants. 2000. TnT – a statistical part-of-speech tagger.	page=7 xpos=0 ypos=5 left-column full-justified font-smallest hanged-line longer-tail year headchar-capital tailchar-period
I-Reference	In ANLP 6, pages 224–231.	page=7 xpos=0 ypos=5 left-column left-indent right-indent font-smallest indented-line shorter-tail headchar-capital tailchar-period
B-Reference	Eric Brill and Jun Wu. 1998. Classifier combination for	page=7 xpos=0 ypos=6 left-column full-justified font-smallest hanged-line longer-tail year headchar-capital
I-Reference	improved lexical disambiguation. In ACL 36/COLING 17,	page=7 xpos=0 ypos=6 left-column left-indent font-smallest indented-line headchar-lower tailchar-comma
I-Reference	pages 191–195.	page=7 xpos=0 ypos=6 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Reference	Eric Brill. 1995. Transformation-based error-driven learning	page=7 xpos=0 ypos=6 left-column full-justified font-smallest hanged-line longer-tail year headchar-capital
I-Reference	and natural language processing: A case study in part-of-	page=7 xpos=0 ypos=6 left-column left-indent font-smallest indented-line headchar-lower tailchar-hiphen
I-Reference	speech tagging. Computational Linguistics, 21(4):543–565.	page=7 xpos=0 ypos=6 left-column left-indent right-indent font-smallest aligned-line headchar-lower tailchar-period
B-Reference	Eugene Charniak, Curtis Hendrickson, Neil Jacobson, and Mike	page=7 xpos=0 ypos=6 left-column full-justified font-smallest hanged-line headchar-capital
I-Reference	Perkowitz. 1993. Equations for part-of-speech tagging. In	page=7 xpos=0 ypos=7 left-column left-indent font-smallest indented-line year headchar-capital
I-Reference	AAAI 11, pages 784–789.	page=7 xpos=0 ypos=7 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Stanley F. Chen and Ronald Rosenfeld. 2000. A survey of	page=7 xpos=0 ypos=7 left-column full-justified font-smallest hanged-line longer-tail year headchar-capital
I-Reference	smoothing techniques for maximum entropy models. IEEE	page=7 xpos=0 ypos=7 left-column left-indent font-smallest indented-line headchar-lower
I-Reference	Transactions on Speech and Audio Processing, 8(1):37–50.	page=7 xpos=0 ypos=7 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	Kenneth W. Church. 1988. A stochastic parts program and	page=7 xpos=0 ypos=7 left-column full-justified font-smallest hanged-line longer-tail year headchar-capital
I-Reference	noun phrase parser for unrestricted text. In ANLP 2, pages	page=7 xpos=0 ypos=8 left-column left-indent font-smallest indented-line headchar-lower
I-Reference	136–143.	page=7 xpos=0 ypos=8 left-column left-indent right-indent font-smallest aligned-line shorter-tail tailchar-period
B-Reference	Michael Collins. 2002. Discriminative training methods for	page=7 xpos=0 ypos=8 left-column full-justified font-smallest hanged-line longer-tail year headchar-capital
I-Reference	Hidden Markov Models: Theory and experiments with per-	page=7 xpos=0 ypos=8 left-column left-indent font-smallest indented-line headchar-capital tailchar-hiphen
I-Reference	ceptron algorithms. In EMNLP 2002.	page=7 xpos=0 ypos=8 left-column left-indent right-indent font-smallest aligned-line shorter-tail year headchar-lower tailchar-period
B-Reference	Robert G. Cowell, A. Philip Dawid, Steffen L. Lauritzen, and	page=7 xpos=0 ypos=8 left-column full-justified font-smallest hanged-line longer-tail headchar-capital
I-Reference	David J. Spiegelhalter. 1999. Probabilistic Networks and	page=7 xpos=0 ypos=8 left-column left-indent font-smallest indented-line year headchar-capital
I-Reference	Expert Systems. Springer-Verlag, New York.	page=7 xpos=0 ypos=9 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-capital tailchar-period
B-Reference	David Heckerman, David Maxwell Chickering, Christopher	page=7 xpos=0 ypos=9 left-column full-justified font-smallest hanged-line longer-tail headchar-capital
I-Reference	Meek, Robert Rounthwaite, and Carl Myers Kadie. 2000.	page=7 xpos=0 ypos=9 left-column left-indent font-smallest indented-line year headchar-capital tailchar-period
I-Reference	Dependency networks for inference, collaborative filtering	page=7 xpos=0 ypos=9 left-column left-indent font-smallest aligned-line headchar-capital
I-Reference	and data visualization. Journal of Machine Learning Re-	page=7 xpos=0 ypos=9 left-column left-indent font-smallest aligned-line headchar-lower tailchar-hiphen
I-Reference	search, 1(1):49–75.	page=7 xpos=0 ypos=9 left-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period column-bottom
B-Reference	Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and	page=7 xpos=5 ypos=0 right-column full-justified font-smallest column-top headchar-capital
I-Reference	Stefan Riezler. 1999. Estimators for stochastic “unification-	page=7 xpos=5 ypos=0 right-column left-indent font-smallest indented-line year headchar-capital tailchar-hiphen
I-Reference	based” grammars. In ACL 37, pages 535–541.	page=7 xpos=5 ypos=0 right-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Reference	Dan Klein and Christopher D. Manning. 2002. Conditional	page=7 xpos=5 ypos=0 right-column full-justified font-smallest hanged-line longer-tail year headchar-capital
I-Reference	structure versus conditional estimation in NLP models. In	page=7 xpos=5 ypos=0 right-column left-indent font-smallest indented-line headchar-lower
I-Reference	EMNLP 2002, pages 9–16.	page=7 xpos=5 ypos=0 right-column left-indent right-indent font-smallest aligned-line shorter-tail year headchar-capital tailchar-period
B-Reference	John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.	page=7 xpos=5 ypos=0 right-column full-justified font-smallest hanged-line longer-tail year headchar-capital tailchar-period
I-Reference	Conditional random fields: Probabilistic models for seg-	page=7 xpos=5 ypos=1 right-column left-indent font-smallest indented-line headchar-capital tailchar-hiphen
I-Reference	menting and labeling sequence data. In ICML-2001, pages	page=7 xpos=5 ypos=1 right-column left-indent font-smallest aligned-line year headchar-lower
I-Reference	282–289.	page=7 xpos=5 ypos=1 right-column left-indent right-indent font-smallest aligned-line shorter-tail tailchar-period
B-Reference	Sang-Zoo Lee, Jun ichi Tsujii, and Hae-Chang Rim. 2000. Part-	page=7 xpos=5 ypos=1 right-column full-justified font-smallest hanged-line longer-tail year headchar-capital tailchar-hiphen
I-Reference	of-speech tagging based on Hidden Markov Model assuming	page=7 xpos=5 ypos=1 right-column left-indent font-smallest indented-line headchar-lower
I-Reference	joint independence. In ACL 38, pages 263–169.	page=7 xpos=5 ypos=1 right-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Reference	Mitchell P. Marcus, Beatrice Santorini, and Mary A. Marcinkie-	page=7 xpos=5 ypos=2 right-column full-justified font-smallest hanged-line longer-tail headchar-capital tailchar-hiphen
I-Reference	wicz. 1994. Building a large annotated corpus of English:	page=7 xpos=5 ypos=2 right-column left-indent font-smallest indented-line year headchar-lower tailchar-colon
I-Reference	The Penn Treebank. Computational Linguistics, 19:313–	page=7 xpos=5 ypos=2 right-column left-indent font-smallest aligned-line headchar-capital
I-Reference	330.	page=7 xpos=5 ypos=2 right-column left-indent right-indent font-smallest aligned-line shorter-tail tailchar-period
B-Reference	Ian Marshall. 1987. Tag selection using probabilistic methods.	page=7 xpos=5 ypos=2 right-column full-justified font-smallest hanged-line longer-tail year headchar-capital tailchar-period
I-Reference	In Roger Garside, Geoffrey Sampson, and Geoffrey Leech,	page=7 xpos=5 ypos=2 right-column left-indent font-smallest indented-line headchar-capital tailchar-comma
I-Reference	editors, The Computational analysis of English: a corpus-	page=7 xpos=5 ypos=2 right-column left-indent font-smallest aligned-line headchar-lower tailchar-hiphen
I-Reference	based approach, pages 42–65. Longman, London.	page=7 xpos=5 ypos=3 right-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
B-Reference	Adwait Ratnaparkhi. 1996. A maximum entropy model for	page=7 xpos=5 ypos=3 right-column full-justified font-smallest hanged-line longer-tail year headchar-capital
I-Reference	part-of-speech tagging. In EMNLP 1, pages 133–142.	page=7 xpos=5 ypos=3 right-column left-indent right-indent font-smallest indented-line shorter-tail headchar-lower tailchar-period
B-Reference	Scott M. Thede and Mary P. Harper. 1999. Second-order hidden	page=7 xpos=5 ypos=3 right-column full-justified font-smallest hanged-line longer-tail year headchar-capital
I-Reference	Markov model for part-of-speech tagging. In ACL 37, pages	page=7 xpos=5 ypos=3 right-column left-indent font-smallest indented-line headchar-capital
I-Reference	175–182.	page=7 xpos=5 ypos=3 right-column left-indent right-indent font-smallest aligned-line shorter-tail tailchar-period
B-Reference	Kristina Toutanova and Christopher Manning. 2000. Enriching	page=7 xpos=5 ypos=4 right-column full-justified font-smallest hanged-line longer-tail year headchar-capital
I-Reference	the knowledge sources used in a maximum entropy part-of-	page=7 xpos=5 ypos=4 right-column left-indent font-smallest indented-line headchar-lower tailchar-hiphen
I-Reference	speech tagger. In EMNLP/VLC 1999, pages 63–71.	page=7 xpos=5 ypos=4 right-column left-indent right-indent font-smallest aligned-line shorter-tail year headchar-lower tailchar-period
B-Reference	Tong Zhang and Frank J. Oles. 2001. Text categorization based	page=7 xpos=5 ypos=4 right-column full-justified font-smallest hanged-line longer-tail year headchar-capital
I-Reference	on regularized linear classification methods. Information Re-	page=7 xpos=5 ypos=4 right-column left-indent font-smallest indented-line headchar-lower tailchar-hiphen
I-Reference	trieval, 4:5–31.	page=7 xpos=5 ypos=4 right-column left-indent right-indent font-smallest aligned-line shorter-tail headchar-lower tailchar-period
